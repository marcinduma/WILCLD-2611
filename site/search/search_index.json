{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to our Cisco Live Hands-on Lab Speakers: Faisal Chaudhry , Distinguished Engineer, Cisco Systems, Inc. - Distinguished Speaker Filip Wardzichowski , Consulting Engineer, Cisco Systems, Inc. - Distinguished Speaker Marcin Duma , Consulting Engineer, Cisco Systems, Inc. Containerized Applications on Hybrid Cloud Environments Nowadays application world is very dynamic. Changes in the infrastructures happens fast, requirements changes even faster. Application components starts to be deployed in different locations based on various technical and business requirements. It's nothing rare to deploy different layers of applications in physically different locations. Rate of use of Public Clouds vs on premise resources changes. Public Cloud become more and more considered for applications which can be hosted outside of own physicall capacity. Rate of containeraized applications grow. Customers are deploying Kubernetes to orchestrate tenants, where they run their systems. Very often having dozens of Kubernetes deployments is nothing strange. However management of them become complicated. Important is to have control at your overall environment. Cisco Container Platform (CCP) gives the possibility to deploy, orchestrate and provide a controll for all your Kubernetes tenants regardless of locations they are deployed. High Level Design of Lab scenario. During this lab session you will be able to experience of deployment multi-cloud application run on Kubernetes tenants. Three tier application is running in two different locations: On-Prem AWS Components of the application and their interconnection is presented on the figure below. By using CCP, you will be able to deploy Kubernetes clusters in vSphere - on premise and AWS. Once done, you will manage each of Kubernetes tenant from central terminal server running Kubernetes commands to deploy our App. In the end of excercise you finally will see the results of Web-based interface showing you data processes by the app. Enjoy!","title":"Home"},{"location":"#welcome-to-our-cisco-live-hands-on-lab","text":"Speakers: Faisal Chaudhry , Distinguished Engineer, Cisco Systems, Inc. - Distinguished Speaker Filip Wardzichowski , Consulting Engineer, Cisco Systems, Inc. - Distinguished Speaker Marcin Duma , Consulting Engineer, Cisco Systems, Inc.","title":"Welcome to our Cisco Live Hands-on Lab"},{"location":"#containerized-applications-on-hybrid-cloud-environments","text":"Nowadays application world is very dynamic. Changes in the infrastructures happens fast, requirements changes even faster. Application components starts to be deployed in different locations based on various technical and business requirements. It's nothing rare to deploy different layers of applications in physically different locations. Rate of use of Public Clouds vs on premise resources changes. Public Cloud become more and more considered for applications which can be hosted outside of own physicall capacity. Rate of containeraized applications grow. Customers are deploying Kubernetes to orchestrate tenants, where they run their systems. Very often having dozens of Kubernetes deployments is nothing strange. However management of them become complicated. Important is to have control at your overall environment. Cisco Container Platform (CCP) gives the possibility to deploy, orchestrate and provide a controll for all your Kubernetes tenants regardless of locations they are deployed.","title":"Containerized Applications on Hybrid Cloud Environments"},{"location":"#high-level-design-of-lab-scenario","text":"During this lab session you will be able to experience of deployment multi-cloud application run on Kubernetes tenants. Three tier application is running in two different locations: On-Prem AWS Components of the application and their interconnection is presented on the figure below. By using CCP, you will be able to deploy Kubernetes clusters in vSphere - on premise and AWS. Once done, you will manage each of Kubernetes tenant from central terminal server running Kubernetes commands to deploy our App. In the end of excercise you finally will see the results of Web-based interface showing you data processes by the app. Enjoy!","title":"High Level Design of Lab scenario."},{"location":"LAB_access%20-%20bck/","text":"Connectivity Check 1. Lab access general description The lab has been built leveraging multiple cloud environments as following: Amazon Web Services Private intrastructure on-prem You will have access to Cisco Container Platform GUI, where you will setup new Kubernetes Clusters deployed in AWS and On-Prem. In the end you will manage your application that will be deployed in 2 different environments. In this lab you will see how to connect microservices together to make whole application work. Most of the tasks you will do from Linux Jumphost that is running on-premise. From there you will deploy components of your application in Kubernetes Cluster in AWS and on-prem. 2. Cisco dCloud dashboard The entire lab for the session is built using Cisco dCloud environment. To access it, you need to login to dCloud dashboard first. To do so, open https://dcloud.cisco.com in your browser. On the right top corner is button to login. To enter dCloud dashboard, please use your CCO account. It's same credentials which you use to access cisco.com resources ie downloading the Cisco software. Once login to the Cisco dCloud dashboard, change your Data Center to be LON - EMEAR. To do it, click on the button left to your profile - right top corner. When moved to correct DataCenter for the session, navigate to \"My Hub\" section, where you will see your assigned session. On your session list you will see the one for HOLCLD-2101. Please open \"view\" to check details, how to login to VPN. Now navigate to Details, where you find credentials to connect to dCloud VPN network. Cisco Anyconnect Mobility Client Run Cisco Anyconnect VPN client available on your desktop. Check credentials and URL in the dCloud session details as described in section above. Your are connected to infrastructure on-prem. 3. Accessing Linux Jumphost Open PuTTY client or any equivalent SSH client on your desktop. Enter following IP address, make sure SSH is the selected protocol. Computer: 198.18.133.10 User name: cisco User password: C1sco12345 4. Accessing Cisco Container Platform Cisco Container Platform manages Kuberenetes clusters in the private infrasturcture. You will have access to dedicated instance of Cisco Container Platform, from which you will manage your own Kuberenetes Clusters used later on to deploy application. Please find login credentials and URL to your CCP instance below: URL: https://198.18.133.100 User name: admin User password: C1sco12345 You can explore CCP through the GUI, but please do not delete content already created. 5. Accessing CSR1kv Lab router Your session contain Cisco CSR1kv router. It will be used to terminate Site-2-Site tunnels with AWS and GCP tenants. The VPN configuration will be generated by cloud platforms and then copied to the CLI. Please find login credentials and IP address to your CSR1kv router below - use SSH: Computer: 198.18.133.254 User name: admin User password: C1sco12345 Do not delete configuration already existing on the router. 6. Accessing vCenter for Lab Whole setup is done on ESXi in Cisco dCloud environment. During the lab you don't need to perform actions on vCenter itself. However for case of troubleshooting or exploration, credentials to your vCenter below. URL: https://vc65.demo.dcloud.cisco.com/ui User name: administrator User password: C1sco12345! Do not delete configuration nor VM machines already existing on the vCenter. 7. Amazon Web Services access From Chrome web browser on your desktop, please open URL https://fwardzic.signin.aws.amazon.com/console Enter your credentials that you can find in credentials page. Once logged in, make sure you will use Frankfurt region (eu-central-1). This is important since VPN Site-2-Site tunnel will be established to that particular region.","title":"Connectivity Check"},{"location":"LAB_access%20-%20bck/#connectivity-check","text":"","title":"Connectivity Check"},{"location":"LAB_access%20-%20bck/#1-lab-access-general-description","text":"The lab has been built leveraging multiple cloud environments as following: Amazon Web Services Private intrastructure on-prem You will have access to Cisco Container Platform GUI, where you will setup new Kubernetes Clusters deployed in AWS and On-Prem. In the end you will manage your application that will be deployed in 2 different environments. In this lab you will see how to connect microservices together to make whole application work. Most of the tasks you will do from Linux Jumphost that is running on-premise. From there you will deploy components of your application in Kubernetes Cluster in AWS and on-prem.","title":"1. Lab access general description"},{"location":"LAB_access%20-%20bck/#2-cisco-dcloud-dashboard","text":"The entire lab for the session is built using Cisco dCloud environment. To access it, you need to login to dCloud dashboard first. To do so, open https://dcloud.cisco.com in your browser. On the right top corner is button to login. To enter dCloud dashboard, please use your CCO account. It's same credentials which you use to access cisco.com resources ie downloading the Cisco software. Once login to the Cisco dCloud dashboard, change your Data Center to be LON - EMEAR. To do it, click on the button left to your profile - right top corner. When moved to correct DataCenter for the session, navigate to \"My Hub\" section, where you will see your assigned session. On your session list you will see the one for HOLCLD-2101. Please open \"view\" to check details, how to login to VPN. Now navigate to Details, where you find credentials to connect to dCloud VPN network.","title":"2. Cisco dCloud dashboard"},{"location":"LAB_access%20-%20bck/#cisco-anyconnect-mobility-client","text":"Run Cisco Anyconnect VPN client available on your desktop. Check credentials and URL in the dCloud session details as described in section above. Your are connected to infrastructure on-prem.","title":"Cisco Anyconnect Mobility Client"},{"location":"LAB_access%20-%20bck/#3-accessing-linux-jumphost","text":"Open PuTTY client or any equivalent SSH client on your desktop. Enter following IP address, make sure SSH is the selected protocol. Computer: 198.18.133.10 User name: cisco User password: C1sco12345","title":"3. Accessing Linux Jumphost"},{"location":"LAB_access%20-%20bck/#4-accessing-cisco-container-platform","text":"Cisco Container Platform manages Kuberenetes clusters in the private infrasturcture. You will have access to dedicated instance of Cisco Container Platform, from which you will manage your own Kuberenetes Clusters used later on to deploy application. Please find login credentials and URL to your CCP instance below: URL: https://198.18.133.100 User name: admin User password: C1sco12345 You can explore CCP through the GUI, but please do not delete content already created.","title":"4. Accessing Cisco Container Platform"},{"location":"LAB_access%20-%20bck/#5-accessing-csr1kv-lab-router","text":"Your session contain Cisco CSR1kv router. It will be used to terminate Site-2-Site tunnels with AWS and GCP tenants. The VPN configuration will be generated by cloud platforms and then copied to the CLI. Please find login credentials and IP address to your CSR1kv router below - use SSH: Computer: 198.18.133.254 User name: admin User password: C1sco12345 Do not delete configuration already existing on the router.","title":"5. Accessing CSR1kv Lab router"},{"location":"LAB_access%20-%20bck/#6-accessing-vcenter-for-lab","text":"Whole setup is done on ESXi in Cisco dCloud environment. During the lab you don't need to perform actions on vCenter itself. However for case of troubleshooting or exploration, credentials to your vCenter below. URL: https://vc65.demo.dcloud.cisco.com/ui User name: administrator User password: C1sco12345! Do not delete configuration nor VM machines already existing on the vCenter.","title":"6. Accessing vCenter for Lab"},{"location":"LAB_access%20-%20bck/#7-amazon-web-services-access","text":"From Chrome web browser on your desktop, please open URL https://fwardzic.signin.aws.amazon.com/console Enter your credentials that you can find in credentials page. Once logged in, make sure you will use Frankfurt region (eu-central-1). This is important since VPN Site-2-Site tunnel will be established to that particular region.","title":"7. Amazon Web Services access"},{"location":"LAB_access/","text":"Connectivity Check 1. Lab access general description The lab has been built leveraging multiple cloud environments as following: Amazon Web Services Private intrastructure on-prem You will have access to Cisco Container Platform GUI, where you will setup new Kubernetes Clusters deployed in AWS and On-Prem. In the end you will manage your application that will be deployed in 2 different environments. In this lab you will see how to connect microservices together to make whole application work. Most of the tasks you will do from Linux Jumphost that is running on-premise. From there you will deploy components of your application in Kubernetes Cluster in AWS and on-prem. 2. Cisco dCloud dashboard The entire lab for the session is built using Cisco dCloud environment. To access it, you need to login to eXpo dashboard first. To do so, open https://dcloud2-rtp.cisco.com/expo/cla9t9xukip4x5qc9z7hygsu3/ in your browser. Select Exlore button to launch the lab session. New window popup will come. Please provide your e-mail address, check \"terms&conditions\" and click Continue button. Once you click Continue on the window above, you will get your session details. Now navigate to Details, where you find credentials to connect to dCloud VPN network. Cisco Anyconnect Mobility Client Run Cisco Anyconnect VPN client available on your desktop. Check credentials and URL in the dCloud session details as described in section above. You are connected to infrastructure on-prem. Access session with webRDP When you prefer to not install Cisco Anyconnect VPN Client on your PC, you have possibility to do whole LAB using webRDP session to terminal server installed. To do so, open Network section in your session. Once you see screen with devices, please launch Remote Desktop link. When you click on \"Remote Desktop button, browser will open new TAB with access to Windows desktop. The webRDP has installed Chrome as web browser, from where you get access to CCP GUI. WinSCP will be used to upload kubeconfig files to your Linux jump host. To access CSR router and Linux jumphost, use Putty installed - shortcut is on Desktop. Info Please do not use \"Remote Desktop\" for other devices from the list at Network tab. ONLY win2k16 can be accessed that way. Tip If you use webRDP you are still able to copy/paste between your 'main PC' and webRDP interface. You can use Guacamole interface - explained in Appendix: Guacamole. 3. Accessing Linux Jumphost Open PuTTY client or any equivalent SSH client on your desktop. Enter following IP address, make sure SSH is the selected protocol. Computer: 198.18.133.10 User name: cisco User password: C1sco12345 4. Accessing Cisco Container Platform Cisco Container Platform manages Kuberenetes clusters in the private infrasturcture. You will have access to dedicated instance of Cisco Container Platform, from which you will manage your own Kuberenetes Clusters used later on to deploy application. Please find login credentials and URL to your CCP instance below: URL: https://198.18.133.100 User name: admin User password: C1sco12345 Warning You can explore CCP through the GUI, but please do not delete content already created. 5. Accessing CSR1kv Lab router Your session contain Cisco CSR1kv router. It will be used to terminate Site-2-Site tunnels with AWS and GCP tenants. The VPN configuration will be generated by cloud platforms and then copied to the CLI. Please find login credentials and IP address to your CSR1kv router below - use SSH: Computer: 198.18.133.254 User name: admin User password: C1sco12345 Warning Do not delete configuration already existing on the router. 6. Accessing vCenter for Lab Whole setup is done on ESXi in Cisco dCloud environment. During the lab you don't need to perform actions on vCenter itself. However for case of troubleshooting or exploration, credentials to your vCenter below. URL: https://vc65.demo.dcloud.cisco.com/ui User name: administrator User password: C1sco12345! Warning Do not delete configuration nor VM machines already existing on the vCenter. 7. Amazon Web Services access From Chrome web browser on your desktop, please open URL https://console.aws.amazon.com/console/home?# please select IAM user, and paste account ID number provided by instructor Enter your credentials provided by instructor, account ID should be already populated with the value you provided in the previous screen. Warning Once logged in, please change region to Frankfurt region (eu-central-1)","title":"Accessing the Lab Environment"},{"location":"LAB_access/#connectivity-check","text":"","title":"Connectivity Check"},{"location":"LAB_access/#1-lab-access-general-description","text":"The lab has been built leveraging multiple cloud environments as following: Amazon Web Services Private intrastructure on-prem You will have access to Cisco Container Platform GUI, where you will setup new Kubernetes Clusters deployed in AWS and On-Prem. In the end you will manage your application that will be deployed in 2 different environments. In this lab you will see how to connect microservices together to make whole application work. Most of the tasks you will do from Linux Jumphost that is running on-premise. From there you will deploy components of your application in Kubernetes Cluster in AWS and on-prem.","title":"1. Lab access general description"},{"location":"LAB_access/#2-cisco-dcloud-dashboard","text":"The entire lab for the session is built using Cisco dCloud environment. To access it, you need to login to eXpo dashboard first. To do so, open https://dcloud2-rtp.cisco.com/expo/cla9t9xukip4x5qc9z7hygsu3/ in your browser. Select Exlore button to launch the lab session. New window popup will come. Please provide your e-mail address, check \"terms&conditions\" and click Continue button. Once you click Continue on the window above, you will get your session details. Now navigate to Details, where you find credentials to connect to dCloud VPN network.","title":"2. Cisco dCloud dashboard"},{"location":"LAB_access/#cisco-anyconnect-mobility-client","text":"Run Cisco Anyconnect VPN client available on your desktop. Check credentials and URL in the dCloud session details as described in section above. You are connected to infrastructure on-prem.","title":"Cisco Anyconnect Mobility Client"},{"location":"LAB_access/#access-session-with-webrdp","text":"When you prefer to not install Cisco Anyconnect VPN Client on your PC, you have possibility to do whole LAB using webRDP session to terminal server installed. To do so, open Network section in your session. Once you see screen with devices, please launch Remote Desktop link. When you click on \"Remote Desktop button, browser will open new TAB with access to Windows desktop. The webRDP has installed Chrome as web browser, from where you get access to CCP GUI. WinSCP will be used to upload kubeconfig files to your Linux jump host. To access CSR router and Linux jumphost, use Putty installed - shortcut is on Desktop. Info Please do not use \"Remote Desktop\" for other devices from the list at Network tab. ONLY win2k16 can be accessed that way. Tip If you use webRDP you are still able to copy/paste between your 'main PC' and webRDP interface. You can use Guacamole interface - explained in Appendix: Guacamole.","title":"Access session with webRDP"},{"location":"LAB_access/#3-accessing-linux-jumphost","text":"Open PuTTY client or any equivalent SSH client on your desktop. Enter following IP address, make sure SSH is the selected protocol. Computer: 198.18.133.10 User name: cisco User password: C1sco12345","title":"3. Accessing Linux Jumphost"},{"location":"LAB_access/#4-accessing-cisco-container-platform","text":"Cisco Container Platform manages Kuberenetes clusters in the private infrasturcture. You will have access to dedicated instance of Cisco Container Platform, from which you will manage your own Kuberenetes Clusters used later on to deploy application. Please find login credentials and URL to your CCP instance below: URL: https://198.18.133.100 User name: admin User password: C1sco12345 Warning You can explore CCP through the GUI, but please do not delete content already created.","title":"4. Accessing Cisco Container Platform"},{"location":"LAB_access/#5-accessing-csr1kv-lab-router","text":"Your session contain Cisco CSR1kv router. It will be used to terminate Site-2-Site tunnels with AWS and GCP tenants. The VPN configuration will be generated by cloud platforms and then copied to the CLI. Please find login credentials and IP address to your CSR1kv router below - use SSH: Computer: 198.18.133.254 User name: admin User password: C1sco12345 Warning Do not delete configuration already existing on the router.","title":"5. Accessing CSR1kv Lab router"},{"location":"LAB_access/#6-accessing-vcenter-for-lab","text":"Whole setup is done on ESXi in Cisco dCloud environment. During the lab you don't need to perform actions on vCenter itself. However for case of troubleshooting or exploration, credentials to your vCenter below. URL: https://vc65.demo.dcloud.cisco.com/ui User name: administrator User password: C1sco12345! Warning Do not delete configuration nor VM machines already existing on the vCenter.","title":"6. Accessing vCenter for Lab"},{"location":"LAB_access/#7-amazon-web-services-access","text":"From Chrome web browser on your desktop, please open URL https://console.aws.amazon.com/console/home?# please select IAM user, and paste account ID number provided by instructor Enter your credentials provided by instructor, account ID should be already populated with the value you provided in the previous screen. Warning Once logged in, please change region to Frankfurt region (eu-central-1)","title":"7. Amazon Web Services access"},{"location":"backend_exercise/","text":"Explore Backend App and Kubernetes Dashboard: Your Hybrid Cloud App's backend compoents (MariaDB, MQTT DB Agent, and REST API agent) should be up and running. Now let's explore some low level details to understand the application containers and kubernetes better. Task 0: Tasks in this section will be executed in Kubernetes on-premise. Change kubectl context to the correct one. Execute command below on linux jumphost: kubectl config use-context admin@on-prem-backend Task 1: Find the size of the 'Persistent Volume Claim' used for MariaDB database? To check 'CAPACITY' value of PVC execute this command on linux jumphost: kubectl get pvc mariadb-pv-claim Task 2: Login to the MariaDB database and explore the data tables. Find the pod name for 'MariaDB' using the kubectl command kubectl get pods and locate the pod name starting with 'iot-backend-mariadb-'. Login to the MariaDB container using the kubectl command kubectl exec -it <pod name> /bin/bash Tip (replace the <pod name> in command above with correct value). Inside the container execute ss -tulpn and check the port MariaDB is listing to for incoming connections. You can also check the version of MariaDB using the shell command: mysql --version Now connect to the MariaDB from the container shell and check Databases and Tables. Use mysql -u root -pcisco123 command to login to MaraDB. On mariaDB shell, use show databases; command to list all the databases. Look for ' sensor_db ' in the output (this is the database where we are storing the sensor data). Switch to sensor_db using the command use sensor_db; and list all the tables in this database using the command show tables; You should see only one table with the name 'sensor_data' ; Try to list the data from this table using the SQL statement select * from sensor_data; Now check the record count in this table using the sql statement: select count(*) from sensor_data; Repeat the SQL from step 8 several times and check if the record count is increasing (each sensor would send the data after every 10 seconds). Use exit command at MariaDB prompt to exit the DB shell. Use exit command again to exit 'iot-backend-mariadb' container shell. Task 3: Connect to the REST API Agent container and find the port it is listing on for incoming REST calls. Find the pod name for 'REST API Agent' using the kubectl command kubectl get pods and locate the pod name starting with 'iot-backend-rest-api-agent-' (you may see more than one pod for this service, pick any one). login to the REST API Agent container using the kubectl command kubectl exec -it <pod name> /bin/ash Tip (replace the <pod name> in command above with correct value). Execute netstat -an command on the container shell and check the output. This container listens on port '5050' for incoming REST connections and connects with MariaDB using port '3306' (Connection to DB will timeout in case there are no requests coming in). Use exit command to come out of the container shell. Task 4: Check the logs messages from 'REST API Agent'. Find the pod name for 'REST API Agent' using the kubectl command kubectl get pods and locate the pod name starting with 'iot-backend-rest-api-agent-' (you may see more than one pod for this service, pick any one). Check the logs using the kubectl command kubectl logs <pod name> Tip (replace the <pod name> in command above with correct value). Task 5: How the traffic would be distributed, if you have multiple Kubernetes Pods behind a Kubernetes NodePort Service? Right click on top of your putty window and click on \"Duplicate Session\" (You should have two putty windows side by side logged into same linux jumphost). In the first putty window (logged into linux jumphost) execute the command kubectl get nodes -o wide and note down the Kubernetes master node's external IP address. Open a web browser on your machine and access the following url (NAT the master node ip before in 1:1 example: 10.200.0.158 -> 198.18.133.158 and then use in the url) - http://<kubernetes node's external ip NAT>:<port>/temperature In the first putty window, run kubectl get pods on the linux jumphost. In the output you should see 2 pods for 'iot-backend-rest-api-agent'. In the first putty window, run the command watch kubectl logs <first pod name> (Use the first 'iot-backend-rest-api-agent' pod out of 2 pods that you saw in the output of the command executed in step#5). In the Second putty window, display the logs for second 'iot-backend-rest-api-agent' pod using the command watch kubectl logs <second pod name> Refresh or reload the web page with the url you used in step 4. Repeat step 8 and check the requests hitting the kubernetes pods in the log messages.","title":"Explore Backend Application"},{"location":"backend_exercise/#explore-backend-app-and-kubernetes-dashboard","text":"Your Hybrid Cloud App's backend compoents (MariaDB, MQTT DB Agent, and REST API agent) should be up and running. Now let's explore some low level details to understand the application containers and kubernetes better. Task 0: Tasks in this section will be executed in Kubernetes on-premise. Change kubectl context to the correct one. Execute command below on linux jumphost: kubectl config use-context admin@on-prem-backend Task 1: Find the size of the 'Persistent Volume Claim' used for MariaDB database? To check 'CAPACITY' value of PVC execute this command on linux jumphost: kubectl get pvc mariadb-pv-claim Task 2: Login to the MariaDB database and explore the data tables. Find the pod name for 'MariaDB' using the kubectl command kubectl get pods and locate the pod name starting with 'iot-backend-mariadb-'. Login to the MariaDB container using the kubectl command kubectl exec -it <pod name> /bin/bash Tip (replace the <pod name> in command above with correct value). Inside the container execute ss -tulpn and check the port MariaDB is listing to for incoming connections. You can also check the version of MariaDB using the shell command: mysql --version Now connect to the MariaDB from the container shell and check Databases and Tables. Use mysql -u root -pcisco123 command to login to MaraDB. On mariaDB shell, use show databases; command to list all the databases. Look for ' sensor_db ' in the output (this is the database where we are storing the sensor data). Switch to sensor_db using the command use sensor_db; and list all the tables in this database using the command show tables; You should see only one table with the name 'sensor_data' ; Try to list the data from this table using the SQL statement select * from sensor_data; Now check the record count in this table using the sql statement: select count(*) from sensor_data; Repeat the SQL from step 8 several times and check if the record count is increasing (each sensor would send the data after every 10 seconds). Use exit command at MariaDB prompt to exit the DB shell. Use exit command again to exit 'iot-backend-mariadb' container shell. Task 3: Connect to the REST API Agent container and find the port it is listing on for incoming REST calls. Find the pod name for 'REST API Agent' using the kubectl command kubectl get pods and locate the pod name starting with 'iot-backend-rest-api-agent-' (you may see more than one pod for this service, pick any one). login to the REST API Agent container using the kubectl command kubectl exec -it <pod name> /bin/ash Tip (replace the <pod name> in command above with correct value). Execute netstat -an command on the container shell and check the output. This container listens on port '5050' for incoming REST connections and connects with MariaDB using port '3306' (Connection to DB will timeout in case there are no requests coming in). Use exit command to come out of the container shell. Task 4: Check the logs messages from 'REST API Agent'. Find the pod name for 'REST API Agent' using the kubectl command kubectl get pods and locate the pod name starting with 'iot-backend-rest-api-agent-' (you may see more than one pod for this service, pick any one). Check the logs using the kubectl command kubectl logs <pod name> Tip (replace the <pod name> in command above with correct value). Task 5: How the traffic would be distributed, if you have multiple Kubernetes Pods behind a Kubernetes NodePort Service? Right click on top of your putty window and click on \"Duplicate Session\" (You should have two putty windows side by side logged into same linux jumphost). In the first putty window (logged into linux jumphost) execute the command kubectl get nodes -o wide and note down the Kubernetes master node's external IP address. Open a web browser on your machine and access the following url (NAT the master node ip before in 1:1 example: 10.200.0.158 -> 198.18.133.158 and then use in the url) - http://<kubernetes node's external ip NAT>:<port>/temperature In the first putty window, run kubectl get pods on the linux jumphost. In the output you should see 2 pods for 'iot-backend-rest-api-agent'. In the first putty window, run the command watch kubectl logs <first pod name> (Use the first 'iot-backend-rest-api-agent' pod out of 2 pods that you saw in the output of the command executed in step#5). In the Second putty window, display the logs for second 'iot-backend-rest-api-agent' pod using the command watch kubectl logs <second pod name> Refresh or reload the web page with the url you used in step 4. Repeat step 8 and check the requests hitting the kubernetes pods in the log messages.","title":"Explore Backend App and Kubernetes Dashboard:"},{"location":"basic_kubectl_cmds/","text":"kubectl version kubectl get nodes kubectl run --image= --port= Kubernetes Deployments: A Deployment controller provides declarative updates for Pods and ReplicaSets. You describe a desired state in a Deployment object, and the Deployment controller changes the actual state to the desired state at a controlled rate. You can define Deployments to create new ReplicaSets, or to remove existing Deployments and adopt all their resources with new Deployments. Create Deployment Use the following command to create a deployment using a yaml file - kubectl create -f <yaml file path> List Deployments: Use the following command to list all kubernetes deployments - kubectl get deployments Deployment Details: Use the following command to display the details of deployment - kubectl describe deployment Scale Deployment: Use the following command to scale (up/down) a kubernetes deployment - kubectl scale deployments/<deployment_name> --replicas=<number of replicas> Delete Deployment: Use the following command to delete a deployment - kubectl delete deployment <deployment_name> Kubernetes Pods: A Pod represents a unit of deployment: a single instance of an application in Kubernetes, which might consist of either a single container or a small number of containers that are tightly coupled and that share resources. List Pods: Use the following command to list all the pods: kubectl get pods or use wide option to see more details - kubectl get pods -o wide List Pods Filter: You can filter the pods using the labels used in deployment - kubectl get pods -l <label_name>=<label_value> Pod Details: Use the following command to see the containers and used images for pods - kubectl describe pods Pod Logs: Use the following command to check the pod logs - kubectl logs <pod_name> Kubernetes Services: A Kubernetes Service is an abstraction which defines a logical set of Pods and a policy by which to access them - sometimes called a micro-service. List Service: Use the following command to list the current Services - kubectl get services You can filter the services using the labels used in deployment - kubectl get services -l <label_name>=<label_value> Create Service: Use the following command to create a new service - kubectl expose deployment/<deployment_name> --type=\"NodePort\" --port <port> Service Details: Use the following command to find out what port was opened externally (by the NodePort option) - kubectl describe services/<service_name> Delete Service: Use the following command to delete a service - kubectl delete service/<service_name> or kubectl delete service -l <label_name>=<label_value> Parse Service Node Port: Use the following script to filterout the node-port of a service (change the service name) - export NODE_PORT=$(kubectl get services/<service-name> -o go-template='{{(index .spec.ports 0).nodePort}}') echo NODE_PORT=$NODE_PORT Kubernetes Secrets: A Secret is an object that stores a piece of sensitive data like a password or key. List Secrets: Use the following command to list all secrets kubectl get secrets Secret Details: Use the following command to list the secret details - kubectl describe secrets/<secret_name> Create Secret: Use the following command to create secret - kubectl create secret generic <secret_name> --from-literal=<key_name>=<key_value> Delete Secret: use the following command to delete a secret - kubectl delete secret <secret_name> Interacting with Pod Containers List Env Variables: Use the following command to list the environment variables - kubectl exec <pod_name> env Access Container Shell: Use the following command to access bash shell in a container - kubectl exec -ti <pod_name> bash Note: To close your container connection type ' exit '. ========================================= kubectl proxy curl http://localhost:8001/version export POD_NAME=$(kubectl get pods -o go-template --template '{{range .items}}{{.metadata.name}}{{\"\\n\"}}{{end}}') echo Name of the Pod: $POD_NAME curl http://localhost:8001/api/v1/namespaces/default/pods/$POD_NAME/proxy =========================================","title":"Basic kubectl cmds"},{"location":"basic_kubectl_cmds/#kubernetes-deployments","text":"A Deployment controller provides declarative updates for Pods and ReplicaSets. You describe a desired state in a Deployment object, and the Deployment controller changes the actual state to the desired state at a controlled rate. You can define Deployments to create new ReplicaSets, or to remove existing Deployments and adopt all their resources with new Deployments. Create Deployment Use the following command to create a deployment using a yaml file - kubectl create -f <yaml file path> List Deployments: Use the following command to list all kubernetes deployments - kubectl get deployments Deployment Details: Use the following command to display the details of deployment - kubectl describe deployment Scale Deployment: Use the following command to scale (up/down) a kubernetes deployment - kubectl scale deployments/<deployment_name> --replicas=<number of replicas> Delete Deployment: Use the following command to delete a deployment - kubectl delete deployment <deployment_name>","title":"Kubernetes Deployments:"},{"location":"basic_kubectl_cmds/#kubernetes-pods","text":"A Pod represents a unit of deployment: a single instance of an application in Kubernetes, which might consist of either a single container or a small number of containers that are tightly coupled and that share resources. List Pods: Use the following command to list all the pods: kubectl get pods or use wide option to see more details - kubectl get pods -o wide List Pods Filter: You can filter the pods using the labels used in deployment - kubectl get pods -l <label_name>=<label_value> Pod Details: Use the following command to see the containers and used images for pods - kubectl describe pods Pod Logs: Use the following command to check the pod logs - kubectl logs <pod_name>","title":"Kubernetes Pods:"},{"location":"basic_kubectl_cmds/#kubernetes-services","text":"A Kubernetes Service is an abstraction which defines a logical set of Pods and a policy by which to access them - sometimes called a micro-service. List Service: Use the following command to list the current Services - kubectl get services You can filter the services using the labels used in deployment - kubectl get services -l <label_name>=<label_value> Create Service: Use the following command to create a new service - kubectl expose deployment/<deployment_name> --type=\"NodePort\" --port <port> Service Details: Use the following command to find out what port was opened externally (by the NodePort option) - kubectl describe services/<service_name> Delete Service: Use the following command to delete a service - kubectl delete service/<service_name> or kubectl delete service -l <label_name>=<label_value> Parse Service Node Port: Use the following script to filterout the node-port of a service (change the service name) - export NODE_PORT=$(kubectl get services/<service-name> -o go-template='{{(index .spec.ports 0).nodePort}}') echo NODE_PORT=$NODE_PORT","title":"Kubernetes Services:"},{"location":"basic_kubectl_cmds/#kubernetes-secrets","text":"A Secret is an object that stores a piece of sensitive data like a password or key. List Secrets: Use the following command to list all secrets kubectl get secrets Secret Details: Use the following command to list the secret details - kubectl describe secrets/<secret_name> Create Secret: Use the following command to create secret - kubectl create secret generic <secret_name> --from-literal=<key_name>=<key_value> Delete Secret: use the following command to delete a secret - kubectl delete secret <secret_name>","title":"Kubernetes Secrets:"},{"location":"basic_kubectl_cmds/#interacting-with-pod-containers","text":"List Env Variables: Use the following command to list the environment variables - kubectl exec <pod_name> env Access Container Shell: Use the following command to access bash shell in a container - kubectl exec -ti <pod_name> bash Note: To close your container connection type ' exit '.","title":"Interacting with Pod Containers"},{"location":"basic_kubectl_cmds/#_1","text":"kubectl proxy curl http://localhost:8001/version export POD_NAME=$(kubectl get pods -o go-template --template '{{range .items}}{{.metadata.name}}{{\"\\n\"}}{{end}}') echo Name of the Pod: $POD_NAME curl http://localhost:8001/api/v1/namespaces/default/pods/$POD_NAME/proxy =========================================","title":"========================================="},{"location":"ccp_accessing_kubernetes_cluster/","text":"Accessing Kubernetes Cluster Kubectl - Kubernetes Command Line Interface Getting kubeconfig file from both created Kubernetes Clusters While most of the options in Kubernetes are available through the dashboard, CLI commands are also available, and convenient to use. Kubectl is a software leveraging Kubernetes API and translate commands to specific API calls. This tool will be used during the lab. Once connectivity to your new EKS Kubernetes Cluster is confirmed, go back to Cisco Container Platform (CCP) web page, login if required and select Clusters -> vSphere to display your newely created Kubernetes cluster. Step 1: Accessing new Kubernetes cluster requires to obtain the kubeconfig file. Click on the name of your cluster to see the detailed view. Step 2: Download kubeconfig file, make sure the file name is on-prem-backend-kubeconfig.yaml . Step 3: Copy kubeconfig file to your linux jumphost machine using SCP. Open WinSCP from the desktop, and login to Linux Jumphost using your credentials. Copy kubeconfig file into your home directory (this is the directory opened after successful login) Time to copy second kubeconfig for your AWS Kubernetes Cluster. On CCP Dashboard go to Clusters -> AWS to display your newely created Kubernetes EKS cluster. Step 1: Accessing new Kubernetes cluster requires to obtain the kubeconfig file. Click on the name of your cluster to see the detailed view. Step 2: Copy kubeconfig file to your linux jumphost machine using SCP. Open WinSCP from the desktop, and login to Linux Jumphost using your credentials. Copy kubeconfig file into your home directory (this is the directory opened after successful login) Merge EKS kubeconfig file with existing kubeconfig file for on-premise Kubernetes. SSH to Linux jumphost using PuTTY, confirm that BOTH your kubeconfig files have been copied to home directory ls -lt Merging kubeconfig files let you switch between contexts using kubectl tool, rather specifying full path to your kubeconfig file in every command. This command will merge kubeconfig files and create new file in ~/.kube/config . This is the default location for kubectl to find access information to particular Kubernetes Cluster. Tip Before you merge the kubeconfig files, please verify file extention. It can happen that it is saved as .yml not .yaml . Correct next command when necessary. Merge command: KUBECONFIG=~/on-prem-backend-kubeconfig.yaml:~/kubeconfig.yaml kubectl config view --flatten > ~/.kube/config List files again in ~/.kube/config directory to make sure that new config file is there. cd .kube ls -la IAM AWS Authentication for EKS cluster Step 1: Kubeconfig file usually contains private certificate that is uathorized by kubernetes directly. In case of EKS, AWS IAM authenthentication is involved. Instead of certificate, we will use your AWS access keypair to authenticate. On the linux jumphost, aws cli tool is installed. Similarily to kubectl this tool provides management access to your AWS account. Using this tool you can spawn new VM or create new networking infrastructure like VPC, subnets etc. In this lab, aws cli is leveraged by IAM-Authenticator module to sign kubectl requests towards EKS using Secure Token Service (STS) in AWS. Step 2: Configure aws cli with your access key. In Linux jumphost console, type: aws configure Provide the Access Key and Secret obtained in the previous task (AWS Tenant Cluster creation), you should have them saved in notepad. AWS Access Key ID [None]: AWS Secret Access Key [None]: Default region name [None]: eu-central-1 Default output format [None]: text Make sure to enter exact name of region: eu-central-1 and text as a default output format. Validate access to K8s Clusters from jumphost Validate if your kubeconfigs has been correctly imported: Info For AWS EKS Cluster, as well as for on-prem you will have full admin rights. List updated contexts: kubectl config get-contexts Info Star indicates which kubernetes cluster you will manage using kubectl If you would like to change context to aws EKS Kubernetes Cluster, please use following command: kubectl config use-context aws kubectl config get-contexts Validate your access and list Kubernetes nodes in AWS EKS Cluster kubectl get nodes To change context to on-premise Kuberenetes Cluster use follwing command: kubectl config use-context admin@on-prem-backend kubectl config get-contexts Validate your access and list Kubernetes nodes in on-premise Cluster kubectl get nodes","title":"Access Tenant Cluster"},{"location":"ccp_accessing_kubernetes_cluster/#accessing-kubernetes-cluster","text":"","title":"Accessing Kubernetes Cluster"},{"location":"ccp_accessing_kubernetes_cluster/#kubectl-kubernetes-command-line-interface","text":"","title":"Kubectl - Kubernetes Command Line Interface"},{"location":"ccp_accessing_kubernetes_cluster/#getting-kubeconfig-file-from-both-created-kubernetes-clusters","text":"While most of the options in Kubernetes are available through the dashboard, CLI commands are also available, and convenient to use. Kubectl is a software leveraging Kubernetes API and translate commands to specific API calls. This tool will be used during the lab. Once connectivity to your new EKS Kubernetes Cluster is confirmed, go back to Cisco Container Platform (CCP) web page, login if required and select Clusters -> vSphere to display your newely created Kubernetes cluster. Step 1: Accessing new Kubernetes cluster requires to obtain the kubeconfig file. Click on the name of your cluster to see the detailed view. Step 2: Download kubeconfig file, make sure the file name is on-prem-backend-kubeconfig.yaml . Step 3: Copy kubeconfig file to your linux jumphost machine using SCP. Open WinSCP from the desktop, and login to Linux Jumphost using your credentials. Copy kubeconfig file into your home directory (this is the directory opened after successful login) Time to copy second kubeconfig for your AWS Kubernetes Cluster. On CCP Dashboard go to Clusters -> AWS to display your newely created Kubernetes EKS cluster. Step 1: Accessing new Kubernetes cluster requires to obtain the kubeconfig file. Click on the name of your cluster to see the detailed view. Step 2: Copy kubeconfig file to your linux jumphost machine using SCP. Open WinSCP from the desktop, and login to Linux Jumphost using your credentials. Copy kubeconfig file into your home directory (this is the directory opened after successful login)","title":"Getting kubeconfig file from both created Kubernetes Clusters"},{"location":"ccp_accessing_kubernetes_cluster/#merge-eks-kubeconfig-file-with-existing-kubeconfig-file-for-on-premise-kubernetes","text":"SSH to Linux jumphost using PuTTY, confirm that BOTH your kubeconfig files have been copied to home directory ls -lt Merging kubeconfig files let you switch between contexts using kubectl tool, rather specifying full path to your kubeconfig file in every command. This command will merge kubeconfig files and create new file in ~/.kube/config . This is the default location for kubectl to find access information to particular Kubernetes Cluster. Tip Before you merge the kubeconfig files, please verify file extention. It can happen that it is saved as .yml not .yaml . Correct next command when necessary. Merge command: KUBECONFIG=~/on-prem-backend-kubeconfig.yaml:~/kubeconfig.yaml kubectl config view --flatten > ~/.kube/config List files again in ~/.kube/config directory to make sure that new config file is there. cd .kube ls -la","title":"Merge EKS kubeconfig file with existing kubeconfig file for on-premise Kubernetes."},{"location":"ccp_accessing_kubernetes_cluster/#iam-aws-authentication-for-eks-cluster","text":"Step 1: Kubeconfig file usually contains private certificate that is uathorized by kubernetes directly. In case of EKS, AWS IAM authenthentication is involved. Instead of certificate, we will use your AWS access keypair to authenticate. On the linux jumphost, aws cli tool is installed. Similarily to kubectl this tool provides management access to your AWS account. Using this tool you can spawn new VM or create new networking infrastructure like VPC, subnets etc. In this lab, aws cli is leveraged by IAM-Authenticator module to sign kubectl requests towards EKS using Secure Token Service (STS) in AWS. Step 2: Configure aws cli with your access key. In Linux jumphost console, type: aws configure Provide the Access Key and Secret obtained in the previous task (AWS Tenant Cluster creation), you should have them saved in notepad. AWS Access Key ID [None]: AWS Secret Access Key [None]: Default region name [None]: eu-central-1 Default output format [None]: text Make sure to enter exact name of region: eu-central-1 and text as a default output format.","title":"IAM AWS Authentication for EKS cluster"},{"location":"ccp_accessing_kubernetes_cluster/#validate-access-to-k8s-clusters-from-jumphost","text":"Validate if your kubeconfigs has been correctly imported: Info For AWS EKS Cluster, as well as for on-prem you will have full admin rights. List updated contexts: kubectl config get-contexts Info Star indicates which kubernetes cluster you will manage using kubectl If you would like to change context to aws EKS Kubernetes Cluster, please use following command: kubectl config use-context aws kubectl config get-contexts Validate your access and list Kubernetes nodes in AWS EKS Cluster kubectl get nodes To change context to on-premise Kuberenetes Cluster use follwing command: kubectl config use-context admin@on-prem-backend kubectl config get-contexts Validate your access and list Kubernetes nodes in on-premise Cluster kubectl get nodes","title":"Validate access to K8s Clusters from jumphost"},{"location":"ccp_connect_eks_to_hybrid_network/","text":"Connect AWS EKS Kubernetes Cluster to Hybrid Network After successfully created EKS cluster, now it is a time to connect it to VPN connection towards On-Premise Data Center. CCP created dedicated VPC for every Kubernetes EKS Cluster. You can now start working on it and deploying applicaitons, however your services and applications will be exposed to public Internet. In our scenario, we will be deploying backend part of our application in EKS cluster that needs to have connectivity to database that you will deploy on-premise. In order to provider secure connectivity between application components, we would need to establish IPSec Tunnel between AWS and on-premise data center. The VPC has an attached virtual private gateway, and your on-premises (remote) network includes a customer gateway device, which you must configure to enable the Site-to-Site VPN connection. You set up the routing so that any traffic from the VPC bound for your network is routed to the virtual private gateway. Site-to-Site VPN is used to build eBGP connection between Amazon and on-prem router to exchange networks used for Kubernetes services in secured manner. This solution is well described in the following white paper: https://docs.aws.amazon.com/vpn/latest/s2svpn/VPC_VPN.html In our excercise you will need to: Define Virtual Private Gateway Bound it with VPC created by CCP Create Site-to-Site Tunnel Paste configuration to your on-prem CSR1000v - configuration will be provided by AWS Create AWS VPN Gateway Step 1. Open AWS dashboard, and select VPC service, as on the following picture: Next check the VPC has been created with the name you have provided in CCP (aws-student-XX). Info Make sure your AWS web console is set to use Frankfurt (eu-central-1) region. Step 2. Create new Virtual Private Gateway From the left menu, scrol down and select Virtual Private Gateways Press button Create Virtual Private Gateway Enter your username as a name of Virtual Private Gateway (studentXX) and leave default ASN selected (autonomous system number for BGP). Once create successfully you should see similar screen Step 3. Attach Virtual Private Gateway to your VPC. Select your newely created Gateway and click on Actions menu. Select attach to VPC option. In the new window select your VPC to which Virtual Private Gateway will be attached. You can see attaching status, you can refresh page by clicking refresh button in the right top corner, the status should change to attached for your Virtual Private Gateway. Create Site-to-Site VPN Connection Select Site-to-Site VPN Connections section from left navigation panel. Click on Create VPN Connection as shown on figure below. New configuration wizard will open in the browser. Configure your new VPN Tunnel as follows. Name tag, use studentID i.e. student01. In Virtual Private Gateway select previously created VGW, search for studentXX . Next, select Customer Gateway ID . From drop down menu select cgw-lon-dcloud . Leave rest of the Site-2-Site VPN attributes unchanged. Scroll down to the bottom of the screen and click Create VPN Connection . Info Creation time for site-to-site VPN may take several minutes, use refresh button to check if state has changed. Once completed, select your newly created Site-2-Site VPN from the list. Once done click on button above Download Configuration . Click Download Configuration Select Cisco Systems, Inc. from the Vendor drop-down menu Select CSRv AMI from Platform list Leave software version selected Click Download button Save file on your PC or on Virtual RDP. Tip When you use webRDP, please use WordPad to edit the configuration file. When file is downloaded, open it using text file editor. You will see IOS-XE configuration for your ON-PREM router. In be HEADER section you will find <interface_name/private_IP_on_outside_interface> which has to be replaced in entire file. Using Replace function in your text editor, please replace the mentioned phrase by IP address of your CSR1000v Router: <interface_name/private_IP_on_outside_interface> 198.18.133.254 Now you can copy/paste configuration of Site-2-Site VPN to your on-prem Cisco CRS1kv router ssh admin@198.18.133.254 . Hint In the configuration template you will find command: crypto isakmp keepalive threshold 10 retry 10 . You will get an error when paste it to CSR CLI. You can ignore it or correct it: crypto isakmp keepalive threshold 10 10 . Please copy not-commented lines. You can read comments, which describe each part of configuration. Finally, when copied you will have TWO Site-2-Site VPN tunnels UP and TWO eBGP Sessions UP with AWS peers. Check status of the IPSec tunnels: show crypto session Check BGP peering show ip bgp summary - you should receive 1 prefix from each peer. show ip bgp summary On the AWS Dashboard you will see eBGP Peers UP in that section. Tunnels are UP, eBGP is UP and routes are exchanged. Next section is about steering traffic to right VPN tunnel for on-prem subnets. Add routes for return traffic to on-premise Data Center Step 1. Select your Route Tables from the left menu, and find your VPC. Step 2. Edit VPC Main Routing Table. Check in which row the Main column contains Yes . Make sure you selected Main route table for your VPC and not the default VPC . Select that route table (usually it does not have a name) and select Routes tab from the bottom panel. Click on Edit routes button here. Step 3. Add route to On-Premise data center. Enter following parameters: Prefix: 10.200.0.0/24 Target: type vgw- and you will find your Virtual Private Gateway created earlier. Click Save routes Info Make sure you will see 2 routes in this route table, like in the picture below Step 4. Add the same return route under subnets . From the panel above select aws-student-XX-private-route-table1 only and add the same subnet as in Step 3. Step 5. Add route to On-Premise data center. Enter following parameters: Prefix: 10.200.0.0/24 Target: type vgw- and you will find your Virtual Private Gateway created earlier. Info Make sure you will see 3 routes in this route table, like in the picture below Click Save routes Update Security Groups rules to allow traffic to on-premise Data Center Step 1. Select EC2 service Step 2. You should see only one ec2 instance, which is a worker node of the Kubernetes cluster. Step 3. Update EC2 security group. Select your EC2 instance, on the bottom details area please click on link to edit Security Groups associated to this VM instance. you will be redirected to Security Groups dashboard where you should select Inbound and then click on Edit rules button. Step 4. Add new rule to allow ingress traffic to EC2 instance from On-Premise prefix. Add new rule with following parameters: Type: All traffic Source: 10.200.0.0/24 Click Save. Validate connectivity From Linux jumphost machine, ping IP address of your EC2 instance. To find out your EC2 instance IP address, go to EC2 dashboard and select Instances and your instance (1). In the bottom panel in Details tab (2), look for the IPv4 address (3) associated with the DNS name (4). You can try to ping this IP address from Linux Jumphost.","title":"Connect Cluster in AWS to Hybrid Network"},{"location":"ccp_connect_eks_to_hybrid_network/#connect-aws-eks-kubernetes-cluster-to-hybrid-network","text":"After successfully created EKS cluster, now it is a time to connect it to VPN connection towards On-Premise Data Center. CCP created dedicated VPC for every Kubernetes EKS Cluster. You can now start working on it and deploying applicaitons, however your services and applications will be exposed to public Internet. In our scenario, we will be deploying backend part of our application in EKS cluster that needs to have connectivity to database that you will deploy on-premise. In order to provider secure connectivity between application components, we would need to establish IPSec Tunnel between AWS and on-premise data center. The VPC has an attached virtual private gateway, and your on-premises (remote) network includes a customer gateway device, which you must configure to enable the Site-to-Site VPN connection. You set up the routing so that any traffic from the VPC bound for your network is routed to the virtual private gateway. Site-to-Site VPN is used to build eBGP connection between Amazon and on-prem router to exchange networks used for Kubernetes services in secured manner. This solution is well described in the following white paper: https://docs.aws.amazon.com/vpn/latest/s2svpn/VPC_VPN.html In our excercise you will need to: Define Virtual Private Gateway Bound it with VPC created by CCP Create Site-to-Site Tunnel Paste configuration to your on-prem CSR1000v - configuration will be provided by AWS","title":"Connect AWS EKS Kubernetes Cluster to Hybrid Network"},{"location":"ccp_connect_eks_to_hybrid_network/#create-aws-vpn-gateway","text":"Step 1. Open AWS dashboard, and select VPC service, as on the following picture: Next check the VPC has been created with the name you have provided in CCP (aws-student-XX). Info Make sure your AWS web console is set to use Frankfurt (eu-central-1) region. Step 2. Create new Virtual Private Gateway From the left menu, scrol down and select Virtual Private Gateways Press button Create Virtual Private Gateway Enter your username as a name of Virtual Private Gateway (studentXX) and leave default ASN selected (autonomous system number for BGP). Once create successfully you should see similar screen Step 3. Attach Virtual Private Gateway to your VPC. Select your newely created Gateway and click on Actions menu. Select attach to VPC option. In the new window select your VPC to which Virtual Private Gateway will be attached. You can see attaching status, you can refresh page by clicking refresh button in the right top corner, the status should change to attached for your Virtual Private Gateway.","title":"Create AWS VPN Gateway"},{"location":"ccp_connect_eks_to_hybrid_network/#create-site-to-site-vpn-connection","text":"Select Site-to-Site VPN Connections section from left navigation panel. Click on Create VPN Connection as shown on figure below. New configuration wizard will open in the browser. Configure your new VPN Tunnel as follows. Name tag, use studentID i.e. student01. In Virtual Private Gateway select previously created VGW, search for studentXX . Next, select Customer Gateway ID . From drop down menu select cgw-lon-dcloud . Leave rest of the Site-2-Site VPN attributes unchanged. Scroll down to the bottom of the screen and click Create VPN Connection . Info Creation time for site-to-site VPN may take several minutes, use refresh button to check if state has changed. Once completed, select your newly created Site-2-Site VPN from the list. Once done click on button above Download Configuration . Click Download Configuration Select Cisco Systems, Inc. from the Vendor drop-down menu Select CSRv AMI from Platform list Leave software version selected Click Download button Save file on your PC or on Virtual RDP. Tip When you use webRDP, please use WordPad to edit the configuration file. When file is downloaded, open it using text file editor. You will see IOS-XE configuration for your ON-PREM router. In be HEADER section you will find <interface_name/private_IP_on_outside_interface> which has to be replaced in entire file. Using Replace function in your text editor, please replace the mentioned phrase by IP address of your CSR1000v Router: <interface_name/private_IP_on_outside_interface> 198.18.133.254 Now you can copy/paste configuration of Site-2-Site VPN to your on-prem Cisco CRS1kv router ssh admin@198.18.133.254 . Hint In the configuration template you will find command: crypto isakmp keepalive threshold 10 retry 10 . You will get an error when paste it to CSR CLI. You can ignore it or correct it: crypto isakmp keepalive threshold 10 10 . Please copy not-commented lines. You can read comments, which describe each part of configuration. Finally, when copied you will have TWO Site-2-Site VPN tunnels UP and TWO eBGP Sessions UP with AWS peers. Check status of the IPSec tunnels: show crypto session Check BGP peering show ip bgp summary - you should receive 1 prefix from each peer. show ip bgp summary On the AWS Dashboard you will see eBGP Peers UP in that section. Tunnels are UP, eBGP is UP and routes are exchanged. Next section is about steering traffic to right VPN tunnel for on-prem subnets.","title":"Create Site-to-Site VPN Connection"},{"location":"ccp_connect_eks_to_hybrid_network/#add-routes-for-return-traffic-to-on-premise-data-center","text":"Step 1. Select your Route Tables from the left menu, and find your VPC. Step 2. Edit VPC Main Routing Table. Check in which row the Main column contains Yes . Make sure you selected Main route table for your VPC and not the default VPC . Select that route table (usually it does not have a name) and select Routes tab from the bottom panel. Click on Edit routes button here. Step 3. Add route to On-Premise data center. Enter following parameters: Prefix: 10.200.0.0/24 Target: type vgw- and you will find your Virtual Private Gateway created earlier. Click Save routes Info Make sure you will see 2 routes in this route table, like in the picture below Step 4. Add the same return route under subnets . From the panel above select aws-student-XX-private-route-table1 only and add the same subnet as in Step 3. Step 5. Add route to On-Premise data center. Enter following parameters: Prefix: 10.200.0.0/24 Target: type vgw- and you will find your Virtual Private Gateway created earlier. Info Make sure you will see 3 routes in this route table, like in the picture below Click Save routes","title":"Add routes for return traffic to on-premise Data Center"},{"location":"ccp_connect_eks_to_hybrid_network/#update-security-groups-rules-to-allow-traffic-to-on-premise-data-center","text":"Step 1. Select EC2 service Step 2. You should see only one ec2 instance, which is a worker node of the Kubernetes cluster. Step 3. Update EC2 security group. Select your EC2 instance, on the bottom details area please click on link to edit Security Groups associated to this VM instance. you will be redirected to Security Groups dashboard where you should select Inbound and then click on Edit rules button. Step 4. Add new rule to allow ingress traffic to EC2 instance from On-Premise prefix. Add new rule with following parameters: Type: All traffic Source: 10.200.0.0/24 Click Save.","title":"Update Security Groups rules to allow traffic to on-premise Data Center"},{"location":"ccp_connect_eks_to_hybrid_network/#validate-connectivity","text":"From Linux jumphost machine, ping IP address of your EC2 instance. To find out your EC2 instance IP address, go to EC2 dashboard and select Instances and your instance (1). In the bottom panel in Details tab (2), look for the IPv4 address (3) associated with the DNS name (4). You can try to ping this IP address from Linux Jumphost.","title":"Validate connectivity"},{"location":"ccp_create_cluster-aws/","text":"Create AWS Infrastructure Provider Before you will attempt to create cluster, you will create an infrastructure provider profile for AWS. Step 1: Login to AWS console, and click on your username in the right top corner as in the figure: Step 2: Select \"My Security Credentials\" Step 3: Click \"Create access key\" button Step 4: Copy to Access Key ID and Secret access key. Please note that the \"Secret access key\" is displayed only during creation of the key, and will not be visible after closing this wizzard. Attention Please save the ACCESS KEY and SECRET KEY in the notepad, it will be necessary in the later stage Step 5: Go back to CCP Control Plane panel. From the left panel in CCP, select \"Infrastructure Providers\" and select \"AWS\" in the main section. Step 6: Create a \"New Provider\" profile, provide following parameters: Provider Name studentXX ACCESS KEY ID use access key id copied from AWS SECRET ACCESS KEY use secret access key copied during creation of secret access key in AWS Check if new key has been created, remember the infrastructure provider profile name. Create Tenant Data Cluster After login to Cisco Container Platform, go to AWS tab and click New Cluster button. You will be redirected to the new page where you will provide details of your new Kubernetes EKS cluster. Step 1: Basic Information - select infrastructure provider, AWS Region and Kubernetes cluster version and name. Please use following parameters. Infrastructure Provider: studentXX Kubernetes Cluster Name: aws-student-XX AWS Region: eu-central-1 Kubernetes Version: 1.15 studentXX - XX is your unique student ID, assigned by speakers. Step 2: Node Configuration - here you will configure how EKS nodes will be setup. You will specify EC2 instance type, AMI image, number of worker nodes, IAM Role and SSH public key, so you could ssh into the node for troubleshooting purposes. Please use exact paramaters as instruction says. Instance Type: t2.small Machine Image: hvm-ssd/ccp-ubuntu-bionic-18.04-amd64-server-20200729-7.0.0 Worker Count: 1 IAM Access Role ARN: studentXX-role - provided by speakers SSH KEY: ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIJeLi4z9dYE+6tqTXzsELMch2RjR6R+rl57UEJzMuPkO Select Machine Image by expanding the list, from the drop-down list click on \"SHOW 3 FILTERERD OPTIONS\" then select the 2nd image. You should have similar options selected Your AWS user is configured with IAM policy that allows to assume this specific role. For this CCP leverages IAM-Authenticator add-on, that is installed in Linux jumphost. Later on the Linux jumphost you will login to your aws cli with your associated aws credentials and then you will be able to manage you new EKS Kubernetes Cluster. Instead of using username and password or private certificates, IAM Authenticator leverages AWS STS (Secure Token Service) to tokenize and sign URL requests towards your EKS Cluster. AWS IAM is responsible to authorize request and pass commands to your Kubernetes Cluster. During creation of EKS Cluster, this role is associated with Kubernets System:Masters group which provides full access to your cluster. Step 3: VPC configuration - here specify your VPC subnets. CCP will request VPC creation in AWS, with subnets, internet gateway, nat gateways. It is creating 3 private and 3 public subnets. Please use default subnet's from CIDR: 10.0.0.0/16. Click Next to enter the summary page, and just confirm all data. Once confirmed please click Finish Once finished, you will see progress bar to check status of the cluster creation. Monitor cluster creation You can observe tenant cluster creation from CCP Dashboard, however, if you are interested to see more details, you can login to AWS dashboard and go to EKS to monitor master creation, and then to CloudFormation to see status of CloudFormation Stack deployment. CloudFormation is a AWS tool to automate creation of different objects. Login to AWS dashboard, and select EKS service. Use credentials provided to you by speakers. In EKS dashboard you will see EKS service creation progress. Once finished you can move to CloudFormation dashboard. From there find your stack (based on your username) and watch deployment progress. Once finished, you should see in CCP GUI that the Kubernetes Cluster in AWS has been deployed successfully. At this point go to the next chapter to connect your Kubernetes Cluster in AWS to Hybrid Network that provides access to on-premise Data Center.","title":"Create AWS Tenant Cluster"},{"location":"ccp_create_cluster-aws/#create-aws-infrastructure-provider","text":"Before you will attempt to create cluster, you will create an infrastructure provider profile for AWS. Step 1: Login to AWS console, and click on your username in the right top corner as in the figure: Step 2: Select \"My Security Credentials\" Step 3: Click \"Create access key\" button Step 4: Copy to Access Key ID and Secret access key. Please note that the \"Secret access key\" is displayed only during creation of the key, and will not be visible after closing this wizzard. Attention Please save the ACCESS KEY and SECRET KEY in the notepad, it will be necessary in the later stage Step 5: Go back to CCP Control Plane panel. From the left panel in CCP, select \"Infrastructure Providers\" and select \"AWS\" in the main section. Step 6: Create a \"New Provider\" profile, provide following parameters: Provider Name studentXX ACCESS KEY ID use access key id copied from AWS SECRET ACCESS KEY use secret access key copied during creation of secret access key in AWS Check if new key has been created, remember the infrastructure provider profile name.","title":"Create AWS Infrastructure Provider"},{"location":"ccp_create_cluster-aws/#create-tenant-data-cluster","text":"After login to Cisco Container Platform, go to AWS tab and click New Cluster button. You will be redirected to the new page where you will provide details of your new Kubernetes EKS cluster. Step 1: Basic Information - select infrastructure provider, AWS Region and Kubernetes cluster version and name. Please use following parameters. Infrastructure Provider: studentXX Kubernetes Cluster Name: aws-student-XX AWS Region: eu-central-1 Kubernetes Version: 1.15 studentXX - XX is your unique student ID, assigned by speakers. Step 2: Node Configuration - here you will configure how EKS nodes will be setup. You will specify EC2 instance type, AMI image, number of worker nodes, IAM Role and SSH public key, so you could ssh into the node for troubleshooting purposes. Please use exact paramaters as instruction says. Instance Type: t2.small Machine Image: hvm-ssd/ccp-ubuntu-bionic-18.04-amd64-server-20200729-7.0.0 Worker Count: 1 IAM Access Role ARN: studentXX-role - provided by speakers SSH KEY: ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIJeLi4z9dYE+6tqTXzsELMch2RjR6R+rl57UEJzMuPkO Select Machine Image by expanding the list, from the drop-down list click on \"SHOW 3 FILTERERD OPTIONS\" then select the 2nd image. You should have similar options selected Your AWS user is configured with IAM policy that allows to assume this specific role. For this CCP leverages IAM-Authenticator add-on, that is installed in Linux jumphost. Later on the Linux jumphost you will login to your aws cli with your associated aws credentials and then you will be able to manage you new EKS Kubernetes Cluster. Instead of using username and password or private certificates, IAM Authenticator leverages AWS STS (Secure Token Service) to tokenize and sign URL requests towards your EKS Cluster. AWS IAM is responsible to authorize request and pass commands to your Kubernetes Cluster. During creation of EKS Cluster, this role is associated with Kubernets System:Masters group which provides full access to your cluster. Step 3: VPC configuration - here specify your VPC subnets. CCP will request VPC creation in AWS, with subnets, internet gateway, nat gateways. It is creating 3 private and 3 public subnets. Please use default subnet's from CIDR: 10.0.0.0/16. Click Next to enter the summary page, and just confirm all data. Once confirmed please click Finish Once finished, you will see progress bar to check status of the cluster creation.","title":"Create Tenant Data Cluster"},{"location":"ccp_create_cluster-aws/#monitor-cluster-creation","text":"You can observe tenant cluster creation from CCP Dashboard, however, if you are interested to see more details, you can login to AWS dashboard and go to EKS to monitor master creation, and then to CloudFormation to see status of CloudFormation Stack deployment. CloudFormation is a AWS tool to automate creation of different objects. Login to AWS dashboard, and select EKS service. Use credentials provided to you by speakers. In EKS dashboard you will see EKS service creation progress. Once finished you can move to CloudFormation dashboard. From there find your stack (based on your username) and watch deployment progress. Once finished, you should see in CCP GUI that the Kubernetes Cluster in AWS has been deployed successfully. At this point go to the next chapter to connect your Kubernetes Cluster in AWS to Hybrid Network that provides access to on-premise Data Center.","title":"Monitor cluster creation"},{"location":"ccp_create_cluster-onprem/","text":"Create Tenant Data Cluster After login to Cisco Container Platform, go to Clusters click on vSphere tab and click New Cluster button. You will be redirected to the new page where you will provide details of your new Kubernetes on-prem cluster . Step 1: Basic Information - select infrastructure provider, Kubernetes cluster version and name. Please use following parameters. Infrastructure Provider: vsphere Kubernetes Cluster Name: on-prem-backend Kubernetes Version: 1.17.6 Step 2: Provider Settings - here you will configure how your on-prem cluster will be deployed. You need to define Data Center name, Cluster name, Datastore and more. Please use exact paramaters as instruction says. DATA CENTER NAME: datacenter CLUSTER NAME: CCP DATASTORE NAME: datastore1 VM TEMPLATE NAME: ccp-tenant-image-1.17.6-ubuntu18-7.0.0 VM NETWORK NAME: CCP_Intra Step 3: Node Configuration - here you will configure capacity of your Kubernetes Nodes. You are able to configure amount of Workers, their vCPU or vRAM amount. In this section you need to configure SSH settings to be able to connect directly to your Kubernetes Nodes (Master as well as Workers). Finally you can allocate numbers of Load Balancer VIPs, subnet as well as POD CIDR. Please use exact paramaters as instruction says. WORKER NODES: 2 SSH User: ccpuser SSH KEY: ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIJeLi4z9dYE+6tqTXzsELMch2RjR6R+rl57UEJzMuPkO LOAD BALANCER VIP: 2 SUBNET: default-network-subnet POD CIDR: 192.168.0.0/16 Rest of variables please leave blank. Step 4: Next screen summarizes all variables you did choice. Please review them before you click Finish . Monitor cluster creation You can observe tenant cluster creation from CCP Dashboard. Another place you can observe whats happening on vCenter by connecting via User Interface. Once finished, you should see in CCP GUI that the Kubernetes Cluster in vSphere has been deployed successfully. At this point go to the next chapter to create your Kubernetes Cluster in AWS Public Cloud.","title":"Create ON-PREM Tenant Cluster"},{"location":"ccp_create_cluster-onprem/#create-tenant-data-cluster","text":"After login to Cisco Container Platform, go to Clusters click on vSphere tab and click New Cluster button. You will be redirected to the new page where you will provide details of your new Kubernetes on-prem cluster . Step 1: Basic Information - select infrastructure provider, Kubernetes cluster version and name. Please use following parameters. Infrastructure Provider: vsphere Kubernetes Cluster Name: on-prem-backend Kubernetes Version: 1.17.6 Step 2: Provider Settings - here you will configure how your on-prem cluster will be deployed. You need to define Data Center name, Cluster name, Datastore and more. Please use exact paramaters as instruction says. DATA CENTER NAME: datacenter CLUSTER NAME: CCP DATASTORE NAME: datastore1 VM TEMPLATE NAME: ccp-tenant-image-1.17.6-ubuntu18-7.0.0 VM NETWORK NAME: CCP_Intra Step 3: Node Configuration - here you will configure capacity of your Kubernetes Nodes. You are able to configure amount of Workers, their vCPU or vRAM amount. In this section you need to configure SSH settings to be able to connect directly to your Kubernetes Nodes (Master as well as Workers). Finally you can allocate numbers of Load Balancer VIPs, subnet as well as POD CIDR. Please use exact paramaters as instruction says. WORKER NODES: 2 SSH User: ccpuser SSH KEY: ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIJeLi4z9dYE+6tqTXzsELMch2RjR6R+rl57UEJzMuPkO LOAD BALANCER VIP: 2 SUBNET: default-network-subnet POD CIDR: 192.168.0.0/16 Rest of variables please leave blank. Step 4: Next screen summarizes all variables you did choice. Please review them before you click Finish .","title":"Create Tenant Data Cluster"},{"location":"ccp_create_cluster-onprem/#monitor-cluster-creation","text":"You can observe tenant cluster creation from CCP Dashboard. Another place you can observe whats happening on vCenter by connecting via User Interface. Once finished, you should see in CCP GUI that the Kubernetes Cluster in vSphere has been deployed successfully. At this point go to the next chapter to create your Kubernetes Cluster in AWS Public Cloud.","title":"Monitor cluster creation"},{"location":"ccp_exploring/","text":"Exploring Cisco Container Platform Cisco Container Platform is a production grade platform to manage, monitor and deploy Kubernetes Clusters in your Enterprise. CCP uses 100% upstream Kuberenetes without vendor specific modification, creating seamless experience for developer to deploy application on any kubernetes platform in public or private cloud. CCP provides authentication and authorization, security, high availability, networking, load balancing, and operational capabilities to effectively operate and manage Kubernetes clusters. CCP also provides a validated configuration of Kubernetes and can integrate with underlying infrastructure components such as Cisco HyperFlex and Cisco ACI. Cisco Container Platform has two main architecture components: Control Plane Cluster - to provide management platform for your Kubernetes Clusters where you can deploy new production grade Kubernetes Clusters, scale worker nodes, manage policy and networking. The Control Plane is also build based on Kubernetes. Tenant Cluster - the Kubernetes cluster used to host applications across production, development, staging and many other environments Each user in this lab will have own Cisco Container Platform Control Plane. As described in the Lab Access section, every user will have same IP address of CCP UI, within unique dCloud session. Explore CCP dashboard Login to your dedicated CCP Dashboard - find URL and credentials that you can find in Lab Access Guide. Once logged in, you will be taken to the \"cluster\" page. In this page you can manage your kubernetes clusters across multiple infrastructure providers, edit their configuration, adding nodes or create node policies. In the next menu position, you will see Infrastructure Providers . This is place where you configure to access your cloud infrastructure Provider, either it could be on-premise VMware vSphere (vCenter access) or public cloud - Amazon Web Services or Azure. Cisco Container Platform leverage one of these profiles to deploy Tenant Clusters in specific Infrastructure Provider. Networks provides IP addressing subnets and pools configuration for the tenant clusters on-premise (vSpehre). When tenant cluster is deployed, you can manage pool of IP addresses available to allocate for LoadBalancer under which applications will be exposed externally. Cisco Container Platform provides out-of-the-box ACI integration with Tenant Clusters. During installation of Cisco Container Platform Control Plane (the platform that provides GUI that you are now browsing through), you can select Kubernetes CNI solution. CCP supports Calico (the one used in this lab), Cisco ACI CNI or high performance Contiv-VPP CNI. For Cisco ACI-CNI, new tenant clusters are deployed with ACI integration. No need later to configure it separately. User Management provides panel to manage authentication providers, users, groups and policies. Users can be authenticated against local database or from Active Directory. Authorization policy allocates user to particular Tenant Cluster, You can allocate access for the developer team to manage only their own Kubernetes cluster (expand number of nodes, manage IP addresses pool available for Load Balancing services). The last tab provides Licensing settings, where you can register your Cisco Container Platform with Smart Licensing server.","title":"Explore Cisco Container Platform (CCP)"},{"location":"ccp_exploring/#exploring-cisco-container-platform","text":"Cisco Container Platform is a production grade platform to manage, monitor and deploy Kubernetes Clusters in your Enterprise. CCP uses 100% upstream Kuberenetes without vendor specific modification, creating seamless experience for developer to deploy application on any kubernetes platform in public or private cloud. CCP provides authentication and authorization, security, high availability, networking, load balancing, and operational capabilities to effectively operate and manage Kubernetes clusters. CCP also provides a validated configuration of Kubernetes and can integrate with underlying infrastructure components such as Cisco HyperFlex and Cisco ACI. Cisco Container Platform has two main architecture components: Control Plane Cluster - to provide management platform for your Kubernetes Clusters where you can deploy new production grade Kubernetes Clusters, scale worker nodes, manage policy and networking. The Control Plane is also build based on Kubernetes. Tenant Cluster - the Kubernetes cluster used to host applications across production, development, staging and many other environments Each user in this lab will have own Cisco Container Platform Control Plane. As described in the Lab Access section, every user will have same IP address of CCP UI, within unique dCloud session.","title":"Exploring Cisco Container Platform"},{"location":"ccp_exploring/#explore-ccp-dashboard","text":"Login to your dedicated CCP Dashboard - find URL and credentials that you can find in Lab Access Guide. Once logged in, you will be taken to the \"cluster\" page. In this page you can manage your kubernetes clusters across multiple infrastructure providers, edit their configuration, adding nodes or create node policies. In the next menu position, you will see Infrastructure Providers . This is place where you configure to access your cloud infrastructure Provider, either it could be on-premise VMware vSphere (vCenter access) or public cloud - Amazon Web Services or Azure. Cisco Container Platform leverage one of these profiles to deploy Tenant Clusters in specific Infrastructure Provider. Networks provides IP addressing subnets and pools configuration for the tenant clusters on-premise (vSpehre). When tenant cluster is deployed, you can manage pool of IP addresses available to allocate for LoadBalancer under which applications will be exposed externally. Cisco Container Platform provides out-of-the-box ACI integration with Tenant Clusters. During installation of Cisco Container Platform Control Plane (the platform that provides GUI that you are now browsing through), you can select Kubernetes CNI solution. CCP supports Calico (the one used in this lab), Cisco ACI CNI or high performance Contiv-VPP CNI. For Cisco ACI-CNI, new tenant clusters are deployed with ACI integration. No need later to configure it separately. User Management provides panel to manage authentication providers, users, groups and policies. Users can be authenticated against local database or from Active Directory. Authorization policy allocates user to particular Tenant Cluster, You can allocate access for the developer team to manage only their own Kubernetes cluster (expand number of nodes, manage IP addresses pool available for Load Balancing services). The last tab provides Licensing settings, where you can register your Cisco Container Platform with Smart Licensing server.","title":"Explore CCP dashboard"},{"location":"ccp_grafana_kibana/","text":"Accessing Grafana dashboard Once you are logged in to Kubernetes cluster dashboard, you can obtain password to grafana dashboard which provides grafical view of Kubernetes cluster condition, but also to monitor your applications. Passwords are stored in Kubernetes Secrets object. Grafana admin password can be decoded from base64 encryption, and copy-pasted to grafana login page. Next steps will show you how to find grafana password: First, change namespace to CCP : Next, go to Secrets object in the menu, and look and the main pane on the right, you will be looking for secret called ccp-monitor-grafana Got to second page and there you will find desired Secret. In the Data field you will see admin-password and small eye icon. Please click on that icon to uncover the password. Once uncovered please copy it to clipboard. Alternatively, you can use following one-line kubectl command to obtain grafana admin-password. Please use this command from the master node, rather local PC as it may not have base64 installed. This command works only in linux environment, you can use it on the Kubernetes master node to which you can SSH kubectl -n ccp get secret ccp-monitor-grafana -o=jsonpath='{.data.admin-password}' | base64 --decode Next, please go back to the CCP dashboard, select your cluster, go into the detail mode and select Grafana button. You will be redirected to the Grafana page, where you can login with username admin and copied password from Secret. Once logged in to grafana, please select in the top left corner Home drop down menu - select Kubernetes Cluster Monitoring (Prometheus) . The dashboard where you can monitor resource utilisation of all PODs across all namespaces. You should see graphs like this - 7. Accessing logs on Cisco Container Platform The Elasticsearch, Fluentd, and Kibana (EFK) stack enables you to collect and monitor log data from containerized applications for troubleshooting or compliance purposes. These components are automatically installed when you install Cisco Container Platform. Fluentd is an open source data collector. It works at the backend to collect and forward the log data to Elasticsearch. Kibana is an open source analytics and visualization platform designed to work with Elasticsearch. It allows you to create rich visualizations and dashboards with the aggregated data. By default access to Kibana is not exposed due to security reasons. You can expose Kibana to the external network by using 'NodePort'. Let's check whether kibana service exists currently: kubectl -n ccp get svc -n ccp NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ccp-efk-kibana ClusterIP 10.103.179.126 <none> 5601/TCP 1d ccp-monitor-grafana ClusterIP 10.99.197.50 <none> 80/TCP 1d ccp-monitor-prometheus-alertmanager ClusterIP 10.105.91.170 <none> 80/TCP 1d ccp-monitor-prometheus-kube-state-metrics ClusterIP None <none> 80/TCP 1d ccp-monitor-prometheus-node-exporter ClusterIP None <none> 9100/TCP 1d ccp-monitor-prometheus-pushgateway ClusterIP 10.103.53.72 <none> 9091/TCP 1d ccp-monitor-prometheus-server ClusterIP 10.99.248.1 <none> 80/TCP 1d elasticsearch-logging ClusterIP 10.101.23.250 <none> 9200/TCP 1d kubernetes-dashboard ClusterIP 10.106.224.153 <none> 443/TCP 1d nginx-ingress-controller LoadBalancer 10.99.88.25 172.18.1.239 80:32290/TCP,443:31442/TCP 1d nginx-ingress-default-backend ClusterIP 10.103.155.52 <none> 80/TCP 1d Note the service TYPE of the ccp-efk-kibana . Currently it is ClusterIP, which is not reacheable outside of the Cluster. In order to expose Logging dashboard (Kibana) for administrator, you can change ClusterIP service type to NodePort using following command: kubectl -n ccp edit svc ccp-efk-kibana You will be taken to the vi editor. Please move arrows until line 3rd line from the bottom: (...) type: ClusterIP status: loadBalancer: {} Change mode to edit using combination of keys in following order: ESC then press i . Change value ClusterIP to NodePort , please pay attention to capital letters. Like this: (...) type: NodePort status: loadBalancer: {} Press Esc to exit from editing mode. Press following combination of keys to save file in the following sequence : then type wq then press enter Confirm that the change has been successfully saved and execute following command to confirm service type for ccp-efk-kibana kubectl -n ccp get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ccp-efk-kibana NodePort 10.97.166.41 <none> 5601:30662/TCP 11h Note external service port, in above example is 30662 . Check node IP addresses by using following command: kubectl get nodes -o wide Note any IP address of your node in the collumn External-IP Open Chrome web-browser and enter following URL: http://<node_ip>:30662 In the instruction example: http://172.18.1.240:30662 Once page will be opened, you will be asked to create index pattern. It means, you have to point from which log collection the logs will be presented. Click on Management and type logstash-* in the field - Next, you will be asked to add additional pattern to the presented logs After this, you will be able to see logs from all PODs and applications working on this cluster. To do this, go to Discovery panel, and type in the filter at the top of the page: kubernetes.namespace.name = default Since our application is deployed in default namespace, filter will show only logs from pods deployed in that namespace. Further filtering could be appliend, but this is not the main topic of this lab excercise.","title":"Ccp grafana kibana"},{"location":"ccp_grafana_kibana/#accessing-grafana-dashboard","text":"Once you are logged in to Kubernetes cluster dashboard, you can obtain password to grafana dashboard which provides grafical view of Kubernetes cluster condition, but also to monitor your applications. Passwords are stored in Kubernetes Secrets object. Grafana admin password can be decoded from base64 encryption, and copy-pasted to grafana login page. Next steps will show you how to find grafana password: First, change namespace to CCP : Next, go to Secrets object in the menu, and look and the main pane on the right, you will be looking for secret called ccp-monitor-grafana Got to second page and there you will find desired Secret. In the Data field you will see admin-password and small eye icon. Please click on that icon to uncover the password. Once uncovered please copy it to clipboard. Alternatively, you can use following one-line kubectl command to obtain grafana admin-password. Please use this command from the master node, rather local PC as it may not have base64 installed. This command works only in linux environment, you can use it on the Kubernetes master node to which you can SSH kubectl -n ccp get secret ccp-monitor-grafana -o=jsonpath='{.data.admin-password}' | base64 --decode Next, please go back to the CCP dashboard, select your cluster, go into the detail mode and select Grafana button. You will be redirected to the Grafana page, where you can login with username admin and copied password from Secret. Once logged in to grafana, please select in the top left corner Home drop down menu - select Kubernetes Cluster Monitoring (Prometheus) . The dashboard where you can monitor resource utilisation of all PODs across all namespaces. You should see graphs like this -","title":"Accessing Grafana dashboard"},{"location":"ccp_grafana_kibana/#7-accessing-logs-on-cisco-container-platform","text":"The Elasticsearch, Fluentd, and Kibana (EFK) stack enables you to collect and monitor log data from containerized applications for troubleshooting or compliance purposes. These components are automatically installed when you install Cisco Container Platform. Fluentd is an open source data collector. It works at the backend to collect and forward the log data to Elasticsearch. Kibana is an open source analytics and visualization platform designed to work with Elasticsearch. It allows you to create rich visualizations and dashboards with the aggregated data. By default access to Kibana is not exposed due to security reasons. You can expose Kibana to the external network by using 'NodePort'. Let's check whether kibana service exists currently: kubectl -n ccp get svc -n ccp NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ccp-efk-kibana ClusterIP 10.103.179.126 <none> 5601/TCP 1d ccp-monitor-grafana ClusterIP 10.99.197.50 <none> 80/TCP 1d ccp-monitor-prometheus-alertmanager ClusterIP 10.105.91.170 <none> 80/TCP 1d ccp-monitor-prometheus-kube-state-metrics ClusterIP None <none> 80/TCP 1d ccp-monitor-prometheus-node-exporter ClusterIP None <none> 9100/TCP 1d ccp-monitor-prometheus-pushgateway ClusterIP 10.103.53.72 <none> 9091/TCP 1d ccp-monitor-prometheus-server ClusterIP 10.99.248.1 <none> 80/TCP 1d elasticsearch-logging ClusterIP 10.101.23.250 <none> 9200/TCP 1d kubernetes-dashboard ClusterIP 10.106.224.153 <none> 443/TCP 1d nginx-ingress-controller LoadBalancer 10.99.88.25 172.18.1.239 80:32290/TCP,443:31442/TCP 1d nginx-ingress-default-backend ClusterIP 10.103.155.52 <none> 80/TCP 1d Note the service TYPE of the ccp-efk-kibana . Currently it is ClusterIP, which is not reacheable outside of the Cluster. In order to expose Logging dashboard (Kibana) for administrator, you can change ClusterIP service type to NodePort using following command: kubectl -n ccp edit svc ccp-efk-kibana You will be taken to the vi editor. Please move arrows until line 3rd line from the bottom: (...) type: ClusterIP status: loadBalancer: {} Change mode to edit using combination of keys in following order: ESC then press i . Change value ClusterIP to NodePort , please pay attention to capital letters. Like this: (...) type: NodePort status: loadBalancer: {} Press Esc to exit from editing mode. Press following combination of keys to save file in the following sequence : then type wq then press enter Confirm that the change has been successfully saved and execute following command to confirm service type for ccp-efk-kibana kubectl -n ccp get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ccp-efk-kibana NodePort 10.97.166.41 <none> 5601:30662/TCP 11h Note external service port, in above example is 30662 . Check node IP addresses by using following command: kubectl get nodes -o wide Note any IP address of your node in the collumn External-IP Open Chrome web-browser and enter following URL: http://<node_ip>:30662 In the instruction example: http://172.18.1.240:30662 Once page will be opened, you will be asked to create index pattern. It means, you have to point from which log collection the logs will be presented. Click on Management and type logstash-* in the field - Next, you will be asked to add additional pattern to the presented logs After this, you will be able to see logs from all PODs and applications working on this cluster. To do this, go to Discovery panel, and type in the filter at the top of the page: kubernetes.namespace.name = default Since our application is deployed in default namespace, filter will show only logs from pods deployed in that namespace. Further filtering could be appliend, but this is not the main topic of this lab excercise.","title":"7. Accessing logs on Cisco Container Platform"},{"location":"ccp_modify_numer_of_nodes/","text":"Excercises: Cisco Container Platform - modify node configuration By default, when new cluster is created, it puts Master node and Worker nodes to separate pools of nodes. The reason behind it is to not deploy applications on the master node. This lab should be executed once your application has been deployed, on your Kubernetes cluster. One of the application components - REST APi agent is running on two worker nodes, since the replica has been set to 2. Let's try to remove one node from the cluster, and check how Kubernetes will maintain two copies of the POD. TASK 1 Using kubectl get information about all nodes in the cluster, including theis IP address. kubectl get pods -o wide ccpuser@pod06-a-ccp-data-master40bae43bca:~$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE iot-backend-mariadb-6bbf6f4764-qxw6g 1/1 Running 0 1h 10.161.2.15 pod06-a-ccp-data-workerf49047e761 <none> iot-backend-mqtt-db-agent-57c88c58dd-wcbvs 1/1 Running 0 22m 10.161.2.21 pod06-a-ccp-data-workerf55c089976 <none> iot-backend-rest-api-agent-75bfb74dc4-bd52b 1/1 Running 0 22m 10.161.2.19 pod06-a-ccp-data-workerf49047e761 <none> iot-backend-rest-api-agent-75bfb74dc4-dgvbk 1/1 Running 0 34s 10.161.3.4 pod06-a-ccp-data-workerf55c089976 <none> TASK 2 Decrease number of worker nodes in your cluster to 1 Click on the small square next to default-pool to edit it's settings. Decrease number of nodes to one and save it may take several minutes until node will be removed, since Kubernetes must move PODs running on the removed node to the node that will stay within cluster. Sometimes the other node must download image, since that kind of POD was never deployed before TASK 3 Verify how PODs are distributed. Note how many replicas of iot-backend-rest-api-agent are running. Use kubectl command to check pods discritbution across worker nodes: kubectl get pods -o wide ccpuser@pod06-a-ccp-data-master40bae43bca:~$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE iot-backend-mariadb-6bbf6f4764-qxw6g 1/1 Running 0 1h 10.161.2.15 pod06-a-ccp-data-workerf49047e761 <none> iot-backend-mqtt-db-agent-57c88c58dd-wcbvs 1/1 Running 0 22m 10.161.2.21 pod06-a-ccp-data-workerf49047e761 <none> iot-backend-rest-api-agent-75bfb74dc4-bd52b 1/1 Running 0 22m 10.161.2.19 pod06-a-ccp-data-workerf49047e761 <none> iot-backend-rest-api-agent-75bfb74dc4-dgvbk 1/1 Running 0 34s 10.161.3.4 pod06-a-ccp-data-workerf49047e761 <none> TASK 4 Increase number of nodes back to 2 . TASK 5 Verify how many replicas of iot-backend-rest-api-agent are running and on which worker nodes. TASK 6 Force Kuberenetes to reschedule and distribute iot-backend-rest-api-agent across nodes. Note that adding node to the cluster does not change distribution of PODs automatically. In order to force Kubernetes to deploy POD on the second node, please delete one of the POD iot-backend-rest-api-agent kubectl delete pod iot-backend-rest-api-agent-XXXXXX once this is completed verify what happend with kubectl get pods -o wide command. During delete operation, Kubernetes realized that POD should have 2 replicas, therefore, it deployed one more replica on the next worker node in the pool.","title":"Ccp modify numer of nodes"},{"location":"ccp_modify_numer_of_nodes/#excercises-cisco-container-platform-modify-node-configuration","text":"By default, when new cluster is created, it puts Master node and Worker nodes to separate pools of nodes. The reason behind it is to not deploy applications on the master node. This lab should be executed once your application has been deployed, on your Kubernetes cluster. One of the application components - REST APi agent is running on two worker nodes, since the replica has been set to 2. Let's try to remove one node from the cluster, and check how Kubernetes will maintain two copies of the POD. TASK 1 Using kubectl get information about all nodes in the cluster, including theis IP address. kubectl get pods -o wide ccpuser@pod06-a-ccp-data-master40bae43bca:~$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE iot-backend-mariadb-6bbf6f4764-qxw6g 1/1 Running 0 1h 10.161.2.15 pod06-a-ccp-data-workerf49047e761 <none> iot-backend-mqtt-db-agent-57c88c58dd-wcbvs 1/1 Running 0 22m 10.161.2.21 pod06-a-ccp-data-workerf55c089976 <none> iot-backend-rest-api-agent-75bfb74dc4-bd52b 1/1 Running 0 22m 10.161.2.19 pod06-a-ccp-data-workerf49047e761 <none> iot-backend-rest-api-agent-75bfb74dc4-dgvbk 1/1 Running 0 34s 10.161.3.4 pod06-a-ccp-data-workerf55c089976 <none> TASK 2 Decrease number of worker nodes in your cluster to 1 Click on the small square next to default-pool to edit it's settings. Decrease number of nodes to one and save it may take several minutes until node will be removed, since Kubernetes must move PODs running on the removed node to the node that will stay within cluster. Sometimes the other node must download image, since that kind of POD was never deployed before TASK 3 Verify how PODs are distributed. Note how many replicas of iot-backend-rest-api-agent are running. Use kubectl command to check pods discritbution across worker nodes: kubectl get pods -o wide ccpuser@pod06-a-ccp-data-master40bae43bca:~$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE iot-backend-mariadb-6bbf6f4764-qxw6g 1/1 Running 0 1h 10.161.2.15 pod06-a-ccp-data-workerf49047e761 <none> iot-backend-mqtt-db-agent-57c88c58dd-wcbvs 1/1 Running 0 22m 10.161.2.21 pod06-a-ccp-data-workerf49047e761 <none> iot-backend-rest-api-agent-75bfb74dc4-bd52b 1/1 Running 0 22m 10.161.2.19 pod06-a-ccp-data-workerf49047e761 <none> iot-backend-rest-api-agent-75bfb74dc4-dgvbk 1/1 Running 0 34s 10.161.3.4 pod06-a-ccp-data-workerf49047e761 <none> TASK 4 Increase number of nodes back to 2 . TASK 5 Verify how many replicas of iot-backend-rest-api-agent are running and on which worker nodes. TASK 6 Force Kuberenetes to reschedule and distribute iot-backend-rest-api-agent across nodes. Note that adding node to the cluster does not change distribution of PODs automatically. In order to force Kubernetes to deploy POD on the second node, please delete one of the POD iot-backend-rest-api-agent kubectl delete pod iot-backend-rest-api-agent-XXXXXX once this is completed verify what happend with kubectl get pods -o wide command. During delete operation, Kubernetes realized that POD should have 2 replicas, therefore, it deployed one more replica on the next worker node in the pool.","title":"Excercises: Cisco Container Platform - modify node configuration"},{"location":"create_gke_engine/","text":"Create Kubernetes Cluster (GKE) on Google Cloud Open Kubernetes Engine: 2 3 4 5 6","title":"Create Kubernetes Cluster (GKE) on Google Cloud"},{"location":"create_gke_engine/#create-kubernetes-cluster-gke-on-google-cloud","text":"","title":"Create Kubernetes Cluster (GKE) on Google Cloud"},{"location":"create_gke_engine/#open-kubernetes-engine","text":"","title":"Open Kubernetes Engine:"},{"location":"create_gke_engine/#2","text":"","title":"2"},{"location":"create_gke_engine/#3","text":"","title":"3"},{"location":"create_gke_engine/#4","text":"","title":"4"},{"location":"create_gke_engine/#5","text":"","title":"5"},{"location":"create_gke_engine/#6","text":"","title":"6"},{"location":"deploy_backend/","text":"Deploy the Backend Application Components on CCP Kubernetes Cluster (CCP Tenant Cluster) In this section you would deploy the backend components of the IoT Application on the Kubernetes cluster deployed on your CCP instance. Following diagram shows the high-level architecture of these backend application containers Login to Kubernetes Master CLI Shell: SSH into Linux Jumphost using Putty (198.18.133.10) - use your credentials. From here you will deploy two microservices in on-premise Kubernetes Cluster and one microservice in AWS EKS Kubernetes cluster. You will see how microservices talks to each other and how to establish necessary communication. 1. Deploy MariaDB Databse: MariaDB will be used in the backend to save the sensor data received from AWS IoT platform over MQTT protocol. For this we would create following objects - Kubernetes Secret Kubernetes Persistent Volume Claim (PVC) Kubernetes MariaDB Deployment Kubernetes ClusterIP Service (Headless Service) Following diagram shows the relationship between these objects - 1.1 Create Kubernetes Secret for MariaDB: A Kubernetes Secret is an object that contains a small amount of sensitive data such as a password, a token, or a key. Such information might otherwise be put in a Pod specification or in an image; putting it in a Secret object allows for more control over how it is used, and reduces the risk of accidental exposure. The MariaDB container image uses an environment variable named as 'MYSQL_ROOT_PASSWORD', it hold the root password required to access the database. So you would create a new secret with 'password' key (value as 'cisco123') which would later be used in mariaDB deployment yaml file. 1.1.1: Switch context to on-premise Kubernetes Cluster - Change context of kubectl command to access on-premise Kubernetes Cluster. kubectl config use-context admin@on-prem-backend kubectl config get-contexts 1.1.2: Create DB Password Secret - Use the following command to create a new secret on your kubernetes cluster - kubectl create secret generic mariadb-root-pass --from-literal=password=cisco123 1.1.3: Verify DB Password Secret - Check if the secret was created successfully or not - kubectl get secret mariadb-root-pass You should have the output similar to the following screenshot - 1.2 Create Kubernetes Persistent Volume Claim for MariaDB: A Kubernetes Persistent Volume Claim (PVC) is a request for storage by a user. It is similar to a pod. Pods consume node resources and PVCs consume Persistent Volume (PV) resources. Pods can request specific levels of resources (CPU and Memory). Claims can request specific size and access modes (e.g., can be mounted once read/write or many times read-only). To keep the sensor data safe during Pod restarts, you would create a new Persistent Volume Claim. The following yaml definition would be used to create the 'PersistentVolumeClaim' - --- apiVersion : v1 kind : PersistentVolumeClaim metadata : name : mariadb-pv-claim labels : app : iot-backend spec : accessModes : - ReadWriteOnce resources : requests : storage : 2Gi * 1.2.1: Create Persistent Volume Claim - Use the following command to create a new Persistent Volume Claim for MariaDB Pod - kubectl create -f https://raw.githubusercontent.com/marcinduma/HOLCLD-2101/main/Kubernetes/Backend/Mariadb/mariadb_persistent_volume.yaml 1.2.2: Verify Persistent Volume Claim - Check if the PVC was created successfully or not - kubectl get pvc mariadb-pv-claim You should have the output similar to the following screenshot - Caution It can take up to a few minutes for the PVs to be provisioned. DO NOT procced futher till the PVC deployment gets completed. 1.3 Deploy MariaDB on Kubernetes: MariaDB is a community-developed fork of the MySQL relational database management system intended to remain free under the GNU GPL. Development is led by some of the original developers of MySQL, who forked it due to concerns over its acquisition by Oracle Corporation. MariaDB intends to maintain high compatibility with MySQL, ensuring a drop-in replacement capability with library binary parity and exact matching with MySQL APIs and commands. The following yaml definition will be used to deploy MariaDB pod - --- apiVersion : apps/v1 # for versions before 1.9.0 use apps/v1beta2 kind : Deployment metadata : name : iot-backend-mariadb labels : app : iot-backend spec : selector : matchLabels : app : iot-backend tier : mariadb strategy : type : Recreate template : metadata : labels : app : iot-backend tier : mariadb spec : containers : - image : mariadb:10.3 name : mariadb env : - name : MYSQL_ROOT_PASSWORD valueFrom : secretKeyRef : name : mariadb-root-pass key : password ports : - containerPort : 3306 name : mariadb volumeMounts : - name : mariadb-persistent-storage mountPath : /var/lib/mysql volumes : - name : mariadb-persistent-storage persistentVolumeClaim : claimName : mariadb-pv-claim * 1.3.1: Deploy MariaDB - Use the following command to create a MariaDB kubernetes deployment kubectl create -f https://raw.githubusercontent.com/marcinduma/HOLCLD-2101/main/Kubernetes/Backend/Mariadb/mariadb_deployment.yaml 1.3.2: Check Deployment Status - Use the following command to check if the kubernetes deployment was successfully created or not kubectl get deployment iot-backend-mariadb You should have the output similar to the following screenshot 1.3.3: Check Pod Status - Use the following command to check if the 'iot-backend-mariadb' pod is in ' Running ' state kubectl get pods Caution Kubernetes may take some time to deploy the MariaDB. DO NOT proceed further till the time DB Pod is up. 1.4 Create Kubernetes LoadBalancer Service for MariaDB: Since the MariaDB will be accessed by other services like 'MQTT to DB Agent' and 'REST API Agent'; you need to expose it externally, since 'MQTT to DB Agent' will be running on another Kubernetes Cluster A Kubernetes LoadBalancer Service provides external access to your application from systems outside of Kubernetes. LoadBalancer service is exposed under dedicated VIP address, routable in external network. Traffic directed to this IP address is load balanced by Kubernetes across Kubernetes nodes. Following yaml definition would be used to create the LoadBalancer Service for MariaDB --- apiVersion : v1 kind : Service metadata : name : mariadb-service labels : app : iot-backend spec : ports : - protocol : TCP port : 3306 selector : app : iot-backend tier : mariadb type : \"LoadBalancer\" 1.4.1: Expose MariaDB to other Pods - Create a new kubernetes service using the following command kubectl create -f https://raw.githubusercontent.com/marcinduma/HOLCLD-2101/main/Kubernetes/Backend/Mariadb/mariadb_service.yaml 1.4.2: Verify Service Status - Use the following command to check if the kubernetes service was deployed successfully or not kubectl get service mariadb-service You should have the output similar to the following screenshot 2. Deploy REST API Agent on Kubernetes: The 'REST API Agent' would act as the gateway to the backend application. It will listen to the incoming HTTP requests from the frontend application that you will deploy on AWS. 2.1 Deploy REST API Agent: The following yaml definition will be used to create REST API Agent pods --- apiVersion : apps/v1 # for versions before 1.9.0 use apps/v1beta2 kind : Deployment metadata : name : iot-backend-rest-api-agent labels : app : iot-backend-rest-api-agent spec : replicas : 1 selector : matchLabels : app : iot-backend-rest-api-agent tier : rest-api-agent strategy : type : Recreate template : metadata : labels : app : iot-backend-rest-api-agent tier : rest-api-agent spec : containers : - image : pradeesi/rest_api_agent:v1 name : rest-api-agent env : - name : DB_PASSWORD valueFrom : secretKeyRef : name : mariadb-root-pass key : password 2.1.1: Deploy REST API Agent - Use the following command to create the rest-api-agent kubernetes deployment kubectl create -f https://raw.githubusercontent.com/marcinduma/HOLCLD-2101/main/Kubernetes/Backend/REST_API_Agent/rest_api_agent.yaml 2.1.2: Check Deployment Status - Use the following command to check if the kubernetes deployment was created successfully or not kubectl get deployment iot-backend-rest-api-agent You should have the output similar to the following screenshot 2.1.3: Check Pod Status - Use the following command to check if the 'iot-backend-rest-api-agent' pod is in ' Running ' state kubectl get pods Tip You may check the Pod Logs using the command ' kubectl logs <pod_name> ' 2.2 Create Kubernetes NodePort Service for REST API Agent: Since the frontend app from AWS would access the REST APIs exposed by the 'REST API Agent', you need to create a new kubernetes service for it. The following yaml definition would be used for to create a NodePort Service for the REST API Agent - --- apiVersion : v1 kind : Service metadata : name : rest-api-agent-service labels : app : iot-backend spec : ports : - protocol : TCP port : 5050 selector : app : iot-backend-rest-api-agent tier : rest-api-agent type : \"NodePort\" 2.2.1: Create REST API Agent NodePort Service - You can create a new kubernetes service using the following command kubectl create -f https://raw.githubusercontent.com/marcinduma/HOLCLD-2101/main/Kubernetes/Backend/REST_API_Agent/rest_api_agent_service_node_port.yaml 2.2.2: Check REST API Agent Service Status - You can use the following command to check if the kubernetes service was created successfully or not kubectl get service rest-api-agent-service You should have the output similar to the following screenshot 2.3 Locate the IP and Port to Access Node-Port Service for REST API Agent: You need to find the NodePort and Kubernetes Node external IP to access the 'rest-api-agent. Use the following command to display the port exposed by 'rest-api-agent-service' kubectl get service rest-api-agent-service Use the following command to display the 'External-IP' of you kubernetes nodes kubectl get nodes -o wide Following screenshot highlights the Port and Node IPs in the command outputs Important Note down the Node External IP Address and NodePort Service Port Number. These values will be used in next section for deploying the frontend app as the environment variables values (' BACKEND_HOST ' and ' BACKEND_PORT '). 3. Deploy MQTT to DB Agent on Kubernetes: 'MQTT to DB Agent' will subscribe to the MQTT Topic and listen to the incoming sensor data from AWS IoT platform. It will then parse the sensor data and insert it into the MariaDB. The following yaml definition will be used to create the MQTT to DB Agent pods --- apiVersion : apps/v1 # for versions before 1.9.0 use apps/v1beta2 kind : Deployment metadata : name : iot-backend-mqtt-db-agent labels : app : iot-backend tier : mqtt-db-agent spec : selector : matchLabels : app : iot-backend-mqtt-db-agent strategy : type : Recreate template : metadata : labels : app : iot-backend-mqtt-db-agent spec : containers : - image : pradeesi/mqtt_db_plugin:v2 name : mqtt-db-agent env : - name : DB_PASSWORD valueFrom : secretKeyRef : name : mariadb-root-pass key : password * 3.1: Switch context to AWS EKS Kubernetes Cluster - Change context of kubectl command to access on-premise Kubernetes Cluster. kubectl config use-context aws kubectl config get-contexts 3.2: Create DB Password Secret - Use the following command to create a new secret on your kubernetes cluster kubectl create secret generic mariadb-root-pass --from-literal=password=cisco123 3.3: Verify DB Password Secret - Check if the secret was created successfully or not kubectl get secret mariadb-root-pass You should have the output similar to the following screenshot 3.4: Create external service MQTT needs to send data to database that is deployed in different Kubernetes Cluster. MQTT application is configured to contact with MariaDB using following internal DNS name: mariadb-service . We need to configure Kubernetes to resolve this name to a particular LoadBalancer IP that has been allocated to your mariadb-service in on-premise Kubernetes Cluster. For this we will define service and manually add endpoint that this service will resolve to. In the Endpoint definition you have to specify your LoadBalancer IP address from on-premise Kubernetes Cluster allocated to mariadb-service . --- apiVersion : v1 kind : Service metadata : name : mariadb-service spec : ports : - name : sql protocol : TCP port : 3306 targetPort : 3306 --- apiVersion : v1 kind : Endpoints metadata : name : mariadb-service subsets : - addresses : - ip : LoadBalancerIP ## Specify mariadb-service LoadBalancer IP from step 1.4 ports : - port : 3306 name : sql Download following definition file: wget https://raw.githubusercontent.com/marcinduma/HOLCLD-2101/main/Kubernetes/Backend/MQTT_DB_Agent/mariadb-ext-service-eks.yaml Check string to be replaced by LoadBalancerIP allocated to mariadb-service from Step 1.4.2 cat mariadb-ext-service-eks.yaml Change <mariadb-service_LoadBalancer_IP> with IP address of your load balancer IP, replace 10.200.0.XXX in sed command below with the IP address of LoadBalancer IP allocated to mariadb-service in on-premise Kubernetes Cluster. sed -i 's/<mariadb-service_LoadBalancer_IP>/10.200.0.XXX/g' mariadb-ext-service-eks.yaml Check the manifest file after change of IP: cat mariadb-ext-service-eks.yaml Important Make sure that the IP address of mariadb-service you specified is correct. Apply updated manifest to create external service access: kubectl apply -f mariadb-ext-service-eks.yaml Check services and associated endpoints: kubectl get svc,endpoints 3.5: Deploy MQTT to DB Agent - Use the following command to create mqtt-to-db-agent kubernetes deployment kubectl create -f https://raw.githubusercontent.com/marcinduma/HOLCLD-2101/main/Kubernetes/Backend/MQTT_DB_Agent/mqtt_db_agent_deployment.yaml 3.6: Check Deployment Status - Use the following command to check if the kubernetes deployment was created successfully or not kubectl get deployment iot-backend-mqtt-db-agent You should have the output similar to the following screenshot 3.5: Check Pod Status - Use the following command to check if the 'iot-backend-mqtt-db-agent' pod is in ' Running ' state kubectl get pods 4 Test the REST APIs Exposed by REST API Agent Service: To test the REST API service try to access following url from your web browser (use the node's external ip and service port from the previous section # 2.3) - If you haven't note the IP and port information earlier, please follow those steps: Change kubectl context to on-prem-backend kubectl config use-context admin@on-prem-backend kubectl config get-contexts Use the following command to display the port exposed by 'rest-api-agent-service' kubectl get service rest-api-agent-service Use the following command to display the 'External-IP' of you kubernetes nodes kubectl get nodes -o wide Following screenshot highlights the Port and Node IPs in the command outputs External IP addresses are NATed to 198.18.133.x 1:1. Please replace 10.200.0. in your noted IP to 198.18.133. Now you have open Chrome browser and specify URL based on pattern - http://<kubernetes node's external ip-NAT>:<nodePort> i.e. http://198.18.133.158:31081 If your REST API Agent is working properly, you should see 'Welcome to the API Service...!' message on your browser as shown in the following screenshot - Following are the other urls that you could test - http://<kubernetes node's external ip>:<nodePort>/cities http://<kubernetes node's external ip>:<nodePort>/temperature http://<kubernetes node's external ip>:<nodePort>/humidity http://<kubernetes node's external ip>:<nodePort>/sensor_data/city","title":"Deploy Backend App on Tenant Clusters"},{"location":"deploy_backend/#deploy-the-backend-application-components-on-ccp-kubernetes-cluster-ccp-tenant-cluster","text":"In this section you would deploy the backend components of the IoT Application on the Kubernetes cluster deployed on your CCP instance. Following diagram shows the high-level architecture of these backend application containers","title":"Deploy the Backend Application Components on CCP Kubernetes Cluster (CCP Tenant Cluster)"},{"location":"deploy_backend/#login-to-kubernetes-master-cli-shell","text":"SSH into Linux Jumphost using Putty (198.18.133.10) - use your credentials. From here you will deploy two microservices in on-premise Kubernetes Cluster and one microservice in AWS EKS Kubernetes cluster. You will see how microservices talks to each other and how to establish necessary communication.","title":"Login to Kubernetes Master CLI Shell:"},{"location":"deploy_backend/#1-deploy-mariadb-databse","text":"MariaDB will be used in the backend to save the sensor data received from AWS IoT platform over MQTT protocol. For this we would create following objects - Kubernetes Secret Kubernetes Persistent Volume Claim (PVC) Kubernetes MariaDB Deployment Kubernetes ClusterIP Service (Headless Service) Following diagram shows the relationship between these objects -","title":"1. Deploy MariaDB Databse:"},{"location":"deploy_backend/#11-create-kubernetes-secret-for-mariadb","text":"A Kubernetes Secret is an object that contains a small amount of sensitive data such as a password, a token, or a key. Such information might otherwise be put in a Pod specification or in an image; putting it in a Secret object allows for more control over how it is used, and reduces the risk of accidental exposure. The MariaDB container image uses an environment variable named as 'MYSQL_ROOT_PASSWORD', it hold the root password required to access the database. So you would create a new secret with 'password' key (value as 'cisco123') which would later be used in mariaDB deployment yaml file. 1.1.1: Switch context to on-premise Kubernetes Cluster - Change context of kubectl command to access on-premise Kubernetes Cluster. kubectl config use-context admin@on-prem-backend kubectl config get-contexts 1.1.2: Create DB Password Secret - Use the following command to create a new secret on your kubernetes cluster - kubectl create secret generic mariadb-root-pass --from-literal=password=cisco123 1.1.3: Verify DB Password Secret - Check if the secret was created successfully or not - kubectl get secret mariadb-root-pass You should have the output similar to the following screenshot -","title":"1.1 Create Kubernetes Secret for MariaDB:"},{"location":"deploy_backend/#12-create-kubernetes-persistent-volume-claim-for-mariadb","text":"A Kubernetes Persistent Volume Claim (PVC) is a request for storage by a user. It is similar to a pod. Pods consume node resources and PVCs consume Persistent Volume (PV) resources. Pods can request specific levels of resources (CPU and Memory). Claims can request specific size and access modes (e.g., can be mounted once read/write or many times read-only). To keep the sensor data safe during Pod restarts, you would create a new Persistent Volume Claim. The following yaml definition would be used to create the 'PersistentVolumeClaim' - --- apiVersion : v1 kind : PersistentVolumeClaim metadata : name : mariadb-pv-claim labels : app : iot-backend spec : accessModes : - ReadWriteOnce resources : requests : storage : 2Gi * 1.2.1: Create Persistent Volume Claim - Use the following command to create a new Persistent Volume Claim for MariaDB Pod - kubectl create -f https://raw.githubusercontent.com/marcinduma/HOLCLD-2101/main/Kubernetes/Backend/Mariadb/mariadb_persistent_volume.yaml 1.2.2: Verify Persistent Volume Claim - Check if the PVC was created successfully or not - kubectl get pvc mariadb-pv-claim You should have the output similar to the following screenshot - Caution It can take up to a few minutes for the PVs to be provisioned. DO NOT procced futher till the PVC deployment gets completed.","title":"1.2 Create Kubernetes Persistent Volume Claim for MariaDB:"},{"location":"deploy_backend/#13-deploy-mariadb-on-kubernetes","text":"MariaDB is a community-developed fork of the MySQL relational database management system intended to remain free under the GNU GPL. Development is led by some of the original developers of MySQL, who forked it due to concerns over its acquisition by Oracle Corporation. MariaDB intends to maintain high compatibility with MySQL, ensuring a drop-in replacement capability with library binary parity and exact matching with MySQL APIs and commands. The following yaml definition will be used to deploy MariaDB pod - --- apiVersion : apps/v1 # for versions before 1.9.0 use apps/v1beta2 kind : Deployment metadata : name : iot-backend-mariadb labels : app : iot-backend spec : selector : matchLabels : app : iot-backend tier : mariadb strategy : type : Recreate template : metadata : labels : app : iot-backend tier : mariadb spec : containers : - image : mariadb:10.3 name : mariadb env : - name : MYSQL_ROOT_PASSWORD valueFrom : secretKeyRef : name : mariadb-root-pass key : password ports : - containerPort : 3306 name : mariadb volumeMounts : - name : mariadb-persistent-storage mountPath : /var/lib/mysql volumes : - name : mariadb-persistent-storage persistentVolumeClaim : claimName : mariadb-pv-claim * 1.3.1: Deploy MariaDB - Use the following command to create a MariaDB kubernetes deployment kubectl create -f https://raw.githubusercontent.com/marcinduma/HOLCLD-2101/main/Kubernetes/Backend/Mariadb/mariadb_deployment.yaml 1.3.2: Check Deployment Status - Use the following command to check if the kubernetes deployment was successfully created or not kubectl get deployment iot-backend-mariadb You should have the output similar to the following screenshot 1.3.3: Check Pod Status - Use the following command to check if the 'iot-backend-mariadb' pod is in ' Running ' state kubectl get pods Caution Kubernetes may take some time to deploy the MariaDB. DO NOT proceed further till the time DB Pod is up.","title":"1.3 Deploy MariaDB on Kubernetes:"},{"location":"deploy_backend/#14-create-kubernetes-loadbalancer-service-for-mariadb","text":"Since the MariaDB will be accessed by other services like 'MQTT to DB Agent' and 'REST API Agent'; you need to expose it externally, since 'MQTT to DB Agent' will be running on another Kubernetes Cluster A Kubernetes LoadBalancer Service provides external access to your application from systems outside of Kubernetes. LoadBalancer service is exposed under dedicated VIP address, routable in external network. Traffic directed to this IP address is load balanced by Kubernetes across Kubernetes nodes. Following yaml definition would be used to create the LoadBalancer Service for MariaDB --- apiVersion : v1 kind : Service metadata : name : mariadb-service labels : app : iot-backend spec : ports : - protocol : TCP port : 3306 selector : app : iot-backend tier : mariadb type : \"LoadBalancer\" 1.4.1: Expose MariaDB to other Pods - Create a new kubernetes service using the following command kubectl create -f https://raw.githubusercontent.com/marcinduma/HOLCLD-2101/main/Kubernetes/Backend/Mariadb/mariadb_service.yaml 1.4.2: Verify Service Status - Use the following command to check if the kubernetes service was deployed successfully or not kubectl get service mariadb-service You should have the output similar to the following screenshot","title":"1.4 Create Kubernetes LoadBalancer Service for MariaDB:"},{"location":"deploy_backend/#2-deploy-rest-api-agent-on-kubernetes","text":"The 'REST API Agent' would act as the gateway to the backend application. It will listen to the incoming HTTP requests from the frontend application that you will deploy on AWS.","title":"2. Deploy REST API Agent on Kubernetes:"},{"location":"deploy_backend/#21-deploy-rest-api-agent","text":"The following yaml definition will be used to create REST API Agent pods --- apiVersion : apps/v1 # for versions before 1.9.0 use apps/v1beta2 kind : Deployment metadata : name : iot-backend-rest-api-agent labels : app : iot-backend-rest-api-agent spec : replicas : 1 selector : matchLabels : app : iot-backend-rest-api-agent tier : rest-api-agent strategy : type : Recreate template : metadata : labels : app : iot-backend-rest-api-agent tier : rest-api-agent spec : containers : - image : pradeesi/rest_api_agent:v1 name : rest-api-agent env : - name : DB_PASSWORD valueFrom : secretKeyRef : name : mariadb-root-pass key : password 2.1.1: Deploy REST API Agent - Use the following command to create the rest-api-agent kubernetes deployment kubectl create -f https://raw.githubusercontent.com/marcinduma/HOLCLD-2101/main/Kubernetes/Backend/REST_API_Agent/rest_api_agent.yaml 2.1.2: Check Deployment Status - Use the following command to check if the kubernetes deployment was created successfully or not kubectl get deployment iot-backend-rest-api-agent You should have the output similar to the following screenshot 2.1.3: Check Pod Status - Use the following command to check if the 'iot-backend-rest-api-agent' pod is in ' Running ' state kubectl get pods Tip You may check the Pod Logs using the command ' kubectl logs <pod_name> '","title":"2.1 Deploy REST API Agent:"},{"location":"deploy_backend/#22-create-kubernetes-nodeport-service-for-rest-api-agent","text":"Since the frontend app from AWS would access the REST APIs exposed by the 'REST API Agent', you need to create a new kubernetes service for it. The following yaml definition would be used for to create a NodePort Service for the REST API Agent - --- apiVersion : v1 kind : Service metadata : name : rest-api-agent-service labels : app : iot-backend spec : ports : - protocol : TCP port : 5050 selector : app : iot-backend-rest-api-agent tier : rest-api-agent type : \"NodePort\" 2.2.1: Create REST API Agent NodePort Service - You can create a new kubernetes service using the following command kubectl create -f https://raw.githubusercontent.com/marcinduma/HOLCLD-2101/main/Kubernetes/Backend/REST_API_Agent/rest_api_agent_service_node_port.yaml 2.2.2: Check REST API Agent Service Status - You can use the following command to check if the kubernetes service was created successfully or not kubectl get service rest-api-agent-service You should have the output similar to the following screenshot","title":"2.2 Create Kubernetes NodePort Service for REST API Agent:"},{"location":"deploy_backend/#23-locate-the-ip-and-port-to-access-node-port-service-for-rest-api-agent","text":"You need to find the NodePort and Kubernetes Node external IP to access the 'rest-api-agent. Use the following command to display the port exposed by 'rest-api-agent-service' kubectl get service rest-api-agent-service Use the following command to display the 'External-IP' of you kubernetes nodes kubectl get nodes -o wide Following screenshot highlights the Port and Node IPs in the command outputs Important Note down the Node External IP Address and NodePort Service Port Number. These values will be used in next section for deploying the frontend app as the environment variables values (' BACKEND_HOST ' and ' BACKEND_PORT ').","title":"2.3 Locate the IP and Port to Access Node-Port Service for REST API Agent:"},{"location":"deploy_backend/#3-deploy-mqtt-to-db-agent-on-kubernetes","text":"'MQTT to DB Agent' will subscribe to the MQTT Topic and listen to the incoming sensor data from AWS IoT platform. It will then parse the sensor data and insert it into the MariaDB. The following yaml definition will be used to create the MQTT to DB Agent pods --- apiVersion : apps/v1 # for versions before 1.9.0 use apps/v1beta2 kind : Deployment metadata : name : iot-backend-mqtt-db-agent labels : app : iot-backend tier : mqtt-db-agent spec : selector : matchLabels : app : iot-backend-mqtt-db-agent strategy : type : Recreate template : metadata : labels : app : iot-backend-mqtt-db-agent spec : containers : - image : pradeesi/mqtt_db_plugin:v2 name : mqtt-db-agent env : - name : DB_PASSWORD valueFrom : secretKeyRef : name : mariadb-root-pass key : password * 3.1: Switch context to AWS EKS Kubernetes Cluster - Change context of kubectl command to access on-premise Kubernetes Cluster. kubectl config use-context aws kubectl config get-contexts 3.2: Create DB Password Secret - Use the following command to create a new secret on your kubernetes cluster kubectl create secret generic mariadb-root-pass --from-literal=password=cisco123 3.3: Verify DB Password Secret - Check if the secret was created successfully or not kubectl get secret mariadb-root-pass You should have the output similar to the following screenshot 3.4: Create external service MQTT needs to send data to database that is deployed in different Kubernetes Cluster. MQTT application is configured to contact with MariaDB using following internal DNS name: mariadb-service . We need to configure Kubernetes to resolve this name to a particular LoadBalancer IP that has been allocated to your mariadb-service in on-premise Kubernetes Cluster. For this we will define service and manually add endpoint that this service will resolve to. In the Endpoint definition you have to specify your LoadBalancer IP address from on-premise Kubernetes Cluster allocated to mariadb-service . --- apiVersion : v1 kind : Service metadata : name : mariadb-service spec : ports : - name : sql protocol : TCP port : 3306 targetPort : 3306 --- apiVersion : v1 kind : Endpoints metadata : name : mariadb-service subsets : - addresses : - ip : LoadBalancerIP ## Specify mariadb-service LoadBalancer IP from step 1.4 ports : - port : 3306 name : sql Download following definition file: wget https://raw.githubusercontent.com/marcinduma/HOLCLD-2101/main/Kubernetes/Backend/MQTT_DB_Agent/mariadb-ext-service-eks.yaml Check string to be replaced by LoadBalancerIP allocated to mariadb-service from Step 1.4.2 cat mariadb-ext-service-eks.yaml Change <mariadb-service_LoadBalancer_IP> with IP address of your load balancer IP, replace 10.200.0.XXX in sed command below with the IP address of LoadBalancer IP allocated to mariadb-service in on-premise Kubernetes Cluster. sed -i 's/<mariadb-service_LoadBalancer_IP>/10.200.0.XXX/g' mariadb-ext-service-eks.yaml Check the manifest file after change of IP: cat mariadb-ext-service-eks.yaml Important Make sure that the IP address of mariadb-service you specified is correct. Apply updated manifest to create external service access: kubectl apply -f mariadb-ext-service-eks.yaml Check services and associated endpoints: kubectl get svc,endpoints 3.5: Deploy MQTT to DB Agent - Use the following command to create mqtt-to-db-agent kubernetes deployment kubectl create -f https://raw.githubusercontent.com/marcinduma/HOLCLD-2101/main/Kubernetes/Backend/MQTT_DB_Agent/mqtt_db_agent_deployment.yaml 3.6: Check Deployment Status - Use the following command to check if the kubernetes deployment was created successfully or not kubectl get deployment iot-backend-mqtt-db-agent You should have the output similar to the following screenshot 3.5: Check Pod Status - Use the following command to check if the 'iot-backend-mqtt-db-agent' pod is in ' Running ' state kubectl get pods","title":"3. Deploy MQTT to DB Agent on Kubernetes:"},{"location":"deploy_backend/#4-test-the-rest-apis-exposed-by-rest-api-agent-service","text":"To test the REST API service try to access following url from your web browser (use the node's external ip and service port from the previous section # 2.3) - If you haven't note the IP and port information earlier, please follow those steps: Change kubectl context to on-prem-backend kubectl config use-context admin@on-prem-backend kubectl config get-contexts Use the following command to display the port exposed by 'rest-api-agent-service' kubectl get service rest-api-agent-service Use the following command to display the 'External-IP' of you kubernetes nodes kubectl get nodes -o wide Following screenshot highlights the Port and Node IPs in the command outputs External IP addresses are NATed to 198.18.133.x 1:1. Please replace 10.200.0. in your noted IP to 198.18.133. Now you have open Chrome browser and specify URL based on pattern - http://<kubernetes node's external ip-NAT>:<nodePort> i.e. http://198.18.133.158:31081 If your REST API Agent is working properly, you should see 'Welcome to the API Service...!' message on your browser as shown in the following screenshot - Following are the other urls that you could test - http://<kubernetes node's external ip>:<nodePort>/cities http://<kubernetes node's external ip>:<nodePort>/temperature http://<kubernetes node's external ip>:<nodePort>/humidity http://<kubernetes node's external ip>:<nodePort>/sensor_data/city","title":"4 Test the REST APIs Exposed by REST API Agent Service:"},{"location":"deploy_frontend-aws/","text":"Deploy the Frontend Application Components on AWS In this section you would deploy the frontend components of the IoT Application on the Amazon Cloud. Following diagram shows the high-level architecture of these frontend application containers Login to Kubernetes Master CLI Shell: SSH into Linux Jumphost using Putty (198.18.133.10) - use your credentials. Switch to context 'aws'. kubectl config use-context aws kubectl config get-contexts 1. Deploy frontend-iot: Frontend is nginx web server connected to frontend-server which connect to REST-API Agent over already created Site-2-Site connection. In this section you will configure: Kubernetes ConfigMap Kubernetes frontend-iot Deployment Kubernetes frontend-iot Load-Balancer AWS Service You will create kubernetes deployment for frontend app and expose it to the internet using Kubernetes Load Balancer Service as shown in the following diagram 1.1 Create ConfigMap Create now configmap which will be used by your FrontEnd deployment. Replace ' BACKEND_HOST ' and ' BACKEND_PORT ' in the configmap command (below) with value noted down in section 2.3 in chapter Deploy REST API Agent on Kubernetes . Following screenshot highlights the Port and Node IPs in the command outputs in on-prem cluster. Warning External IP addresses of your on-prem workers can vary from one visible on figure below. Important Note down the Node External IP Address of one of on-prem WORKER and NodePort Service Port Number. ConfigMap create command: kubectl create configmap iot-frontend-config --from-literal=BACKEND_HOST=<NODE_IP> --from-literal=BACKEND_PORT=<PORT> 1.2 Create new deployment: iot-frontend Now is time to deploy our web frontend. Deployment contain two containers in one POD. The frontend server uses existing Site-2-Site connection to on-prem to access data which will be presented in GUI. Monifest file is represented below: --- apiVersion : \"extensions/v1beta1\" kind : \"Deployment\" metadata : name : \"iot-frontend\" namespace : \"default\" labels : app : \"iot-frontend\" spec : replicas : 3 selector : matchLabels : app : \"iot-frontend\" template : metadata : labels : app : \"iot-frontend\" spec : containers : - name : \"frontend-server\" image : \"eu.gcr.io/fwardz001-poc-ci1s/frontend_server:latest\" env : - name : \"BACKEND_HOST\" valueFrom : configMapKeyRef : key : \"BACKEND_HOST\" name : \"iot-frontend-config\" - name : \"BACKEND_PORT\" valueFrom : configMapKeyRef : key : \"BACKEND_PORT\" name : \"iot-frontend-config\" - name : \"nginx-srvr\" image : \"eu.gcr.io/fwardz001-poc-ci1s/nginx_srvr:latest\" --- Create deployment of frontend using command below: kubectl create -f https://raw.githubusercontent.com/marcinduma/HOLCLD-2101/main/Kubernetes/Frontend/frontend_and_nginx_deployment.yaml Check deployment status and associated PODs: kubectl get pods,deployment | egrep \"NAME|iot-front\" 2. Expose the Application by Creating Kubernetes Service: Next step of our deployment is exposure of frontend web to the Internet. By using following manifest file, you will be able to create Service type LoadBalancer in AWS Kubernetes cluster. --- apiVersion : v1 kind : Service metadata : name : frontend-iot-service labels : app : iot-frontend spec : ports : - protocol : TCP port : 80 targetPort : 80 selector : app : iot-frontend type : \"LoadBalancer\" using the following command - kubectl create -f https://raw.githubusercontent.com/marcinduma/HOLCLD-2101/main/Kubernetes/Frontend/frontend-iot-service.yml you get service created and exposed in TCP port 80. 2.1 Check status of newly created service Once service was created, run following command kubectl get svc Copy service name from your output as marked by green frame on the screenshot above. Tip Place the DNS name of service copied to your web browser. It may not work imediately. Please wait few minutes if frontend is not visible yet. If you use webRDP we recommend to copy URL from RDP to your main PC - use Guacamole interface - explained in Appendix: Guacamole. 3. Open the Application Dashboard: 3.1: Use \u201chttp\u201d to open the Dashboard for URL captured in step 2.1. You should see the following webpage in the new tab of your browser. Click on the \"Open Dashboard\" button to open the application dashboard as shown in the following screenshot 3.2: If you see the following web-page with charts filled with data, your application is working :-) Congratulations!!!","title":"Deploy Frontend App on Amazon Cloud"},{"location":"deploy_frontend-aws/#deploy-the-frontend-application-components-on-aws","text":"In this section you would deploy the frontend components of the IoT Application on the Amazon Cloud. Following diagram shows the high-level architecture of these frontend application containers","title":"Deploy the Frontend Application Components on AWS"},{"location":"deploy_frontend-aws/#login-to-kubernetes-master-cli-shell","text":"SSH into Linux Jumphost using Putty (198.18.133.10) - use your credentials. Switch to context 'aws'. kubectl config use-context aws kubectl config get-contexts","title":"Login to Kubernetes Master CLI Shell:"},{"location":"deploy_frontend-aws/#1-deploy-frontend-iot","text":"Frontend is nginx web server connected to frontend-server which connect to REST-API Agent over already created Site-2-Site connection. In this section you will configure: Kubernetes ConfigMap Kubernetes frontend-iot Deployment Kubernetes frontend-iot Load-Balancer AWS Service You will create kubernetes deployment for frontend app and expose it to the internet using Kubernetes Load Balancer Service as shown in the following diagram","title":"1. Deploy frontend-iot:"},{"location":"deploy_frontend-aws/#11-create-configmap","text":"Create now configmap which will be used by your FrontEnd deployment. Replace ' BACKEND_HOST ' and ' BACKEND_PORT ' in the configmap command (below) with value noted down in section 2.3 in chapter Deploy REST API Agent on Kubernetes . Following screenshot highlights the Port and Node IPs in the command outputs in on-prem cluster. Warning External IP addresses of your on-prem workers can vary from one visible on figure below. Important Note down the Node External IP Address of one of on-prem WORKER and NodePort Service Port Number. ConfigMap create command: kubectl create configmap iot-frontend-config --from-literal=BACKEND_HOST=<NODE_IP> --from-literal=BACKEND_PORT=<PORT>","title":"1.1 Create ConfigMap"},{"location":"deploy_frontend-aws/#12-create-new-deployment-iot-frontend","text":"Now is time to deploy our web frontend. Deployment contain two containers in one POD. The frontend server uses existing Site-2-Site connection to on-prem to access data which will be presented in GUI. Monifest file is represented below: --- apiVersion : \"extensions/v1beta1\" kind : \"Deployment\" metadata : name : \"iot-frontend\" namespace : \"default\" labels : app : \"iot-frontend\" spec : replicas : 3 selector : matchLabels : app : \"iot-frontend\" template : metadata : labels : app : \"iot-frontend\" spec : containers : - name : \"frontend-server\" image : \"eu.gcr.io/fwardz001-poc-ci1s/frontend_server:latest\" env : - name : \"BACKEND_HOST\" valueFrom : configMapKeyRef : key : \"BACKEND_HOST\" name : \"iot-frontend-config\" - name : \"BACKEND_PORT\" valueFrom : configMapKeyRef : key : \"BACKEND_PORT\" name : \"iot-frontend-config\" - name : \"nginx-srvr\" image : \"eu.gcr.io/fwardz001-poc-ci1s/nginx_srvr:latest\" --- Create deployment of frontend using command below: kubectl create -f https://raw.githubusercontent.com/marcinduma/HOLCLD-2101/main/Kubernetes/Frontend/frontend_and_nginx_deployment.yaml Check deployment status and associated PODs: kubectl get pods,deployment | egrep \"NAME|iot-front\"","title":"1.2 Create new deployment: iot-frontend"},{"location":"deploy_frontend-aws/#2-expose-the-application-by-creating-kubernetes-service","text":"Next step of our deployment is exposure of frontend web to the Internet. By using following manifest file, you will be able to create Service type LoadBalancer in AWS Kubernetes cluster. --- apiVersion : v1 kind : Service metadata : name : frontend-iot-service labels : app : iot-frontend spec : ports : - protocol : TCP port : 80 targetPort : 80 selector : app : iot-frontend type : \"LoadBalancer\" using the following command - kubectl create -f https://raw.githubusercontent.com/marcinduma/HOLCLD-2101/main/Kubernetes/Frontend/frontend-iot-service.yml you get service created and exposed in TCP port 80.","title":"2. Expose the Application by Creating Kubernetes Service:"},{"location":"deploy_frontend-aws/#21-check-status-of-newly-created-service","text":"Once service was created, run following command kubectl get svc Copy service name from your output as marked by green frame on the screenshot above. Tip Place the DNS name of service copied to your web browser. It may not work imediately. Please wait few minutes if frontend is not visible yet. If you use webRDP we recommend to copy URL from RDP to your main PC - use Guacamole interface - explained in Appendix: Guacamole.","title":"2.1 Check status of newly created service"},{"location":"deploy_frontend-aws/#3-open-the-application-dashboard","text":"3.1: Use \u201chttp\u201d to open the Dashboard for URL captured in step 2.1. You should see the following webpage in the new tab of your browser. Click on the \"Open Dashboard\" button to open the application dashboard as shown in the following screenshot 3.2: If you see the following web-page with charts filled with data, your application is working :-) Congratulations!!!","title":"3. Open the Application Dashboard:"},{"location":"deploy_frontend/","text":"Deploy the Frontend Application Components on Google Kubernetes Engine (GKE) In this section you would deploy the frontend components of the IoT Application on the Google Kubernetes Engine. Following diagram shows the high-level architecture of these frontend application containers - 1. Login to Google Cloud Console and Open Kubernetes Engine: Login to Google Cloud Console using the credentials from credentials page. You can find the details of login method at this link - Google Cloud Access 2. Creating Deployment Definition using GKE UI on Google Cloud Console: You will create kubernetes deployment for frontend app and expose it to the internet using Kubernetes Load Balancer Service as shown in the following diagram - 2.1: Select the ' Workloads ' option on \" Google Cloud Console --> Kubernetes Engine \" page, and click on the ' Deploy ' button as shown in the following screenshot - 3. Add 'frontend_server' Container Image to the Deployment Definition: 3.1: Select the ' Existing container image ' radio button on the 'Create Deployment' page and then click on 'SELECT' button to select the image as shown in the following screenshot - 3.2: Select the ' frontend_server ' container image from the pop-up window and click on the ' SELECT ' button as shown in the following screenshot - 4. Add Environment Variables to the 'frontend_server' Container: 4.1: Click on the '+ Add environment variable' button to add the environment variables for the 'frontend_server' container as shown in the following screenshot - 4.2: Add ' BACKEND_HOST ' and ' BACKEND_PORT ' variables as shown in the following screenshot (Use the values for 'BACKEND_HOST' and 'BACKEND_PORT' from the REST API Agent NodePort Service created earlier) - 5. Add Second Container Image ('nginx_srvr') to the Deployment Definition: 5.1: After clicking on the ' + Add Container ' button (shown in the previous screenshot), click again on the ' Existing ontainer image ' and click on the ' SELECT ' button. It will display the popup window. For this popup window, select the ' nginx_srvr ' image with 'latest' tag. 5.2: Click on the 'CONTINUE' button as shown in the following screenshot - 6. Add Application Name, Select Cluster and Deploy the Application: 6.1: Change the application name to ' iot-frontend-user-X ' (Replace X with you POD number) and select the 'Cluster' from the drop down menu. Now you can click on the 'Deploy' button as shown in the following screenshot - Importnat: Application deployment may take some time. Wait for it's completion before proceeding with the next steps. 7. Expose the Application by Creating Kubernetes Service: Click on ' Workloads ' option from the left panel on the GKE Dashboard. 7.1: Click on your Workload name ' iot-frontend-user-X ' (Kubernetes Deployment) as shown in the following screenshot - 7.2: Click on the ' Deploy ' button as shown in the following screenshot - 7.3: In the 'New Port Mapping' set ' Port ' and ' Target Port ' as ' 80 ' and click on the ' Done ' button. Make sure the ' Service type ' is ' Load balancer ' and click on the ' Expose ' button as shown in the following screenshot - Important: Service deployment may take some time. Wait for it before proceeding with the next steps. 8. Open the Application Dashboard: 8.1: Go to ' Kubernetes Engine --> Services ' and click on the ' Endpoints ' respective to your kubernetes service as shown in the following screenshot - 8.2: You should see the following webpage in the new tab of your browser. Click on the \"Open Dashboard\" button to open the application dashboard as shown in the following screenshot - 8.3: If you see the following web-page with charts filled with data, your application is working :-) Congratulations!!!","title":"Deploy the Frontend Application Components on Google Kubernetes Engine (GKE)"},{"location":"deploy_frontend/#deploy-the-frontend-application-components-on-google-kubernetes-engine-gke","text":"In this section you would deploy the frontend components of the IoT Application on the Google Kubernetes Engine. Following diagram shows the high-level architecture of these frontend application containers -","title":"Deploy the Frontend Application Components on Google Kubernetes Engine (GKE)"},{"location":"deploy_frontend/#1-login-to-google-cloud-console-and-open-kubernetes-engine","text":"Login to Google Cloud Console using the credentials from credentials page. You can find the details of login method at this link - Google Cloud Access","title":"1. Login to Google Cloud Console and Open Kubernetes Engine:"},{"location":"deploy_frontend/#2-creating-deployment-definition-using-gke-ui-on-google-cloud-console","text":"You will create kubernetes deployment for frontend app and expose it to the internet using Kubernetes Load Balancer Service as shown in the following diagram - 2.1: Select the ' Workloads ' option on \" Google Cloud Console --> Kubernetes Engine \" page, and click on the ' Deploy ' button as shown in the following screenshot -","title":"2. Creating Deployment Definition using GKE UI on Google Cloud Console:"},{"location":"deploy_frontend/#3-add-frontend_server-container-image-to-the-deployment-definition","text":"3.1: Select the ' Existing container image ' radio button on the 'Create Deployment' page and then click on 'SELECT' button to select the image as shown in the following screenshot - 3.2: Select the ' frontend_server ' container image from the pop-up window and click on the ' SELECT ' button as shown in the following screenshot -","title":"3. Add 'frontend_server' Container Image to the Deployment Definition:"},{"location":"deploy_frontend/#4-add-environment-variables-to-the-frontend_server-container","text":"4.1: Click on the '+ Add environment variable' button to add the environment variables for the 'frontend_server' container as shown in the following screenshot - 4.2: Add ' BACKEND_HOST ' and ' BACKEND_PORT ' variables as shown in the following screenshot (Use the values for 'BACKEND_HOST' and 'BACKEND_PORT' from the REST API Agent NodePort Service created earlier) -","title":"4. Add Environment Variables to the 'frontend_server' Container:"},{"location":"deploy_frontend/#5-add-second-container-image-nginx_srvr-to-the-deployment-definition","text":"5.1: After clicking on the ' + Add Container ' button (shown in the previous screenshot), click again on the ' Existing ontainer image ' and click on the ' SELECT ' button. It will display the popup window. For this popup window, select the ' nginx_srvr ' image with 'latest' tag. 5.2: Click on the 'CONTINUE' button as shown in the following screenshot -","title":"5. Add Second Container Image ('nginx_srvr') to the Deployment Definition:"},{"location":"deploy_frontend/#6-add-application-name-select-cluster-and-deploy-the-application","text":"6.1: Change the application name to ' iot-frontend-user-X ' (Replace X with you POD number) and select the 'Cluster' from the drop down menu. Now you can click on the 'Deploy' button as shown in the following screenshot - Importnat: Application deployment may take some time. Wait for it's completion before proceeding with the next steps.","title":"6. Add Application Name, Select Cluster and Deploy the Application:"},{"location":"deploy_frontend/#7-expose-the-application-by-creating-kubernetes-service","text":"Click on ' Workloads ' option from the left panel on the GKE Dashboard. 7.1: Click on your Workload name ' iot-frontend-user-X ' (Kubernetes Deployment) as shown in the following screenshot - 7.2: Click on the ' Deploy ' button as shown in the following screenshot - 7.3: In the 'New Port Mapping' set ' Port ' and ' Target Port ' as ' 80 ' and click on the ' Done ' button. Make sure the ' Service type ' is ' Load balancer ' and click on the ' Expose ' button as shown in the following screenshot - Important: Service deployment may take some time. Wait for it before proceeding with the next steps.","title":"7. Expose the Application by Creating Kubernetes Service:"},{"location":"deploy_frontend/#8-open-the-application-dashboard","text":"8.1: Go to ' Kubernetes Engine --> Services ' and click on the ' Endpoints ' respective to your kubernetes service as shown in the following screenshot - 8.2: You should see the following webpage in the new tab of your browser. Click on the \"Open Dashboard\" button to open the application dashboard as shown in the following screenshot - 8.3: If you see the following web-page with charts filled with data, your application is working :-) Congratulations!!!","title":"8. Open the Application Dashboard:"},{"location":"guacamole/","text":"Copy and Paste in and out Guacamole Apache Guacamole serves as a proxy to provide clientless access to other servers. The copy paste function is achieved through browser. Guacamole Menu The guacamole menu is a sidebar which is hidden. On a Windows device with an external keyboard, the guacamole menu is displayed by pressing Ctrl+Alt+Shift On a Mac device with an external keyboard, the guacamole menu is displayed by pressing Ctrl+Command+Shift 3. On a device that doesn\u2019t have a keyboard, e.g. mobile or touchscreen device, the guacamole menu is displayed by swiping right from the left edge of the screen. To Hide the guacamole menu, press Ctrl+Alt+Shift/ Ctrl+Command+Shift or swipe left across the screen again. Guacamole Use The Clipboard text area functions as an interface between the remote clipboard and local clipboard. Text from the local clipboard can be pasted into the text area, and the text is sent to the remote clipboard. 2. After the text is copied to the remote clipboard, the Guacamole menu can be closed. The text from the clipboard can be pasted in the desired location. 3. Similarly, copying text from the remote desktop sends the text to the Clipboard text area on the Guacamole Menu, which can then be copied to the local clipboard.","title":"Guacamole"},{"location":"guacamole/#copy-and-paste-in-and-out-guacamole","text":"Apache Guacamole serves as a proxy to provide clientless access to other servers. The copy paste function is achieved through browser.","title":"Copy and Paste in and out Guacamole"},{"location":"guacamole/#guacamole-menu","text":"The guacamole menu is a sidebar which is hidden. On a Windows device with an external keyboard, the guacamole menu is displayed by pressing Ctrl+Alt+Shift On a Mac device with an external keyboard, the guacamole menu is displayed by pressing Ctrl+Command+Shift 3. On a device that doesn\u2019t have a keyboard, e.g. mobile or touchscreen device, the guacamole menu is displayed by swiping right from the left edge of the screen. To Hide the guacamole menu, press Ctrl+Alt+Shift/ Ctrl+Command+Shift or swipe left across the screen again.","title":"Guacamole Menu"},{"location":"guacamole/#guacamole-use","text":"The Clipboard text area functions as an interface between the remote clipboard and local clipboard. Text from the local clipboard can be pasted into the text area, and the text is sent to the remote clipboard. 2. After the text is copied to the remote clipboard, the Guacamole menu can be closed. The text from the clipboard can be pasted in the desired location. 3. Similarly, copying text from the remote desktop sends the text to the Clipboard text area on the Guacamole Menu, which can then be copied to the local clipboard.","title":"Guacamole Use"},{"location":"kubernetes_basics/","text":"Appendix - 3: Kubernetes Basic Docs 1. Install and Configure kubectl For information on \"kubectl\" installation and configuration use the following document - https://kubernetes.io/docs/tasks/tools/install-kubectl/ 2. Google Cloud Container Registry: Pushing and Pulling Container Images To push or pull the container images to/from Google Container Registry you can follow the following article - https://cloud.google.com/container-registry/docs/pushing-and-pulling Note: You need to login to your Google Cloud account before pushing the container image. You can do it using the following command - gcloud auth login If you have multiple projects, you can select a specific one using the following command - gcloud config set project <Project_ID> Kubernetes Cheat Sheet: https://kubernetes.io/docs/reference/kubectl/cheatsheet/","title":"Appendix - 3: Kubernetes Basic Docs"},{"location":"kubernetes_basics/#appendix-3-kubernetes-basic-docs","text":"","title":"Appendix - 3: Kubernetes Basic Docs"},{"location":"kubernetes_basics/#1-install-and-configure-kubectl","text":"For information on \"kubectl\" installation and configuration use the following document - https://kubernetes.io/docs/tasks/tools/install-kubectl/","title":"1. Install and Configure kubectl"},{"location":"kubernetes_basics/#2-google-cloud-container-registry-pushing-and-pulling-container-images","text":"To push or pull the container images to/from Google Container Registry you can follow the following article - https://cloud.google.com/container-registry/docs/pushing-and-pulling Note: You need to login to your Google Cloud account before pushing the container image. You can do it using the following command - gcloud auth login If you have multiple projects, you can select a specific one using the following command - gcloud config set project <Project_ID>","title":"2. Google Cloud Container Registry: Pushing and Pulling Container Images"},{"location":"kubernetes_basics/#kubernetes-cheat-sheet","text":"https://kubernetes.io/docs/reference/kubectl/cheatsheet/","title":"Kubernetes Cheat Sheet:"},{"location":"security_policies/","text":"Applying Kubernetes Network Policies to Secure the Application A network policy is a specification of how groups of pods are allowed to communicate with each other and other network endpoints. NetworkPolicy resources use labels to select pods and define rules which specify what traffic is allowed to the selected pods. By now you should have your Hybrid Cloud IoT Application working end to end. Let's apply some Kubernetes Network Policies to allow traffic from the frontend app to port '5050' only (REST API AGENT container accepts HTTP requests on port '5050'). 1. Apply Deny All Network Policy: Following Kubernetes Network Policy yaml definition would block all the traffic coming towards REST API Agent - apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: deny-all-rest-api-agent spec: podSelector: matchLabels: app: iot-backend-rest-api-agent tier: rest-api-agent policyTypes: - Ingress 1.0: Prerequisite - set kubectl context to on-prem-1 and make sure that your default namespace is set to your student ID. kubectl config use-context admin@on-prem-backend kubectl config get-contexts 1.1: Execute the following command on Kubernetes master node to apply this Network Policy on your REST API Agent - kubectl create -f https://raw.githubusercontent.com/marcinduma/HOLCLD-2101/main/Kubernetes/Backend/Network_Policies/deny_all_rest_api_agent.yaml 1.2: Now try to referesh your Frontend App webpage. It should stop working. 2. Apply Permit Port 5111 Network Policy: Following Kubernetes Network Policy yaml definition would allow the traffic on port 5111 to REST API Agent - apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: permit-port-5111-rest-api-agent spec: podSelector: matchLabels: app: iot-backend-rest-api-agent tier: rest-api-agent policyTypes: - Ingress ingress: - from: [] ports: - protocol: TCP port: 5111 2.1: Execute the following command on Kubernetes master node to apply this Network Policy on your REST API Agent - kubectl create -f https://raw.githubusercontent.com/marcinduma/HOLCLD-2101/main/Kubernetes/Backend/Network_Policies/permit_port_5111_rest_api_agent.yaml 2.2: Now try to referesh your Frontend App webpage. Does it work? Why? 3. Apply Permit Port 5050 Network Policy: Following Kubernetes Network Policy yaml definition would allow the traffic on port 5050 to REST API Agent - apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: permit-port-5050-rest-api-agent spec: podSelector: matchLabels: app: iot-backend-rest-api-agent tier: rest-api-agent policyTypes: - Ingress ingress: - from: [] ports: - protocol: TCP port: 5050 3.1: Execute the following command on Kubernetes master node to apply this Network Policy on your REST API Agent - kubectl create -f https://raw.githubusercontent.com/marcinduma/HOLCLD-2101/main/Kubernetes/Backend/Network_Policies/permit_port_5050_rest_api_agent.yaml 3.2: Now try to referesh your Frontend App webpage. Does it work? Why? Note: Other command related to Network Policy that you may use - Display Kubernetes Network Policies - kubectl get NetworkPolicies Display Network Policy Details - kubectl describe NetworkPolicy <Network Policy Name> Delete Network Policy - kubectl delete NetworkPolicy <Network Policy Name>","title":"Apply Security Policy to REST API Agent"},{"location":"security_policies/#applying-kubernetes-network-policies-to-secure-the-application","text":"A network policy is a specification of how groups of pods are allowed to communicate with each other and other network endpoints. NetworkPolicy resources use labels to select pods and define rules which specify what traffic is allowed to the selected pods. By now you should have your Hybrid Cloud IoT Application working end to end. Let's apply some Kubernetes Network Policies to allow traffic from the frontend app to port '5050' only (REST API AGENT container accepts HTTP requests on port '5050').","title":"Applying Kubernetes Network Policies to Secure the Application"},{"location":"security_policies/#1-apply-deny-all-network-policy","text":"Following Kubernetes Network Policy yaml definition would block all the traffic coming towards REST API Agent - apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: deny-all-rest-api-agent spec: podSelector: matchLabels: app: iot-backend-rest-api-agent tier: rest-api-agent policyTypes: - Ingress 1.0: Prerequisite - set kubectl context to on-prem-1 and make sure that your default namespace is set to your student ID. kubectl config use-context admin@on-prem-backend kubectl config get-contexts 1.1: Execute the following command on Kubernetes master node to apply this Network Policy on your REST API Agent - kubectl create -f https://raw.githubusercontent.com/marcinduma/HOLCLD-2101/main/Kubernetes/Backend/Network_Policies/deny_all_rest_api_agent.yaml 1.2: Now try to referesh your Frontend App webpage. It should stop working.","title":"1. Apply Deny All Network Policy:"},{"location":"security_policies/#2-apply-permit-port-5111-network-policy","text":"Following Kubernetes Network Policy yaml definition would allow the traffic on port 5111 to REST API Agent - apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: permit-port-5111-rest-api-agent spec: podSelector: matchLabels: app: iot-backend-rest-api-agent tier: rest-api-agent policyTypes: - Ingress ingress: - from: [] ports: - protocol: TCP port: 5111 2.1: Execute the following command on Kubernetes master node to apply this Network Policy on your REST API Agent - kubectl create -f https://raw.githubusercontent.com/marcinduma/HOLCLD-2101/main/Kubernetes/Backend/Network_Policies/permit_port_5111_rest_api_agent.yaml 2.2: Now try to referesh your Frontend App webpage. Does it work? Why?","title":"2. Apply Permit Port 5111 Network Policy:"},{"location":"security_policies/#3-apply-permit-port-5050-network-policy","text":"Following Kubernetes Network Policy yaml definition would allow the traffic on port 5050 to REST API Agent - apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: permit-port-5050-rest-api-agent spec: podSelector: matchLabels: app: iot-backend-rest-api-agent tier: rest-api-agent policyTypes: - Ingress ingress: - from: [] ports: - protocol: TCP port: 5050 3.1: Execute the following command on Kubernetes master node to apply this Network Policy on your REST API Agent - kubectl create -f https://raw.githubusercontent.com/marcinduma/HOLCLD-2101/main/Kubernetes/Backend/Network_Policies/permit_port_5050_rest_api_agent.yaml 3.2: Now try to referesh your Frontend App webpage. Does it work? Why? Note: Other command related to Network Policy that you may use - Display Kubernetes Network Policies - kubectl get NetworkPolicies Display Network Policy Details - kubectl describe NetworkPolicy <Network Policy Name> Delete Network Policy - kubectl delete NetworkPolicy <Network Policy Name>","title":"3. Apply Permit Port 5050 Network Policy:"}]}