{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to our Cisco Live Hands-on Lab Speakers: Faisal Chaudhry , Distinguished Engineer, Cisco Systems, Inc. - Distinguished Speaker Filip Wardzichowski , Software Engineering Technical Leader, Cisco Systems, Inc. - Distinguished Speaker Marcin Duma , Customer Delivery Architect, Cisco Systems, Inc. Containerized Applications on Hybrid Cloud Environments Nowadays application world is very dynamic. Changes in the infrastructures happens fast, requirements changes even faster. Application components starts to be deployed in different locations based on various technical and business requirements. It's nothing rare to deploy different layers of applications in physically different locations. Rate of use of Public Clouds vs on premise resources changes. Public Cloud become more and more considered for applications which can be hosted outside of own physicall capacity. Rate of containeraized applications grow. Customers are deploying Kubernetes to orchestrate tenants, where they run their systems. Very often having dozens of Kubernetes deployments is nothing strange. However management of them become complicated. Important is to have control at your overall environment. Cisco Intersight gives the possibility to deploy, orchestrate and provide a controll for all your Kubernetes clusters regardless of locations they are deployed (Roadmap) . High Level Design of Lab scenario. During this lab session you will be able to experience of deployment multi-cloud application run on Kubernetes tenants. Three tier application is running in two different locations: On-Prem AWS Components of the application and their interconnection is presented on the figure below. By using Cisco Intersight, you will be able to operate Kubernetes clusters in vSphere - on premise. You will manage each of Kubernetes tenant from central terminal server running Kubernetes commands to deploy our App. Outcome of the lab is showing to attendees how to leverage Cisco Intersight and Kubernetes add-ons in daily K8s operations. In the end of excercise you finally will see the results of Web-based interface showing you data processes by the app. Enjoy!","title":"Home"},{"location":"#welcome-to-our-cisco-live-hands-on-lab","text":"Speakers: Faisal Chaudhry , Distinguished Engineer, Cisco Systems, Inc. - Distinguished Speaker Filip Wardzichowski , Software Engineering Technical Leader, Cisco Systems, Inc. - Distinguished Speaker Marcin Duma , Customer Delivery Architect, Cisco Systems, Inc.","title":"Welcome to our Cisco Live Hands-on Lab"},{"location":"#containerized-applications-on-hybrid-cloud-environments","text":"Nowadays application world is very dynamic. Changes in the infrastructures happens fast, requirements changes even faster. Application components starts to be deployed in different locations based on various technical and business requirements. It's nothing rare to deploy different layers of applications in physically different locations. Rate of use of Public Clouds vs on premise resources changes. Public Cloud become more and more considered for applications which can be hosted outside of own physicall capacity. Rate of containeraized applications grow. Customers are deploying Kubernetes to orchestrate tenants, where they run their systems. Very often having dozens of Kubernetes deployments is nothing strange. However management of them become complicated. Important is to have control at your overall environment. Cisco Intersight gives the possibility to deploy, orchestrate and provide a controll for all your Kubernetes clusters regardless of locations they are deployed (Roadmap) .","title":"Containerized Applications on Hybrid Cloud Environments"},{"location":"#high-level-design-of-lab-scenario","text":"During this lab session you will be able to experience of deployment multi-cloud application run on Kubernetes tenants. Three tier application is running in two different locations: On-Prem AWS Components of the application and their interconnection is presented on the figure below. By using Cisco Intersight, you will be able to operate Kubernetes clusters in vSphere - on premise. You will manage each of Kubernetes tenant from central terminal server running Kubernetes commands to deploy our App. Outcome of the lab is showing to attendees how to leverage Cisco Intersight and Kubernetes add-ons in daily K8s operations. In the end of excercise you finally will see the results of Web-based interface showing you data processes by the app. Enjoy!","title":"High Level Design of Lab scenario."},{"location":"LAB_access/","text":"Connectivity Check 1. Lab access general description The lab has been built leveraging multiple cloud environments as following: Amazon Web Services Private intrastructure on-prem You will have access to Cisco Intersight GUI, where you will see Kubernetes Clusters deployed On-Prem. In the end you will manage your application that will be deployed in 2 different environments. The infrastructure between On-prem and AWS is ready and functioning. In this lab you will see how to connect microservices together to make whole application work. Most of the tasks you will do from Linux Jumphost that is running on-premise. From there you will deploy components of your application in Kubernetes Cluster in AWS and on-prem. 2. Cisco dCloud dashboard The entire lab for the session is built using Cisco dCloud environment. Access to the Session will be provided by the proctor assigned to you. Access session with webRDP Once logged to the dCloud session, you will see dashboard like on the following picture: To open WebRDP follow the procedure from the figures: 1) Click on the blue triangle highlighted on figure below 2) Follow to \"Remote Desktop\" by using link in red frame on the figure: When you click on \"Remote Desktop button, browser will open new TAB with access to Windows desktop. The webRDP has installed Chrome as web browser, from where you get access to Cisco Intersight page. To access CSR router and Linux jumphost, use Putty installed - shortcut is on Desktop. Info Please do not use \"Remote Desktop\" for other devices from the list at Network tab. ONLY win2k16 can be accessed that way. Tip When you use webRDP you are still able to copy/paste between your 'main PC' and webRDP interface. You can use Guacamole interface - explained in Appendix: Guacamole. 3. Accessing Linux Jumphost Open PuTTY client on webRDP taskbar. PuTTY has pre-defined session to CSR router as well as to ubuntu-terminal. Use predefined ubuntu-terminal session by selecting it and click Open button. Username: dcloud User password: C1sco12345 4. Accessing Cisco Intersight Platform Cisco Intersight Platform manages Kuberenetes clusters in the private infrasturcture. You will have access to dedicated instance of Cisco Intersight, from which you will manage your own Kuberenetes Clusters used later on to deploy application. Please find login credentials and URL to your CCP instance below: URL: https://intersight.com/ User name: holcld2611+pod'X'@gmail.com Where X will be provided by proctor User password: CiscoLive!2022 Warning You can explore Cisco Intersight through the GUI, but please do not delete content already created. 5. Accessing Cisco Intersight Assist Cisco Intersight requires installation of local (on-prem) satelite system. Cisco Intersight Assist collect data from local vCenter and send it to Cisco Intersight PaaS in secure manner. You can login to local Cisco Intersight Assist using credentials provided below: URL: https://iva.dcloud.cisco.com User name: admin Where X will be provided by proctor User password: C1sco12345 Warning You can explore Cisco Intersight Assist through the GUI, but please do not delete content already created. 5. Accessing CSR1kv Lab router Your session contain Cisco CSR1kv router. It is used to terminate Site-2-Site tunnels with AWS tenant. All configuration is ready and doesn't require any modifications. Please find login credentials and IP address to your CSR1kv router below - use PuTTY predefined session: User name: admin User password: C1sco12345 Warning Do not delete configuration already existing on the router. 6. Accessing vCenter for Lab Whole setup is done on ESXi in Cisco dCloud environment. During the lab you don't need to perform actions on vCenter itself. However for case of troubleshooting or exploration, credentials to your vCenter below. URL: https://vc1.dcloud.cisco.com/ui User name: administrator@vsphere.local User password: C1sco12345! Warning Do not delete configuration nor VM machines already existing on the vCenter.","title":"Accessing the Lab Environment"},{"location":"LAB_access/#connectivity-check","text":"","title":"Connectivity Check"},{"location":"LAB_access/#1-lab-access-general-description","text":"The lab has been built leveraging multiple cloud environments as following: Amazon Web Services Private intrastructure on-prem You will have access to Cisco Intersight GUI, where you will see Kubernetes Clusters deployed On-Prem. In the end you will manage your application that will be deployed in 2 different environments. The infrastructure between On-prem and AWS is ready and functioning. In this lab you will see how to connect microservices together to make whole application work. Most of the tasks you will do from Linux Jumphost that is running on-premise. From there you will deploy components of your application in Kubernetes Cluster in AWS and on-prem.","title":"1. Lab access general description"},{"location":"LAB_access/#2-cisco-dcloud-dashboard","text":"The entire lab for the session is built using Cisco dCloud environment. Access to the Session will be provided by the proctor assigned to you.","title":"2. Cisco dCloud dashboard"},{"location":"LAB_access/#access-session-with-webrdp","text":"Once logged to the dCloud session, you will see dashboard like on the following picture: To open WebRDP follow the procedure from the figures: 1) Click on the blue triangle highlighted on figure below 2) Follow to \"Remote Desktop\" by using link in red frame on the figure: When you click on \"Remote Desktop button, browser will open new TAB with access to Windows desktop. The webRDP has installed Chrome as web browser, from where you get access to Cisco Intersight page. To access CSR router and Linux jumphost, use Putty installed - shortcut is on Desktop. Info Please do not use \"Remote Desktop\" for other devices from the list at Network tab. ONLY win2k16 can be accessed that way. Tip When you use webRDP you are still able to copy/paste between your 'main PC' and webRDP interface. You can use Guacamole interface - explained in Appendix: Guacamole.","title":"Access session with webRDP"},{"location":"LAB_access/#3-accessing-linux-jumphost","text":"Open PuTTY client on webRDP taskbar. PuTTY has pre-defined session to CSR router as well as to ubuntu-terminal. Use predefined ubuntu-terminal session by selecting it and click Open button. Username: dcloud User password: C1sco12345","title":"3. Accessing Linux Jumphost"},{"location":"LAB_access/#4-accessing-cisco-intersight-platform","text":"Cisco Intersight Platform manages Kuberenetes clusters in the private infrasturcture. You will have access to dedicated instance of Cisco Intersight, from which you will manage your own Kuberenetes Clusters used later on to deploy application. Please find login credentials and URL to your CCP instance below: URL: https://intersight.com/ User name: holcld2611+pod'X'@gmail.com Where X will be provided by proctor User password: CiscoLive!2022 Warning You can explore Cisco Intersight through the GUI, but please do not delete content already created.","title":"4. Accessing Cisco Intersight Platform"},{"location":"LAB_access/#5-accessing-cisco-intersight-assist","text":"Cisco Intersight requires installation of local (on-prem) satelite system. Cisco Intersight Assist collect data from local vCenter and send it to Cisco Intersight PaaS in secure manner. You can login to local Cisco Intersight Assist using credentials provided below: URL: https://iva.dcloud.cisco.com User name: admin Where X will be provided by proctor User password: C1sco12345 Warning You can explore Cisco Intersight Assist through the GUI, but please do not delete content already created.","title":"5. Accessing Cisco Intersight Assist"},{"location":"LAB_access/#5-accessing-csr1kv-lab-router","text":"Your session contain Cisco CSR1kv router. It is used to terminate Site-2-Site tunnels with AWS tenant. All configuration is ready and doesn't require any modifications. Please find login credentials and IP address to your CSR1kv router below - use PuTTY predefined session: User name: admin User password: C1sco12345 Warning Do not delete configuration already existing on the router.","title":"5. Accessing CSR1kv Lab router"},{"location":"LAB_access/#6-accessing-vcenter-for-lab","text":"Whole setup is done on ESXi in Cisco dCloud environment. During the lab you don't need to perform actions on vCenter itself. However for case of troubleshooting or exploration, credentials to your vCenter below. URL: https://vc1.dcloud.cisco.com/ui User name: administrator@vsphere.local User password: C1sco12345! Warning Do not delete configuration nor VM machines already existing on the vCenter.","title":"6. Accessing vCenter for Lab"},{"location":"backend_exercise/","text":"Explore Backend App and Kubernetes Dashboard: Your Hybrid Cloud App's backend compoents (MariaDB, MQTT DB Agent, and REST API agent) should be up and running. Now let's explore some low level details to understand the application containers and kubernetes better. Task 0: Tasks in this section will be executed in Kubernetes on-premise. Change kubectl context to the correct one. Execute command below on linux jumphost: kubectl config use-context admin@on-prem-backend Task 1: Find the size of the 'Persistent Volume Claim' used for MariaDB database? To check 'CAPACITY' value of PVC execute this command on linux jumphost: kubectl get pvc mariadb-pv-claim Task 2: Login to the MariaDB database and explore the data tables. Find the pod name for 'MariaDB' using the kubectl command kubectl get pods and locate the pod name starting with 'iot-backend-mariadb-'. Login to the MariaDB container using the kubectl command kubectl exec -it <pod name> -- /bin/bash Tip (replace the <pod name> in command above with correct value). Inside the container execute ss -tulpn and check the port MariaDB is listing to for incoming connections. You can also check the version of MariaDB using the shell command: mysql --version Now connect to the MariaDB from the container shell and check Databases and Tables. Use mysql -u root -pcisco123 command to login to MaraDB. On mariaDB shell, use show databases; command to list all the databases. Look for ' sensor_db ' in the output (this is the database where we are storing the sensor data). Switch to sensor_db using the command use sensor_db; and list all the tables in this database using the command show tables; You should see only one table with the name 'sensor_data' ; Try to list the data from this table using the SQL statement select * from sensor_data; Now check the record count in this table using the sql statement: select count(*) from sensor_data; Repeat the SQL from step 8 several times and check if the record count is increasing (each sensor would send the data after every 10 seconds). Use exit command at MariaDB prompt to exit the DB shell. Use exit command again to exit 'iot-backend-mariadb' container shell. Task 3: Connect to the REST API Agent container and find the port it is listing on for incoming REST calls. Find the pod name for 'REST API Agent' using the kubectl command kubectl get pods and locate the pod name starting with 'iot-backend-rest-api-agent-' (you may see more than one pod for this service, pick any one). login to the REST API Agent container using the kubectl command kubectl exec -it <pod name> -- /bin/ash Tip (replace the <pod name> in command above with correct value). Execute netstat -an command on the container shell and check the output. This container listens on port '5050' for incoming REST connections and connects with MariaDB using port '3306' (Connection to DB will timeout in case there are no requests coming in). Use exit command to come out of the container shell. Task 4: Check the logs messages from 'REST API Agent'. Find the pod name for 'REST API Agent' using the kubectl command kubectl get pods and locate the pod name starting with 'iot-backend-rest-api-agent-' (you may see more than one pod for this service, pick any one). Check the logs using the kubectl command kubectl logs <pod name> Tip (replace the <pod name> in command above with correct value). Task 5: How the traffic would be distributed, if you have multiple Kubernetes Pods behind a Kubernetes NodePort Service? Right click on top of your putty window and click on \"Duplicate Session\" (You should have two putty windows side by side logged into same linux jumphost). In the first putty window (logged into linux jumphost) execute the command kubectl get nodes -o wide and note down the Kubernetes master node's external IP address. Open a web browser on your machine and access the following url (NAT the master node ip before in 1:1 example: 10.200.0.158 -> 198.18.133.158 and then use in the url) - http://<kubernetes node's external ip NAT>:<port>/temperature In the first putty window, run kubectl get pods on the linux jumphost. In the output you should see 2 pods for 'iot-backend-rest-api-agent'. In the first putty window, run the command watch kubectl logs <first pod name> (Use the first 'iot-backend-rest-api-agent' pod out of 2 pods that you saw in the output of the command executed in step#5). In the Second putty window, display the logs for second 'iot-backend-rest-api-agent' pod using the command watch kubectl logs <second pod name> Refresh or reload the web page with the url you used in step 4. Repeat step 8 and check the requests hitting the kubernetes pods in the log messages.","title":"Explore Backend Application"},{"location":"backend_exercise/#explore-backend-app-and-kubernetes-dashboard","text":"Your Hybrid Cloud App's backend compoents (MariaDB, MQTT DB Agent, and REST API agent) should be up and running. Now let's explore some low level details to understand the application containers and kubernetes better. Task 0: Tasks in this section will be executed in Kubernetes on-premise. Change kubectl context to the correct one. Execute command below on linux jumphost: kubectl config use-context admin@on-prem-backend Task 1: Find the size of the 'Persistent Volume Claim' used for MariaDB database? To check 'CAPACITY' value of PVC execute this command on linux jumphost: kubectl get pvc mariadb-pv-claim Task 2: Login to the MariaDB database and explore the data tables. Find the pod name for 'MariaDB' using the kubectl command kubectl get pods and locate the pod name starting with 'iot-backend-mariadb-'. Login to the MariaDB container using the kubectl command kubectl exec -it <pod name> -- /bin/bash Tip (replace the <pod name> in command above with correct value). Inside the container execute ss -tulpn and check the port MariaDB is listing to for incoming connections. You can also check the version of MariaDB using the shell command: mysql --version Now connect to the MariaDB from the container shell and check Databases and Tables. Use mysql -u root -pcisco123 command to login to MaraDB. On mariaDB shell, use show databases; command to list all the databases. Look for ' sensor_db ' in the output (this is the database where we are storing the sensor data). Switch to sensor_db using the command use sensor_db; and list all the tables in this database using the command show tables; You should see only one table with the name 'sensor_data' ; Try to list the data from this table using the SQL statement select * from sensor_data; Now check the record count in this table using the sql statement: select count(*) from sensor_data; Repeat the SQL from step 8 several times and check if the record count is increasing (each sensor would send the data after every 10 seconds). Use exit command at MariaDB prompt to exit the DB shell. Use exit command again to exit 'iot-backend-mariadb' container shell. Task 3: Connect to the REST API Agent container and find the port it is listing on for incoming REST calls. Find the pod name for 'REST API Agent' using the kubectl command kubectl get pods and locate the pod name starting with 'iot-backend-rest-api-agent-' (you may see more than one pod for this service, pick any one). login to the REST API Agent container using the kubectl command kubectl exec -it <pod name> -- /bin/ash Tip (replace the <pod name> in command above with correct value). Execute netstat -an command on the container shell and check the output. This container listens on port '5050' for incoming REST connections and connects with MariaDB using port '3306' (Connection to DB will timeout in case there are no requests coming in). Use exit command to come out of the container shell. Task 4: Check the logs messages from 'REST API Agent'. Find the pod name for 'REST API Agent' using the kubectl command kubectl get pods and locate the pod name starting with 'iot-backend-rest-api-agent-' (you may see more than one pod for this service, pick any one). Check the logs using the kubectl command kubectl logs <pod name> Tip (replace the <pod name> in command above with correct value). Task 5: How the traffic would be distributed, if you have multiple Kubernetes Pods behind a Kubernetes NodePort Service? Right click on top of your putty window and click on \"Duplicate Session\" (You should have two putty windows side by side logged into same linux jumphost). In the first putty window (logged into linux jumphost) execute the command kubectl get nodes -o wide and note down the Kubernetes master node's external IP address. Open a web browser on your machine and access the following url (NAT the master node ip before in 1:1 example: 10.200.0.158 -> 198.18.133.158 and then use in the url) - http://<kubernetes node's external ip NAT>:<port>/temperature In the first putty window, run kubectl get pods on the linux jumphost. In the output you should see 2 pods for 'iot-backend-rest-api-agent'. In the first putty window, run the command watch kubectl logs <first pod name> (Use the first 'iot-backend-rest-api-agent' pod out of 2 pods that you saw in the output of the command executed in step#5). In the Second putty window, display the logs for second 'iot-backend-rest-api-agent' pod using the command watch kubectl logs <second pod name> Refresh or reload the web page with the url you used in step 4. Repeat step 8 and check the requests hitting the kubernetes pods in the log messages.","title":"Explore Backend App and Kubernetes Dashboard:"},{"location":"basic_kubectl_cmds/","text":"kubectl version kubectl get nodes kubectl run --image= --port= Kubernetes Deployments: A Deployment controller provides declarative updates for Pods and ReplicaSets. You describe a desired state in a Deployment object, and the Deployment controller changes the actual state to the desired state at a controlled rate. You can define Deployments to create new ReplicaSets, or to remove existing Deployments and adopt all their resources with new Deployments. Create Deployment Use the following command to create a deployment using a yaml file - kubectl create -f <yaml file path> List Deployments: Use the following command to list all kubernetes deployments - kubectl get deployments Deployment Details: Use the following command to display the details of deployment - kubectl describe deployment Scale Deployment: Use the following command to scale (up/down) a kubernetes deployment - kubectl scale deployments/<deployment_name> --replicas=<number of replicas> Delete Deployment: Use the following command to delete a deployment - kubectl delete deployment <deployment_name> Kubernetes Pods: A Pod represents a unit of deployment: a single instance of an application in Kubernetes, which might consist of either a single container or a small number of containers that are tightly coupled and that share resources. List Pods: Use the following command to list all the pods: kubectl get pods or use wide option to see more details - kubectl get pods -o wide List Pods Filter: You can filter the pods using the labels used in deployment - kubectl get pods -l <label_name>=<label_value> Pod Details: Use the following command to see the containers and used images for pods - kubectl describe pods Pod Logs: Use the following command to check the pod logs - kubectl logs <pod_name> Kubernetes Services: A Kubernetes Service is an abstraction which defines a logical set of Pods and a policy by which to access them - sometimes called a micro-service. List Service: Use the following command to list the current Services - kubectl get services You can filter the services using the labels used in deployment - kubectl get services -l <label_name>=<label_value> Create Service: Use the following command to create a new service - kubectl expose deployment/<deployment_name> --type=\"NodePort\" --port <port> Service Details: Use the following command to find out what port was opened externally (by the NodePort option) - kubectl describe services/<service_name> Delete Service: Use the following command to delete a service - kubectl delete service/<service_name> or kubectl delete service -l <label_name>=<label_value> Parse Service Node Port: Use the following script to filterout the node-port of a service (change the service name) - export NODE_PORT=$(kubectl get services/<service-name> -o go-template='{{(index .spec.ports 0).nodePort}}') echo NODE_PORT=$NODE_PORT Kubernetes Secrets: A Secret is an object that stores a piece of sensitive data like a password or key. List Secrets: Use the following command to list all secrets kubectl get secrets Secret Details: Use the following command to list the secret details - kubectl describe secrets/<secret_name> Create Secret: Use the following command to create secret - kubectl create secret generic <secret_name> --from-literal=<key_name>=<key_value> Delete Secret: use the following command to delete a secret - kubectl delete secret <secret_name> Interacting with Pod Containers List Env Variables: Use the following command to list the environment variables - kubectl exec <pod_name> env Access Container Shell: Use the following command to access bash shell in a container - kubectl exec -ti <pod_name> bash Note: To close your container connection type ' exit '. ========================================= kubectl proxy curl http://localhost:8001/version export POD_NAME=$(kubectl get pods -o go-template --template '{{range .items}}{{.metadata.name}}{{\"\\n\"}}{{end}}') echo Name of the Pod: $POD_NAME curl http://localhost:8001/api/v1/namespaces/default/pods/$POD_NAME/proxy =========================================","title":"Basic kubectl cmds"},{"location":"basic_kubectl_cmds/#kubernetes-deployments","text":"A Deployment controller provides declarative updates for Pods and ReplicaSets. You describe a desired state in a Deployment object, and the Deployment controller changes the actual state to the desired state at a controlled rate. You can define Deployments to create new ReplicaSets, or to remove existing Deployments and adopt all their resources with new Deployments. Create Deployment Use the following command to create a deployment using a yaml file - kubectl create -f <yaml file path> List Deployments: Use the following command to list all kubernetes deployments - kubectl get deployments Deployment Details: Use the following command to display the details of deployment - kubectl describe deployment Scale Deployment: Use the following command to scale (up/down) a kubernetes deployment - kubectl scale deployments/<deployment_name> --replicas=<number of replicas> Delete Deployment: Use the following command to delete a deployment - kubectl delete deployment <deployment_name>","title":"Kubernetes Deployments:"},{"location":"basic_kubectl_cmds/#kubernetes-pods","text":"A Pod represents a unit of deployment: a single instance of an application in Kubernetes, which might consist of either a single container or a small number of containers that are tightly coupled and that share resources. List Pods: Use the following command to list all the pods: kubectl get pods or use wide option to see more details - kubectl get pods -o wide List Pods Filter: You can filter the pods using the labels used in deployment - kubectl get pods -l <label_name>=<label_value> Pod Details: Use the following command to see the containers and used images for pods - kubectl describe pods Pod Logs: Use the following command to check the pod logs - kubectl logs <pod_name>","title":"Kubernetes Pods:"},{"location":"basic_kubectl_cmds/#kubernetes-services","text":"A Kubernetes Service is an abstraction which defines a logical set of Pods and a policy by which to access them - sometimes called a micro-service. List Service: Use the following command to list the current Services - kubectl get services You can filter the services using the labels used in deployment - kubectl get services -l <label_name>=<label_value> Create Service: Use the following command to create a new service - kubectl expose deployment/<deployment_name> --type=\"NodePort\" --port <port> Service Details: Use the following command to find out what port was opened externally (by the NodePort option) - kubectl describe services/<service_name> Delete Service: Use the following command to delete a service - kubectl delete service/<service_name> or kubectl delete service -l <label_name>=<label_value> Parse Service Node Port: Use the following script to filterout the node-port of a service (change the service name) - export NODE_PORT=$(kubectl get services/<service-name> -o go-template='{{(index .spec.ports 0).nodePort}}') echo NODE_PORT=$NODE_PORT","title":"Kubernetes Services:"},{"location":"basic_kubectl_cmds/#kubernetes-secrets","text":"A Secret is an object that stores a piece of sensitive data like a password or key. List Secrets: Use the following command to list all secrets kubectl get secrets Secret Details: Use the following command to list the secret details - kubectl describe secrets/<secret_name> Create Secret: Use the following command to create secret - kubectl create secret generic <secret_name> --from-literal=<key_name>=<key_value> Delete Secret: use the following command to delete a secret - kubectl delete secret <secret_name>","title":"Kubernetes Secrets:"},{"location":"basic_kubectl_cmds/#interacting-with-pod-containers","text":"List Env Variables: Use the following command to list the environment variables - kubectl exec <pod_name> env Access Container Shell: Use the following command to access bash shell in a container - kubectl exec -ti <pod_name> bash Note: To close your container connection type ' exit '.","title":"Interacting with Pod Containers"},{"location":"basic_kubectl_cmds/#_1","text":"kubectl proxy curl http://localhost:8001/version export POD_NAME=$(kubectl get pods -o go-template --template '{{range .items}}{{.metadata.name}}{{\"\\n\"}}{{end}}') echo Name of the Pod: $POD_NAME curl http://localhost:8001/api/v1/namespaces/default/pods/$POD_NAME/proxy =========================================","title":"========================================="},{"location":"deploy_backend/","text":"Deploy the Backend Application Components on IKS Kubernetes Cluster (IKS Tenant Cluster) In this section you would deploy the backend components of the IoT Application on the Kubernetes cluster deployed on-prem using Intesight. Following diagram shows the high-level architecture of these backend application containers Login to Kubernetes Master CLI Shell: SSH into Linux Jumphost using Putty, use predefined session named \"ubuntu-terminal\" (198.18.133.11) - use password C1sco12345. From here you will deploy two microservices in on-premise Kubernetes Cluster and one microservice in AWS EKS Kubernetes cluster. You will see how microservices talks to each other and how to establish necessary communication. 1. Deploy MariaDB Databse: MariaDB will be used in the backend to save the sensor data received from AWS IoT platform over MQTT protocol. For this we would create following objects - Secret Persistent Volume Claim (PVC) MariaDB Deployment ClusterIP Service (Headless Service) Following diagram shows the relationship between these objects - 1.1 Create Kubernetes Secret for MariaDB: A Secret is an object that contains a small amount of sensitive data such as a password, a token, or a key. Such information might otherwise be put in a Pod specification or in an image; putting it in a Secret object allows for more control over how it is used, and reduces the risk of accidental exposure. The MariaDB container image uses an environment variable named as 'MYSQL_ROOT_PASSWORD', it hold the root password required to access the database. So you would create a new secret with 'password' key (value as 'cisco123') which would later be used in mariaDB deployment yaml file. 1.1.1: Switch context to on-premise Kubernetes Cluster - Change context of kubectl command to access on-premise Kubernetes Cluster. kubectl config use-context admin@CLUS-IKS-1 kubectl config get-contexts 1.1.2: Create DB Password Secret - Use the following command to create a new secret on your kubernetes cluster - kubectl create secret generic mariadb-root-pass --from-literal=password=cisco123 1.1.3: Verify DB Password Secret - Check if the secret was created successfully or not - kubectl get secret mariadb-root-pass You should have the output similar to the following screenshot - 1.2 Create Kubernetes Persistent Volume Claim for MariaDB: A Persistent Volume Claim (PVC) is a request for storage by a user. It is similar to a pod. Pods consume node resources and PVCs consume Persistent Volume (PV) resources. Pods can request specific levels of resources (CPU and Memory). Claims can request specific size and access modes (e.g., can be mounted once read/write or many times read-only). To keep the sensor data safe during Pod restarts, you would create a new Persistent Volume Claim. The following yaml definition would be used to create the 'PersistentVolumeClaim' - --- apiVersion : v1 kind : PersistentVolumeClaim metadata : name : mariadb-pv-claim labels : app : iot-backend spec : accessModes : - ReadWriteOnce resources : requests : storage : 2Gi * 1.2.1: Create Persistent Volume Claim - Use the following command to create a new Persistent Volume Claim for MariaDB Pod - kubectl create -f https://raw.githubusercontent.com/marcinduma/WILCLD-2611/main/Kubernetes/Backend/Mariadb/mariadb_persistent_volume.yaml 1.2.2: Verify Persistent Volume Claim - Check if the PVC was created successfully or not - kubectl get pvc mariadb-pv-claim You should have the output similar to the following screenshot - Caution It can take up to a few minutes for the PVs to be provisioned. DO NOT procced futher till the PVC deployment gets completed. 1.3 Deploy MariaDB on Kubernetes: MariaDB is a community-developed fork of the MySQL relational database management system intended to remain free under the GNU GPL. Development is led by some of the original developers of MySQL, who forked it due to concerns over its acquisition by Oracle Corporation. MariaDB intends to maintain high compatibility with MySQL, ensuring a drop-in replacement capability with library binary parity and exact matching with MySQL APIs and commands. The following yaml definition will be used to deploy MariaDB pod - --- apiVersion : apps/v1 kind : Deployment metadata : name : iot-backend-mariadb labels : app : iot-backend spec : selector : matchLabels : app : iot-backend tier : mariadb strategy : type : Recreate template : metadata : labels : app : iot-backend tier : mariadb spec : containers : - image : mariadb:10.3 name : mariadb env : - name : MYSQL_ROOT_PASSWORD valueFrom : secretKeyRef : name : mariadb-root-pass key : password ports : - containerPort : 3306 name : mariadb volumeMounts : - name : mariadb-persistent-storage mountPath : /var/lib/mysql volumes : - name : mariadb-persistent-storage persistentVolumeClaim : claimName : mariadb-pv-claim * 1.3.1: Deploy MariaDB - Use the following command to create a MariaDB kubernetes deployment kubectl create -f https://raw.githubusercontent.com/marcinduma/WILCLD-2611/main/Kubernetes/Backend/Mariadb/mariadb_deployment.yaml 1.3.2: Check Deployment Status - Use the following command to check if the kubernetes deployment was successfully created or not kubectl get deployment iot-backend-mariadb You should have the output similar to the following screenshot 1.3.3: Check Pod Status - Use the following command to check if the 'iot-backend-mariadb' pod is in ' Running ' state kubectl get pods Caution Kubernetes may take some time to deploy the MariaDB. DO NOT proceed further till the time DB Pod is up. 1.4 Create Kubernetes LoadBalancer Service for MariaDB: Since the MariaDB will be accessed by other services like 'MQTT to DB Agent' and 'REST API Agent'; you need to expose it externally, since 'MQTT to DB Agent' will be running on another Kubernetes Cluster A LoadBalancer Service provides external access to your application from systems outside of Kubernetes. LoadBalancer service is exposed under dedicated VIP address, routable in external network. Traffic directed to this IP address is load balanced by Kubernetes across Kubernetes nodes. Following yaml definition would be used to create the LoadBalancer Service for MariaDB --- apiVersion : v1 kind : Service metadata : name : mariadb-service labels : app : iot-backend spec : ports : - protocol : TCP port : 3306 selector : app : iot-backend tier : mariadb type : \"LoadBalancer\" 1.4.1: Expose MariaDB to other Pods - Create a new kubernetes service using the following command kubectl create -f https://raw.githubusercontent.com/marcinduma/WILCLD-2611/main/Kubernetes/Backend/Mariadb/mariadb_service.yaml 1.4.2: Verify Service Status - Use the following command to check if the kubernetes service was deployed successfully or not kubectl get service mariadb-service You should have the output similar to the following screenshot 2. Deploy REST API Agent on Kubernetes: The 'REST API Agent' would act as the gateway to the backend application. It will listen to the incoming HTTP requests from the frontend application that you will deploy on AWS. 2.1 Deploy REST API Agent: The following yaml definition will be used to create REST API Agent pods --- apiVersion : apps/v1 kind : Deployment metadata : name : iot-backend-rest-api-agent labels : app : iot-backend-rest-api-agent spec : replicas : 1 selector : matchLabels : app : iot-backend-rest-api-agent tier : rest-api-agent strategy : type : Recreate template : metadata : labels : app : iot-backend-rest-api-agent tier : rest-api-agent spec : containers : - image : pradeesi/rest_api_agent:v1 name : rest-api-agent env : - name : DB_PASSWORD valueFrom : secretKeyRef : name : mariadb-root-pass key : password 2.1.1: Deploy REST API Agent - Use the following command to create the rest-api-agent kubernetes deployment kubectl create -f https://raw.githubusercontent.com/marcinduma/WILCLD-2611/main/Kubernetes/Backend/REST_API_Agent/rest_api_agent.yaml 2.1.2: Check Deployment Status - Use the following command to check if the kubernetes deployment was created successfully or not kubectl get deployment iot-backend-rest-api-agent You should have the output similar to the following screenshot 2.1.3: Check Pod Status - Use the following command to check if the 'iot-backend-rest-api-agent' pod is in ' Running ' state kubectl get pods Tip You may check the Pod Logs using the command ' kubectl logs <pod_name> ' 2.2 Create Kubernetes NodePort Service for REST API Agent: Since the frontend app from AWS would access the REST APIs exposed by the 'REST API Agent', you need to create a new kubernetes service for it. The following yaml definition would be used for to create a NodePort Service for the REST API Agent - --- apiVersion : v1 kind : Service metadata : name : rest-api-agent-service labels : app : iot-backend spec : ports : - protocol : TCP port : 5050 selector : app : iot-backend-rest-api-agent tier : rest-api-agent type : \"NodePort\" 2.2.1: Create REST API Agent NodePort Service - You can create a new kubernetes service using the following command kubectl create -f https://raw.githubusercontent.com/marcinduma/WILCLD-2611/main/Kubernetes/Backend/REST_API_Agent/rest_api_agent_service_node_port.yaml 2.2.2: Check REST API Agent Service Status - You can use the following command to check if the kubernetes service was created successfully or not kubectl get service rest-api-agent-service You should have the output similar to the following screenshot 2.3 Locate the IP and Port to Access Node-Port Service for REST API Agent: You need to find the NodePort and Kubernetes Node external IP to access the 'rest-api-agent. Use the following command to display the port exposed by 'rest-api-agent-service' kubectl get service rest-api-agent-service Use the following command to display the 'External-IP' of you kubernetes nodes kubectl get nodes -o wide Following screenshot highlights the Port and Node IPs in the command outputs Important Note down the Node External IP Address and NodePort Service Port Number. These values will be used in next section for deploying the frontend app as the environment variables values (' BACKEND_HOST ' and ' BACKEND_PORT '). 3. Deploy MQTT to DB Agent on Kubernetes: 'MQTT to DB Agent' will subscribe to the MQTT Topic and listen to the incoming sensor data from AWS IoT platform. It will then parse the sensor data and insert it into the MariaDB. The following yaml definition will be used to create the MQTT to DB Agent pods --- apiVersion : apps/v1 kind : Deployment metadata : name : iot-backend-mqtt-db-agent labels : app : iot-backend tier : mqtt-db-agent spec : selector : matchLabels : app : iot-backend-mqtt-db-agent strategy : type : Recreate template : metadata : labels : app : iot-backend-mqtt-db-agent spec : containers : - image : eu.gcr.io/fwardz001-poc-ci1s/mqtt_db_plugin:v9 name : mqtt-db-agent env : - name : DB_PASSWORD valueFrom : secretKeyRef : name : mariadb-root-pass key : password * 3.1: Switch context to AWS EKS Kubernetes Cluster - Change context of kubectl command to access AWS Kubernetes Cluster. kubectl config get-contexts kubectl config use-context <AWS context-name from previous command output> kubectl config get-contexts 3.2: Create DB Password Secret - Use the following command to create a new secret on your kubernetes cluster kubectl create secret generic mariadb-root-pass --from-literal=password=cisco123 3.3: Verify DB Password Secret - Check if the secret was created successfully or not kubectl get secret mariadb-root-pass You should have the output similar to the following screenshot 3.4: Create external service MQTT needs to send data to database that is deployed in IKS on-prem Kubernetes Cluster. MQTT application is configured to contact with MariaDB using following internal DNS name: mariadb-service . We need to configure Kubernetes to resolve this name to a particular LoadBalancer IP that has been allocated to your mariadb-service in IKS on-premise Kubernetes Cluster. For this we will define service and manually add endpoint IP that this service will resolve to. In the Endpoint definition you have to specify your LoadBalancer IP address from on-premise Kubernetes Cluster allocated to mariadb-service . --- apiVersion : v1 kind : Service metadata : name : mariadb-service spec : ports : - name : sql protocol : TCP port : 3306 targetPort : 3306 --- apiVersion : v1 kind : Endpoints metadata : name : mariadb-service subsets : - addresses : - ip : LoadBalancerIP ## Specify mariadb-service LoadBalancer IP from step 1.4 ports : - port : 3306 name : sql Download following definition file: wget https://raw.githubusercontent.com/marcinduma/WILCLD-2611/main/Kubernetes/Backend/MQTT_DB_Agent/mariadb-ext-service-eks.yaml Check string to be replaced by LoadBalancerIP allocated to mariadb-service from Step 1.4.2 cat mariadb-ext-service-eks.yaml Change <mariadb-service_LoadBalancer_IP> with IP address of your load balancer IP, replace 198.18.134.XXX in sed command below with the IP address of LoadBalancer IP allocated to mariadb-service in on-premise Kubernetes Cluster. sed -i 's/<mariadb-service_LoadBalancer_IP>/198.18.134.XXX/g' mariadb-ext-service-eks.yaml Check the manifest file after change of IP: cat mariadb-ext-service-eks.yaml Important Make sure that the IP address of mariadb-service you specified is correct. Apply updated manifest to create external service access: kubectl apply -f mariadb-ext-service-eks.yaml Check services and associated endpoints: kubectl get svc,endpoints 3.5: Deploy MQTT to DB Agent - Use the following command to create mqtt-to-db-agent kubernetes deployment kubectl create -f https://raw.githubusercontent.com/marcinduma/WILCLD-2611/main/Kubernetes/Backend/MQTT_DB_Agent/mqtt_db_agent_deployment.yaml 3.6: Check Deployment Status - Use the following command to check if the kubernetes deployment was created successfully or not kubectl get deployment iot-backend-mqtt-db-agent You should have the output similar to the following screenshot 3.5: Check Pod Status - Use the following command to check if the 'iot-backend-mqtt-db-agent' pod is in ' Running ' state kubectl get pods 4 Test the REST APIs Exposed by REST API Agent Service: To test the REST API service try to access following url from your web browser (use the node's external ip and service port from the previous section # 2.3) - If you haven't note the IP and port information earlier, please follow those steps: Change kubectl context to CLUS-IKS-1 kubectl config use-context admin@CLUS-IKS-1 kubectl config get-contexts Use the following command to display the port exposed by 'rest-api-agent-service' kubectl get service rest-api-agent-service Use the following command to display the 'External-IP' of you kubernetes nodes kubectl get nodes -o wide Following screenshot highlights the Port and Node IPs in the command outputs Now you have to open Chrome browser and specify URL based on pattern - http://<kubernetes node's external ip>:<nodePort> i.e. http://198.18.134.103:30276 If your REST API Agent is working properly, you should see 'Welcome to the API Service...!' message on your browser as shown in the following screenshot - Following are the other urls that you could test - http://<kubernetes node's external ip>:<nodePort>/cities http://<kubernetes node's external ip>:<nodePort>/temperature http://<kubernetes node's external ip>:<nodePort>/humidity http://<kubernetes node's external ip>:<nodePort>/sensor_data/city","title":"Deploy Backend App on Tenant Clusters"},{"location":"deploy_backend/#deploy-the-backend-application-components-on-iks-kubernetes-cluster-iks-tenant-cluster","text":"In this section you would deploy the backend components of the IoT Application on the Kubernetes cluster deployed on-prem using Intesight. Following diagram shows the high-level architecture of these backend application containers","title":"Deploy the Backend Application Components on IKS Kubernetes Cluster (IKS Tenant Cluster)"},{"location":"deploy_backend/#login-to-kubernetes-master-cli-shell","text":"SSH into Linux Jumphost using Putty, use predefined session named \"ubuntu-terminal\" (198.18.133.11) - use password C1sco12345. From here you will deploy two microservices in on-premise Kubernetes Cluster and one microservice in AWS EKS Kubernetes cluster. You will see how microservices talks to each other and how to establish necessary communication.","title":"Login to Kubernetes Master CLI Shell:"},{"location":"deploy_backend/#1-deploy-mariadb-databse","text":"MariaDB will be used in the backend to save the sensor data received from AWS IoT platform over MQTT protocol. For this we would create following objects - Secret Persistent Volume Claim (PVC) MariaDB Deployment ClusterIP Service (Headless Service) Following diagram shows the relationship between these objects -","title":"1. Deploy MariaDB Databse:"},{"location":"deploy_backend/#11-create-kubernetes-secret-for-mariadb","text":"A Secret is an object that contains a small amount of sensitive data such as a password, a token, or a key. Such information might otherwise be put in a Pod specification or in an image; putting it in a Secret object allows for more control over how it is used, and reduces the risk of accidental exposure. The MariaDB container image uses an environment variable named as 'MYSQL_ROOT_PASSWORD', it hold the root password required to access the database. So you would create a new secret with 'password' key (value as 'cisco123') which would later be used in mariaDB deployment yaml file. 1.1.1: Switch context to on-premise Kubernetes Cluster - Change context of kubectl command to access on-premise Kubernetes Cluster. kubectl config use-context admin@CLUS-IKS-1 kubectl config get-contexts 1.1.2: Create DB Password Secret - Use the following command to create a new secret on your kubernetes cluster - kubectl create secret generic mariadb-root-pass --from-literal=password=cisco123 1.1.3: Verify DB Password Secret - Check if the secret was created successfully or not - kubectl get secret mariadb-root-pass You should have the output similar to the following screenshot -","title":"1.1 Create Kubernetes Secret for MariaDB:"},{"location":"deploy_backend/#12-create-kubernetes-persistent-volume-claim-for-mariadb","text":"A Persistent Volume Claim (PVC) is a request for storage by a user. It is similar to a pod. Pods consume node resources and PVCs consume Persistent Volume (PV) resources. Pods can request specific levels of resources (CPU and Memory). Claims can request specific size and access modes (e.g., can be mounted once read/write or many times read-only). To keep the sensor data safe during Pod restarts, you would create a new Persistent Volume Claim. The following yaml definition would be used to create the 'PersistentVolumeClaim' - --- apiVersion : v1 kind : PersistentVolumeClaim metadata : name : mariadb-pv-claim labels : app : iot-backend spec : accessModes : - ReadWriteOnce resources : requests : storage : 2Gi * 1.2.1: Create Persistent Volume Claim - Use the following command to create a new Persistent Volume Claim for MariaDB Pod - kubectl create -f https://raw.githubusercontent.com/marcinduma/WILCLD-2611/main/Kubernetes/Backend/Mariadb/mariadb_persistent_volume.yaml 1.2.2: Verify Persistent Volume Claim - Check if the PVC was created successfully or not - kubectl get pvc mariadb-pv-claim You should have the output similar to the following screenshot - Caution It can take up to a few minutes for the PVs to be provisioned. DO NOT procced futher till the PVC deployment gets completed.","title":"1.2 Create Kubernetes Persistent Volume Claim for MariaDB:"},{"location":"deploy_backend/#13-deploy-mariadb-on-kubernetes","text":"MariaDB is a community-developed fork of the MySQL relational database management system intended to remain free under the GNU GPL. Development is led by some of the original developers of MySQL, who forked it due to concerns over its acquisition by Oracle Corporation. MariaDB intends to maintain high compatibility with MySQL, ensuring a drop-in replacement capability with library binary parity and exact matching with MySQL APIs and commands. The following yaml definition will be used to deploy MariaDB pod - --- apiVersion : apps/v1 kind : Deployment metadata : name : iot-backend-mariadb labels : app : iot-backend spec : selector : matchLabels : app : iot-backend tier : mariadb strategy : type : Recreate template : metadata : labels : app : iot-backend tier : mariadb spec : containers : - image : mariadb:10.3 name : mariadb env : - name : MYSQL_ROOT_PASSWORD valueFrom : secretKeyRef : name : mariadb-root-pass key : password ports : - containerPort : 3306 name : mariadb volumeMounts : - name : mariadb-persistent-storage mountPath : /var/lib/mysql volumes : - name : mariadb-persistent-storage persistentVolumeClaim : claimName : mariadb-pv-claim * 1.3.1: Deploy MariaDB - Use the following command to create a MariaDB kubernetes deployment kubectl create -f https://raw.githubusercontent.com/marcinduma/WILCLD-2611/main/Kubernetes/Backend/Mariadb/mariadb_deployment.yaml 1.3.2: Check Deployment Status - Use the following command to check if the kubernetes deployment was successfully created or not kubectl get deployment iot-backend-mariadb You should have the output similar to the following screenshot 1.3.3: Check Pod Status - Use the following command to check if the 'iot-backend-mariadb' pod is in ' Running ' state kubectl get pods Caution Kubernetes may take some time to deploy the MariaDB. DO NOT proceed further till the time DB Pod is up.","title":"1.3 Deploy MariaDB on Kubernetes:"},{"location":"deploy_backend/#14-create-kubernetes-loadbalancer-service-for-mariadb","text":"Since the MariaDB will be accessed by other services like 'MQTT to DB Agent' and 'REST API Agent'; you need to expose it externally, since 'MQTT to DB Agent' will be running on another Kubernetes Cluster A LoadBalancer Service provides external access to your application from systems outside of Kubernetes. LoadBalancer service is exposed under dedicated VIP address, routable in external network. Traffic directed to this IP address is load balanced by Kubernetes across Kubernetes nodes. Following yaml definition would be used to create the LoadBalancer Service for MariaDB --- apiVersion : v1 kind : Service metadata : name : mariadb-service labels : app : iot-backend spec : ports : - protocol : TCP port : 3306 selector : app : iot-backend tier : mariadb type : \"LoadBalancer\" 1.4.1: Expose MariaDB to other Pods - Create a new kubernetes service using the following command kubectl create -f https://raw.githubusercontent.com/marcinduma/WILCLD-2611/main/Kubernetes/Backend/Mariadb/mariadb_service.yaml 1.4.2: Verify Service Status - Use the following command to check if the kubernetes service was deployed successfully or not kubectl get service mariadb-service You should have the output similar to the following screenshot","title":"1.4 Create Kubernetes LoadBalancer Service for MariaDB:"},{"location":"deploy_backend/#2-deploy-rest-api-agent-on-kubernetes","text":"The 'REST API Agent' would act as the gateway to the backend application. It will listen to the incoming HTTP requests from the frontend application that you will deploy on AWS.","title":"2. Deploy REST API Agent on Kubernetes:"},{"location":"deploy_backend/#21-deploy-rest-api-agent","text":"The following yaml definition will be used to create REST API Agent pods --- apiVersion : apps/v1 kind : Deployment metadata : name : iot-backend-rest-api-agent labels : app : iot-backend-rest-api-agent spec : replicas : 1 selector : matchLabels : app : iot-backend-rest-api-agent tier : rest-api-agent strategy : type : Recreate template : metadata : labels : app : iot-backend-rest-api-agent tier : rest-api-agent spec : containers : - image : pradeesi/rest_api_agent:v1 name : rest-api-agent env : - name : DB_PASSWORD valueFrom : secretKeyRef : name : mariadb-root-pass key : password 2.1.1: Deploy REST API Agent - Use the following command to create the rest-api-agent kubernetes deployment kubectl create -f https://raw.githubusercontent.com/marcinduma/WILCLD-2611/main/Kubernetes/Backend/REST_API_Agent/rest_api_agent.yaml 2.1.2: Check Deployment Status - Use the following command to check if the kubernetes deployment was created successfully or not kubectl get deployment iot-backend-rest-api-agent You should have the output similar to the following screenshot 2.1.3: Check Pod Status - Use the following command to check if the 'iot-backend-rest-api-agent' pod is in ' Running ' state kubectl get pods Tip You may check the Pod Logs using the command ' kubectl logs <pod_name> '","title":"2.1 Deploy REST API Agent:"},{"location":"deploy_backend/#22-create-kubernetes-nodeport-service-for-rest-api-agent","text":"Since the frontend app from AWS would access the REST APIs exposed by the 'REST API Agent', you need to create a new kubernetes service for it. The following yaml definition would be used for to create a NodePort Service for the REST API Agent - --- apiVersion : v1 kind : Service metadata : name : rest-api-agent-service labels : app : iot-backend spec : ports : - protocol : TCP port : 5050 selector : app : iot-backend-rest-api-agent tier : rest-api-agent type : \"NodePort\" 2.2.1: Create REST API Agent NodePort Service - You can create a new kubernetes service using the following command kubectl create -f https://raw.githubusercontent.com/marcinduma/WILCLD-2611/main/Kubernetes/Backend/REST_API_Agent/rest_api_agent_service_node_port.yaml 2.2.2: Check REST API Agent Service Status - You can use the following command to check if the kubernetes service was created successfully or not kubectl get service rest-api-agent-service You should have the output similar to the following screenshot","title":"2.2 Create Kubernetes NodePort Service for REST API Agent:"},{"location":"deploy_backend/#23-locate-the-ip-and-port-to-access-node-port-service-for-rest-api-agent","text":"You need to find the NodePort and Kubernetes Node external IP to access the 'rest-api-agent. Use the following command to display the port exposed by 'rest-api-agent-service' kubectl get service rest-api-agent-service Use the following command to display the 'External-IP' of you kubernetes nodes kubectl get nodes -o wide Following screenshot highlights the Port and Node IPs in the command outputs Important Note down the Node External IP Address and NodePort Service Port Number. These values will be used in next section for deploying the frontend app as the environment variables values (' BACKEND_HOST ' and ' BACKEND_PORT ').","title":"2.3 Locate the IP and Port to Access Node-Port Service for REST API Agent:"},{"location":"deploy_backend/#3-deploy-mqtt-to-db-agent-on-kubernetes","text":"'MQTT to DB Agent' will subscribe to the MQTT Topic and listen to the incoming sensor data from AWS IoT platform. It will then parse the sensor data and insert it into the MariaDB. The following yaml definition will be used to create the MQTT to DB Agent pods --- apiVersion : apps/v1 kind : Deployment metadata : name : iot-backend-mqtt-db-agent labels : app : iot-backend tier : mqtt-db-agent spec : selector : matchLabels : app : iot-backend-mqtt-db-agent strategy : type : Recreate template : metadata : labels : app : iot-backend-mqtt-db-agent spec : containers : - image : eu.gcr.io/fwardz001-poc-ci1s/mqtt_db_plugin:v9 name : mqtt-db-agent env : - name : DB_PASSWORD valueFrom : secretKeyRef : name : mariadb-root-pass key : password * 3.1: Switch context to AWS EKS Kubernetes Cluster - Change context of kubectl command to access AWS Kubernetes Cluster. kubectl config get-contexts kubectl config use-context <AWS context-name from previous command output> kubectl config get-contexts 3.2: Create DB Password Secret - Use the following command to create a new secret on your kubernetes cluster kubectl create secret generic mariadb-root-pass --from-literal=password=cisco123 3.3: Verify DB Password Secret - Check if the secret was created successfully or not kubectl get secret mariadb-root-pass You should have the output similar to the following screenshot 3.4: Create external service MQTT needs to send data to database that is deployed in IKS on-prem Kubernetes Cluster. MQTT application is configured to contact with MariaDB using following internal DNS name: mariadb-service . We need to configure Kubernetes to resolve this name to a particular LoadBalancer IP that has been allocated to your mariadb-service in IKS on-premise Kubernetes Cluster. For this we will define service and manually add endpoint IP that this service will resolve to. In the Endpoint definition you have to specify your LoadBalancer IP address from on-premise Kubernetes Cluster allocated to mariadb-service . --- apiVersion : v1 kind : Service metadata : name : mariadb-service spec : ports : - name : sql protocol : TCP port : 3306 targetPort : 3306 --- apiVersion : v1 kind : Endpoints metadata : name : mariadb-service subsets : - addresses : - ip : LoadBalancerIP ## Specify mariadb-service LoadBalancer IP from step 1.4 ports : - port : 3306 name : sql Download following definition file: wget https://raw.githubusercontent.com/marcinduma/WILCLD-2611/main/Kubernetes/Backend/MQTT_DB_Agent/mariadb-ext-service-eks.yaml Check string to be replaced by LoadBalancerIP allocated to mariadb-service from Step 1.4.2 cat mariadb-ext-service-eks.yaml Change <mariadb-service_LoadBalancer_IP> with IP address of your load balancer IP, replace 198.18.134.XXX in sed command below with the IP address of LoadBalancer IP allocated to mariadb-service in on-premise Kubernetes Cluster. sed -i 's/<mariadb-service_LoadBalancer_IP>/198.18.134.XXX/g' mariadb-ext-service-eks.yaml Check the manifest file after change of IP: cat mariadb-ext-service-eks.yaml Important Make sure that the IP address of mariadb-service you specified is correct. Apply updated manifest to create external service access: kubectl apply -f mariadb-ext-service-eks.yaml Check services and associated endpoints: kubectl get svc,endpoints 3.5: Deploy MQTT to DB Agent - Use the following command to create mqtt-to-db-agent kubernetes deployment kubectl create -f https://raw.githubusercontent.com/marcinduma/WILCLD-2611/main/Kubernetes/Backend/MQTT_DB_Agent/mqtt_db_agent_deployment.yaml 3.6: Check Deployment Status - Use the following command to check if the kubernetes deployment was created successfully or not kubectl get deployment iot-backend-mqtt-db-agent You should have the output similar to the following screenshot 3.5: Check Pod Status - Use the following command to check if the 'iot-backend-mqtt-db-agent' pod is in ' Running ' state kubectl get pods","title":"3. Deploy MQTT to DB Agent on Kubernetes:"},{"location":"deploy_backend/#4-test-the-rest-apis-exposed-by-rest-api-agent-service","text":"To test the REST API service try to access following url from your web browser (use the node's external ip and service port from the previous section # 2.3) - If you haven't note the IP and port information earlier, please follow those steps: Change kubectl context to CLUS-IKS-1 kubectl config use-context admin@CLUS-IKS-1 kubectl config get-contexts Use the following command to display the port exposed by 'rest-api-agent-service' kubectl get service rest-api-agent-service Use the following command to display the 'External-IP' of you kubernetes nodes kubectl get nodes -o wide Following screenshot highlights the Port and Node IPs in the command outputs Now you have to open Chrome browser and specify URL based on pattern - http://<kubernetes node's external ip>:<nodePort> i.e. http://198.18.134.103:30276 If your REST API Agent is working properly, you should see 'Welcome to the API Service...!' message on your browser as shown in the following screenshot - Following are the other urls that you could test - http://<kubernetes node's external ip>:<nodePort>/cities http://<kubernetes node's external ip>:<nodePort>/temperature http://<kubernetes node's external ip>:<nodePort>/humidity http://<kubernetes node's external ip>:<nodePort>/sensor_data/city","title":"4 Test the REST APIs Exposed by REST API Agent Service:"},{"location":"deploy_frontend-aws/","text":"Deploy the Frontend Application Components on AWS In this section you would deploy the frontend components of the IoT Application on the Amazon Cloud. Following diagram shows the high-level architecture of these frontend application containers Login to Kubernetes Master CLI Shell: SSH into Linux Jumphost using Putty \"ubuntu-terminal\" (198.18.133.11) - use your credentials. Switch to context 'arn:aws:eks:us-east-1:782256189490:cluster/CLUS-EKS-X'. kubectl config get-contexts kubectl config use-context <AWS context-name from previous command output> kubectl config get-contexts 1. Deploy frontend-iot: Frontend is nginx web server connected to frontend-server which connect to REST-API Agent over already created Site-2-Site connection. In this section you will configure: Kubernetes ConfigMap Kubernetes frontend-iot Deployment Kubernetes frontend-iot Load-Balancer AWS Service You will create kubernetes deployment for frontend app and expose it to the internet using Kubernetes Load Balancer Service as shown in the following diagram 1.1 Create ConfigMap Create now configmap which will be used by your FrontEnd deployment. Replace ' BACKEND_HOST ' and ' BACKEND_PORT ' in the configmap command (below) with value noted down in section 2.3 in chapter Deploy REST API Agent on Kubernetes . Following screenshot highlights the Port and Node IPs in the command outputs in on-prem cluster. Warning External IP addresses of your on-prem workers can vary from one visible on figure below. Important Note down the Node External IP Address of one of on-prem WORKER and NodePort Service Port Number. ConfigMap create command: kubectl create configmap iot-frontend-config --from-literal=BACKEND_HOST=<NODE_IP> --from-literal=BACKEND_PORT=<PORT> 1.2 Create new deployment: iot-frontend Now is time to deploy our web frontend. Deployment contain two containers in one POD. The frontend server uses existing Site-2-Site connection to on-prem to access data which will be presented in GUI. Monifest file is represented below: --- apiVersion : \"extensions/v1beta1\" kind : \"Deployment\" metadata : name : \"iot-frontend\" namespace : \"default\" labels : app : \"iot-frontend\" spec : replicas : 3 selector : matchLabels : app : \"iot-frontend\" template : metadata : labels : app : \"iot-frontend\" spec : containers : - name : \"frontend-server\" image : \"eu.gcr.io/fwardz001-poc-ci1s/frontend_server:latest\" env : - name : \"BACKEND_HOST\" valueFrom : configMapKeyRef : key : \"BACKEND_HOST\" name : \"iot-frontend-config\" - name : \"BACKEND_PORT\" valueFrom : configMapKeyRef : key : \"BACKEND_PORT\" name : \"iot-frontend-config\" - name : \"nginx-srvr\" image : \"eu.gcr.io/fwardz001-poc-ci1s/nginx_srvr:latest\" --- Create deployment of frontend using command below: kubectl create -f https://raw.githubusercontent.com/marcinduma/WILCLD-2611/main/Kubernetes/Frontend/frontend_and_nginx_deployment.yaml Check deployment status and associated PODs: kubectl get pods,deployment | egrep \"NAME|iot-front\" 2. Expose the Application by Creating Kubernetes Service: Next step of our deployment is exposure of frontend web to the Internet. By using following manifest file, you will be able to create Service type LoadBalancer in AWS Kubernetes cluster. --- apiVersion : v1 kind : Service metadata : name : frontend-iot-service labels : app : iot-frontend spec : ports : - protocol : TCP port : 80 targetPort : 80 selector : app : iot-frontend type : \"LoadBalancer\" using the following command - kubectl create -f https://raw.githubusercontent.com/marcinduma/WILCLD-2611/main/Kubernetes/Frontend/frontend-iot-service.yml you get service created and exposed in TCP port 80. 2.1 Check status of newly created service Once service was created, run following command kubectl get svc Copy service name from your output as marked by green frame on the screenshot above. Tip Place the DNS name of service copied to your web browser. It may not work imediately. Please wait few minutes if frontend is not visible yet. If you use webRDP we recommend to copy URL from RDP to your main PC - use Guacamole interface - explained in Appendix: Guacamole. 3. Open the Application Dashboard: 3.1: Use \u201chttp\u201d to open the Dashboard for URL captured in step 2.1. You should see the following webpage in the new tab of your browser. Click on the \"Open Dashboard\" button to open the application dashboard as shown in the following screenshot 3.2: If you see the following web-page with charts filled with data, your application is working :-) Congratulations!!!","title":"Deploy Frontend App on Amazon Cloud"},{"location":"deploy_frontend-aws/#deploy-the-frontend-application-components-on-aws","text":"In this section you would deploy the frontend components of the IoT Application on the Amazon Cloud. Following diagram shows the high-level architecture of these frontend application containers","title":"Deploy the Frontend Application Components on AWS"},{"location":"deploy_frontend-aws/#login-to-kubernetes-master-cli-shell","text":"SSH into Linux Jumphost using Putty \"ubuntu-terminal\" (198.18.133.11) - use your credentials. Switch to context 'arn:aws:eks:us-east-1:782256189490:cluster/CLUS-EKS-X'. kubectl config get-contexts kubectl config use-context <AWS context-name from previous command output> kubectl config get-contexts","title":"Login to Kubernetes Master CLI Shell:"},{"location":"deploy_frontend-aws/#1-deploy-frontend-iot","text":"Frontend is nginx web server connected to frontend-server which connect to REST-API Agent over already created Site-2-Site connection. In this section you will configure: Kubernetes ConfigMap Kubernetes frontend-iot Deployment Kubernetes frontend-iot Load-Balancer AWS Service You will create kubernetes deployment for frontend app and expose it to the internet using Kubernetes Load Balancer Service as shown in the following diagram","title":"1. Deploy frontend-iot:"},{"location":"deploy_frontend-aws/#11-create-configmap","text":"Create now configmap which will be used by your FrontEnd deployment. Replace ' BACKEND_HOST ' and ' BACKEND_PORT ' in the configmap command (below) with value noted down in section 2.3 in chapter Deploy REST API Agent on Kubernetes . Following screenshot highlights the Port and Node IPs in the command outputs in on-prem cluster. Warning External IP addresses of your on-prem workers can vary from one visible on figure below. Important Note down the Node External IP Address of one of on-prem WORKER and NodePort Service Port Number. ConfigMap create command: kubectl create configmap iot-frontend-config --from-literal=BACKEND_HOST=<NODE_IP> --from-literal=BACKEND_PORT=<PORT>","title":"1.1 Create ConfigMap"},{"location":"deploy_frontend-aws/#12-create-new-deployment-iot-frontend","text":"Now is time to deploy our web frontend. Deployment contain two containers in one POD. The frontend server uses existing Site-2-Site connection to on-prem to access data which will be presented in GUI. Monifest file is represented below: --- apiVersion : \"extensions/v1beta1\" kind : \"Deployment\" metadata : name : \"iot-frontend\" namespace : \"default\" labels : app : \"iot-frontend\" spec : replicas : 3 selector : matchLabels : app : \"iot-frontend\" template : metadata : labels : app : \"iot-frontend\" spec : containers : - name : \"frontend-server\" image : \"eu.gcr.io/fwardz001-poc-ci1s/frontend_server:latest\" env : - name : \"BACKEND_HOST\" valueFrom : configMapKeyRef : key : \"BACKEND_HOST\" name : \"iot-frontend-config\" - name : \"BACKEND_PORT\" valueFrom : configMapKeyRef : key : \"BACKEND_PORT\" name : \"iot-frontend-config\" - name : \"nginx-srvr\" image : \"eu.gcr.io/fwardz001-poc-ci1s/nginx_srvr:latest\" --- Create deployment of frontend using command below: kubectl create -f https://raw.githubusercontent.com/marcinduma/WILCLD-2611/main/Kubernetes/Frontend/frontend_and_nginx_deployment.yaml Check deployment status and associated PODs: kubectl get pods,deployment | egrep \"NAME|iot-front\"","title":"1.2 Create new deployment: iot-frontend"},{"location":"deploy_frontend-aws/#2-expose-the-application-by-creating-kubernetes-service","text":"Next step of our deployment is exposure of frontend web to the Internet. By using following manifest file, you will be able to create Service type LoadBalancer in AWS Kubernetes cluster. --- apiVersion : v1 kind : Service metadata : name : frontend-iot-service labels : app : iot-frontend spec : ports : - protocol : TCP port : 80 targetPort : 80 selector : app : iot-frontend type : \"LoadBalancer\" using the following command - kubectl create -f https://raw.githubusercontent.com/marcinduma/WILCLD-2611/main/Kubernetes/Frontend/frontend-iot-service.yml you get service created and exposed in TCP port 80.","title":"2. Expose the Application by Creating Kubernetes Service:"},{"location":"deploy_frontend-aws/#21-check-status-of-newly-created-service","text":"Once service was created, run following command kubectl get svc Copy service name from your output as marked by green frame on the screenshot above. Tip Place the DNS name of service copied to your web browser. It may not work imediately. Please wait few minutes if frontend is not visible yet. If you use webRDP we recommend to copy URL from RDP to your main PC - use Guacamole interface - explained in Appendix: Guacamole.","title":"2.1 Check status of newly created service"},{"location":"deploy_frontend-aws/#3-open-the-application-dashboard","text":"3.1: Use \u201chttp\u201d to open the Dashboard for URL captured in step 2.1. You should see the following webpage in the new tab of your browser. Click on the \"Open Dashboard\" button to open the application dashboard as shown in the following screenshot 3.2: If you see the following web-page with charts filled with data, your application is working :-) Congratulations!!!","title":"3. Open the Application Dashboard:"},{"location":"guacamole/","text":"Copy and Paste in and out Guacamole Apache Guacamole serves as a proxy to provide clientless access to other servers. The copy paste function is achieved through browser. Guacamole Menu The guacamole menu is a sidebar which is hidden. On a Windows device with an external keyboard, the guacamole menu is displayed by pressing Ctrl+Alt+Shift On a Mac device with an external keyboard, the guacamole menu is displayed by pressing Ctrl+Command+Shift 3. On a device that doesn\u2019t have a keyboard, e.g. mobile or touchscreen device, the guacamole menu is displayed by swiping right from the left edge of the screen. To Hide the guacamole menu, press Ctrl+Alt+Shift/ Ctrl+Command+Shift or swipe left across the screen again. Guacamole Use The Clipboard text area functions as an interface between the remote clipboard and local clipboard. Text from the local clipboard can be pasted into the text area, and the text is sent to the remote clipboard. 2. After the text is copied to the remote clipboard, the Guacamole menu can be closed. The text from the clipboard can be pasted in the desired location. 3. Similarly, copying text from the remote desktop sends the text to the Clipboard text area on the Guacamole Menu, which can then be copied to the local clipboard.","title":"Guacamole"},{"location":"guacamole/#copy-and-paste-in-and-out-guacamole","text":"Apache Guacamole serves as a proxy to provide clientless access to other servers. The copy paste function is achieved through browser.","title":"Copy and Paste in and out Guacamole"},{"location":"guacamole/#guacamole-menu","text":"The guacamole menu is a sidebar which is hidden. On a Windows device with an external keyboard, the guacamole menu is displayed by pressing Ctrl+Alt+Shift On a Mac device with an external keyboard, the guacamole menu is displayed by pressing Ctrl+Command+Shift 3. On a device that doesn\u2019t have a keyboard, e.g. mobile or touchscreen device, the guacamole menu is displayed by swiping right from the left edge of the screen. To Hide the guacamole menu, press Ctrl+Alt+Shift/ Ctrl+Command+Shift or swipe left across the screen again.","title":"Guacamole Menu"},{"location":"guacamole/#guacamole-use","text":"The Clipboard text area functions as an interface between the remote clipboard and local clipboard. Text from the local clipboard can be pasted into the text area, and the text is sent to the remote clipboard. 2. After the text is copied to the remote clipboard, the Guacamole menu can be closed. The text from the clipboard can be pasted in the desired location. 3. Similarly, copying text from the remote desktop sends the text to the Clipboard text area on the Guacamole Menu, which can then be copied to the local clipboard.","title":"Guacamole Use"},{"location":"kubernetes_basics/","text":"Appendix - 3: Kubernetes Basic Docs 1. Install and Configure kubectl For information on \"kubectl\" installation and configuration use the following document - https://kubernetes.io/docs/tasks/tools/install-kubectl/ 2. Google Cloud Container Registry: Pushing and Pulling Container Images To push or pull the container images to/from Google Container Registry you can follow the following article - https://cloud.google.com/container-registry/docs/pushing-and-pulling Note: You need to login to your Google Cloud account before pushing the container image. You can do it using the following command - gcloud auth login If you have multiple projects, you can select a specific one using the following command - gcloud config set project <Project_ID> Kubernetes Cheat Sheet: https://kubernetes.io/docs/reference/kubectl/cheatsheet/","title":"Appendix - 3: Kubernetes Basic Docs"},{"location":"kubernetes_basics/#appendix-3-kubernetes-basic-docs","text":"","title":"Appendix - 3: Kubernetes Basic Docs"},{"location":"kubernetes_basics/#1-install-and-configure-kubectl","text":"For information on \"kubectl\" installation and configuration use the following document - https://kubernetes.io/docs/tasks/tools/install-kubectl/","title":"1. Install and Configure kubectl"},{"location":"kubernetes_basics/#2-google-cloud-container-registry-pushing-and-pulling-container-images","text":"To push or pull the container images to/from Google Container Registry you can follow the following article - https://cloud.google.com/container-registry/docs/pushing-and-pulling Note: You need to login to your Google Cloud account before pushing the container image. You can do it using the following command - gcloud auth login If you have multiple projects, you can select a specific one using the following command - gcloud config set project <Project_ID>","title":"2. Google Cloud Container Registry: Pushing and Pulling Container Images"},{"location":"kubernetes_basics/#kubernetes-cheat-sheet","text":"https://kubernetes.io/docs/reference/kubectl/cheatsheet/","title":"Kubernetes Cheat Sheet:"},{"location":"security_policies/","text":"Applying Kubernetes Network Policies to Secure the Application A network policy is a specification of how groups of pods are allowed to communicate with each other and other network endpoints. NetworkPolicy resources use labels to select pods and define rules which specify what traffic is allowed to the selected pods. By now you should have your Hybrid Cloud IoT Application working end to end. Let's apply some Kubernetes Network Policies to allow traffic from the frontend app to port '5050' only (REST API AGENT container accepts HTTP requests on port '5050'). 1. Apply Deny All Network Policy: Following Kubernetes Network Policy yaml definition would block all the traffic coming towards REST API Agent - apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: deny-all-rest-api-agent spec: podSelector: matchLabels: app: iot-backend-rest-api-agent tier: rest-api-agent policyTypes: - Ingress 1.0: Prerequisite - set kubectl context to on-prem-1 and make sure that your default namespace is set to your student ID. kubectl config use-context admin@on-prem-backend kubectl config get-contexts 1.1: Execute the following command on Kubernetes master node to apply this Network Policy on your REST API Agent - kubectl create -f https://raw.githubusercontent.com/marcinduma/HOLCLD-2101/main/Kubernetes/Backend/Network_Policies/deny_all_rest_api_agent.yaml 1.2: Now try to referesh your Frontend App webpage. It should stop working. 2. Apply Permit Port 5111 Network Policy: Following Kubernetes Network Policy yaml definition would allow the traffic on port 5111 to REST API Agent - apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: permit-port-5111-rest-api-agent spec: podSelector: matchLabels: app: iot-backend-rest-api-agent tier: rest-api-agent policyTypes: - Ingress ingress: - from: [] ports: - protocol: TCP port: 5111 2.1: Execute the following command on Kubernetes master node to apply this Network Policy on your REST API Agent - kubectl create -f https://raw.githubusercontent.com/marcinduma/HOLCLD-2101/main/Kubernetes/Backend/Network_Policies/permit_port_5111_rest_api_agent.yaml 2.2: Now try to referesh your Frontend App webpage. Does it work? Why? 3. Apply Permit Port 5050 Network Policy: Following Kubernetes Network Policy yaml definition would allow the traffic on port 5050 to REST API Agent - apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: permit-port-5050-rest-api-agent spec: podSelector: matchLabels: app: iot-backend-rest-api-agent tier: rest-api-agent policyTypes: - Ingress ingress: - from: [] ports: - protocol: TCP port: 5050 3.1: Execute the following command on Kubernetes master node to apply this Network Policy on your REST API Agent - kubectl create -f https://raw.githubusercontent.com/marcinduma/HOLCLD-2101/main/Kubernetes/Backend/Network_Policies/permit_port_5050_rest_api_agent.yaml 3.2: Now try to referesh your Frontend App webpage. Does it work? Why? Note: Other command related to Network Policy that you may use - Display Kubernetes Network Policies - kubectl get NetworkPolicies Display Network Policy Details - kubectl describe NetworkPolicy <Network Policy Name> Delete Network Policy - kubectl delete NetworkPolicy <Network Policy Name>","title":"Apply Security Policy to REST API Agent"},{"location":"security_policies/#applying-kubernetes-network-policies-to-secure-the-application","text":"A network policy is a specification of how groups of pods are allowed to communicate with each other and other network endpoints. NetworkPolicy resources use labels to select pods and define rules which specify what traffic is allowed to the selected pods. By now you should have your Hybrid Cloud IoT Application working end to end. Let's apply some Kubernetes Network Policies to allow traffic from the frontend app to port '5050' only (REST API AGENT container accepts HTTP requests on port '5050').","title":"Applying Kubernetes Network Policies to Secure the Application"},{"location":"security_policies/#1-apply-deny-all-network-policy","text":"Following Kubernetes Network Policy yaml definition would block all the traffic coming towards REST API Agent - apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: deny-all-rest-api-agent spec: podSelector: matchLabels: app: iot-backend-rest-api-agent tier: rest-api-agent policyTypes: - Ingress 1.0: Prerequisite - set kubectl context to on-prem-1 and make sure that your default namespace is set to your student ID. kubectl config use-context admin@on-prem-backend kubectl config get-contexts 1.1: Execute the following command on Kubernetes master node to apply this Network Policy on your REST API Agent - kubectl create -f https://raw.githubusercontent.com/marcinduma/HOLCLD-2101/main/Kubernetes/Backend/Network_Policies/deny_all_rest_api_agent.yaml 1.2: Now try to referesh your Frontend App webpage. It should stop working.","title":"1. Apply Deny All Network Policy:"},{"location":"security_policies/#2-apply-permit-port-5111-network-policy","text":"Following Kubernetes Network Policy yaml definition would allow the traffic on port 5111 to REST API Agent - apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: permit-port-5111-rest-api-agent spec: podSelector: matchLabels: app: iot-backend-rest-api-agent tier: rest-api-agent policyTypes: - Ingress ingress: - from: [] ports: - protocol: TCP port: 5111 2.1: Execute the following command on Kubernetes master node to apply this Network Policy on your REST API Agent - kubectl create -f https://raw.githubusercontent.com/marcinduma/HOLCLD-2101/main/Kubernetes/Backend/Network_Policies/permit_port_5111_rest_api_agent.yaml 2.2: Now try to referesh your Frontend App webpage. Does it work? Why?","title":"2. Apply Permit Port 5111 Network Policy:"},{"location":"security_policies/#3-apply-permit-port-5050-network-policy","text":"Following Kubernetes Network Policy yaml definition would allow the traffic on port 5050 to REST API Agent - apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: permit-port-5050-rest-api-agent spec: podSelector: matchLabels: app: iot-backend-rest-api-agent tier: rest-api-agent policyTypes: - Ingress ingress: - from: [] ports: - protocol: TCP port: 5050 3.1: Execute the following command on Kubernetes master node to apply this Network Policy on your REST API Agent - kubectl create -f https://raw.githubusercontent.com/marcinduma/HOLCLD-2101/main/Kubernetes/Backend/Network_Policies/permit_port_5050_rest_api_agent.yaml 3.2: Now try to referesh your Frontend App webpage. Does it work? Why? Note: Other command related to Network Policy that you may use - Display Kubernetes Network Policies - kubectl get NetworkPolicies Display Network Policy Details - kubectl describe NetworkPolicy <Network Policy Name> Delete Network Policy - kubectl delete NetworkPolicy <Network Policy Name>","title":"3. Apply Permit Port 5050 Network Policy:"}]}